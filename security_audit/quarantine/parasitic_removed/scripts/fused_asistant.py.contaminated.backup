#!/usr/bin/env python3
"""
Echoes AI Assistant - Advanced Implementation

A sophisticated AI assistant that combines simplicity with powerful capabilities.
Built from insights gained from ATLAS research and AGI development patterns.

Features:
- Simple, intuitive interface
- Advanced AI capabilities under the hood
- Context-aware conversations
- Tool integration for extended functionality
- Real-time streaming responses
- Learning and adaptation
"""

import json
import logging
import os
import re
import time
import uuid
from collections import Counter, defaultdict
from dataclasses import dataclass
from datetime import UTC, datetime
from pathlib import Path
from typing import Any

# Optional numpy import with fallback
try:
    import numpy as np

    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False

    # Create simple fallback for numpy functions
    class np_fallback:
        @staticmethod
        def random():
            class random_module:
                @staticmethod
                def choice(items):
                    import random

                    return random.choice(items)

                @staticmethod
                def rand():
                    import random

                    return random.random()

            return random_module()

        @staticmethod
        def mean(data):
            return sum(data) / len(data) if data else 0

    np = np_fallback()

# Enhanced CLI features
try:
    from prompt_toolkit import prompt
    from prompt_toolkit.auto_suggest import AutoSuggestFromHistory, Suggestion
    from prompt_toolkit.completion import FuzzyWordCompleter, WordCompleter
    from prompt_toolkit.formatted_text import HTML
    from prompt_toolkit.history import FileHistory
    from prompt_toolkit.key_binding import KeyBindings
    from prompt_toolkit.shortcuts import confirm

    ENHANCED_CLI_AVAILABLE = True
except ImportError:
    ENHANCED_CLI_AVAILABLE = False
    print("‚ö†Ô∏è Enhanced CLI not available. Install with: pip install prompt_toolkit")

# Additional dependencies for advanced features
try:
    from collections import Counter, defaultdict

    import matplotlib.pyplot as plt
    import networkx as nx

    ADVANCED_VISUALIZATION_AVAILABLE = True
except ImportError:
    ADVANCED_VISUALIZATION_AVAILABLE = False
    print(
        "‚ö†Ô∏è Advanced visualization not available. Install with: pip install matplotlib networkx"
    )

# OpenAI Integration for ChatGPT Models
try:
    import openai
    from openai import OpenAI

    OPENAI_AVAILABLE = True
    print("ü§ñ OpenAI integration available")
except ImportError:
    OPENAI_AVAILABLE = False
    print("‚ö†Ô∏è OpenAI not available. Install with: pip install openai")
    openai = None
    OpenAI = None

# ============================================================================
# Enhanced CLI System
# ============================================================================

# ============================================================================
# Advanced Conversational Systems
# ============================================================================


class ConversationalAutocomplete:
    """Enhanced conversational autocomplete with intelligent context awareness."""

    def __init__(self, assistant_instance):
        self.assistant = assistant_instance
        self.intent_patterns = {
            "question": [
                r"^(what|how|why|when|where|who|which|can|could|would|should|is|are|do|does|did)",
                r"\?$",
                r".*\b(explain|describe|tell me|show me|help me understand)\b.*",
            ],
            "command": [
                r"^(enable|disable|set|reset|clear|show|list|get|create|delete|remove|add|export|import)",
                r".*\b(run|execute|start|stop|restart|initialize)\b.*",
                r".*\b(call|invoke|trigger|activate)\b.*",
            ],
            "analysis": [
                r".*\b(analyze|examine|investigate|explore|review|audit|check|inspect)\b.*",
                r".*\b(compare|contrast|evaluate|assess|measure)\b.*",
                r".*\b(statistics|metrics|performance|status|health)\b.*",
            ],
            "creative": [
                r".*\b(create|generate|write|compose|design|imagine|brainstorm)\b.*",
                r".*\b(story|poem|script|article|content|idea)\b.*",
                r".*\b(invent|develop|prototype|mockup)\b.*",
            ],
            "technical": [
                r".*\b(code|program|function|method|algorithm|implement)\b.*",
                r".*\b(debug|fix|resolve|troubleshoot|optimize)\b.*",
                r".*\b(test|validate|verify|benchmark)\b.*",
            ],
            "filesystem": [
                r".*\b(file|files|directory|folder|path|filesystem)\b.*",
                r".*\b(read|write|edit|create|delete|remove|move|copy)\b.*",
                r".*\b(ls|cd|pwd|mkdir|rm|cp|mv|grep|find)\b.*",
                r".*\b(interact|access|manage|organize)\b.*\b(files|filesystem)\b.*",
                r".*\b(open|close|save|load)\b.*\b(file)\b.*",
            ],
            "emotional": [
                r"^(i feel|i am|i\'m|feeling|feels like)",
                r".*\b(happy|sad|angry|frustrated|confused|excited|worried)\b.*",
                r".*\b(help|support|guidance|advice)\b.*\b(emotional|personal)\b.*",
            ],
        }

        # Enhanced suggestion categories
        self.suggestion_categories = {
            "filesystem_tools": [
                "read_file <filename>",
                "write_file <filename> <content>",
                "edit_file <filename>",
                "remove_file <filename>",
                "grep_file <pattern> <filename>",
                "list_files <directory>",
                "create_directory <dirname>",
                "remove_directory <dirname>",
                "move_file <source> <destination>",
                "copy_file <source> <destination>",
                "get_file_info <filename>",
                "change_directory <path>",
                "current_directory",
                "parent_directory",
                "file_exists <filename>",
            ],
            "codebase_components": [
                "show functions",
                "list classes",
                "find definitions",
                "show imports",
                "analyze dependencies",
                "code documentation",
                "function signatures",
                "class hierarchy",
                "module structure",
                "api endpoints",
                "database schema",
            ],
            "editor_commands": [
                "open_in_editor <filename>",
                "save_file <filename>",
                "save_as <filename>",
                "close_editor",
                "find_in_file <pattern>",
                "replace_in_file <pattern> <replacement>",
                "goto_line <number>",
                "toggle_syntax_highlighting",
                "format_code <filename>",
                "check_syntax <filename>",
                "run_linter <filename>",
                "code_completion",
            ],
            "cross_platform_tools": [
                "open_with_default <filename>",
                "edit_with_notepad <filename>",
                "edit_with_nano <filename>",
                "edit_with_vim <filename>",
                "run_python_script <filename>",
                "execute_command <command>",
                "set_executable <filename>",
                "get_file_permissions <filename>",
                "change_permissions <filename>",
                "open_terminal <directory>",
                "run_shell_command <command>",
                "environment_variables",
            ],
            "custom_functions": [
                "create_function <name> <parameters>",
                "call_function <name> <arguments>",
                "list_user_functions",
                "delete_function <name>",
                "function_help <name>",
                "save_function <name>",
                "load_function <filename>",
                "test_function <name>",
                "debug_function <name>",
                "profile_function <name>",
                "export_functions",
            ],
            "basic_commands": [
                "help",
                "stats",
                "reset",
                "train",
                "exit",
                "quit",
                "bye",
            ],
            "openai_commands": [
                "enable openai <api-key>",
                "disable openai",
                "set model <model-name>",
                "refresh models",
                "show models",
                "model info <model-name>",
            ],
            "context_commands": [
                "show context",
                "show history",
                "show memory",
                "export session <filename>",
                "import session <filename>",
                "show visual context",
                "clear context",
            ],
            "history_commands": [
                "search history <query>",
                "filter history <criteria>",
                "threaded view",
                "jump unanswered",
                "add bookmark <message-id>",
                "add tag <message-id> <tag>",
            ],
            "diagnosis_commands": [
                "health check",
                "diagnostic history",
                "enable auto recovery",
                "disable auto recovery",
            ],
            "memory_commands": [
                "add attachment <filename> <description>",
                "search attachments <query>",
                "list attachments",
                "memory stats",
                "remove attachment <attachment-id>",
            ],
            "logging_commands": [
                "show logs",
                "clear logs",
                "export logs <format>",
                "api dashboard",
                "export api logs <format>",
            ],
            "tool_commands": [
                "list tools",
                "add tool <name>",
                "remove tool <name>",
                "tool info <name>",
                "call <tool-name> <arguments>",
            ],
        }

        # Context detection patterns
        self.context_patterns = {
            "filesystem_help": [
                r".*\b(interact|access|work|manage)\b.*\b(files?|directories?|folders?)\b.*",
                r".*\b(file|directory|folder)\b.*\b(help|assist|support)\b.*",
                r".*\b(can you|would you|could you)\b.*\b(deal with|handle|work with)\b.*\b(files?)\b.*",
                r".*\b(need|want)\b.*\b(to)?\s*(work|deal|interact)\b.*\b(with)?\s*files?\b.*",
            ],
            "code_help": [
                r".*\b(code|programming|develop|implement)\b.*\b(help|assist|support)\b.*",
                r".*\b(function|class|method|algorithm)\b.*\b(need|want|require)\b.*",
                r".*\b(debug|fix|troubleshoot)\b.*\b(code|program)\b.*",
                r".*\b(understand|explain|show)\b.*\b(codebase|components)\b.*",
            ],
            "editor_help": [
                r".*\b(edit|modify|change)\b.*\b(file|text|code)\b.*",
                r".*\b(open|close|save)\b.*\b(editor|file)\b.*",
                r".*\b(write|create|compose)\b.*\b(content|text|code)\b.*",
            ],
            "tool_creation": [
                r".*\b(create|make|build|develop)\b.*\b(tool|function|utility)\b.*",
                r".*\b(custom|own|personal)\b.*\b(function|tool|command)\b.*",
                r".*\b(need|want)\b.*\b(to)?\s*create\s*\b(tool|function)\b.*",
            ],
        }

        # Oxford dictionary patterns for general help
        self.general_help_patterns = [
            r".*\b(define|meaning|definition)\b.*\b(word|term)\b.*",
            r".*\b(what does|what is)\b.*\b(mean|mean)\b.*",
            r".*\b(dictionary|lookup|search)\b.*\b(word|term)\b.*",
            r".*\b(explain|describe)\b.*\b(concept|idea|term)\b.*",
        ]

        self.context_suggestions = {
            "openai": [
                "enable openai",
                "disable openai",
                "set model gpt-4o",
                "refresh models",
            ],
            "tools": ["list tools", "add tool", "call calculator", "tool info"],
            "session": [
                "show context",
                "show history",
                "export session",
                "import session",
            ],
            "logs": ["show logs", "clear logs"],
            "analysis": ["analyze this", "explain the concept", "compare options"],
            "creative": ["create a story", "write a poem", "brainstorm ideas"],
            "help": ["help", "show context", "stats"],
        }

        self.faq_suggestions = [
            "How do I enable OpenAI?",
            "What models are available?",
            "How do I add custom tools?",
            "Can I export my conversation?",
            "How does dynamic switching work?",
            "What's the difference between models?",
            "How do I check my session stats?",
            "Can I save my conversation history?",
        ]

    def detect_intent(self, text: str) -> str:
        """Detect user intent from partial input."""
        text_lower = text.lower()

        # Score each intent based on pattern matches
        intent_scores = {}
        for intent, patterns in self.intent_patterns.items():
            score = sum(1 for pattern in patterns if pattern in text_lower)
            intent_scores[intent] = score

        if max(intent_scores.values()) == 0:
            return "general"

        return max(intent_scores, key=intent_scores.get)

    def get_dynamic_suggestions(
        self, input_text: str, conversation_context: list[str] = None
    ) -> list[str]:
        """Get intelligent dynamic suggestions based on input and context."""
        suggestions = []

        if conversation_context is None:
            conversation_context = []

        # Detect specific contexts first
        context_type = self._detect_specific_context(input_text, conversation_context)

        if context_type == "filesystem_help":
            suggestions.extend(self.suggestion_categories["filesystem_tools"])
            suggestions.extend(self.suggestion_categories["cross_platform_tools"])
            suggestions.extend(
                [
                    "show filesystem help",
                    "list available file operations",
                    "how to read files",
                    "how to write files",
                    "file management tutorial",
                ]
            )

        elif context_type == "code_help":
            suggestions.extend(self.suggestion_categories["codebase_components"])
            suggestions.extend(self.suggestion_categories["editor_commands"])
            suggestions.extend(
                [
                    "show code structure",
                    "analyze current codebase",
                    "function documentation",
                    "class overview",
                    "import analysis",
                ]
            )

        elif context_type == "editor_help":
            suggestions.extend(self.suggestion_categories["editor_commands"])
            suggestions.extend(
                [
                    "open editor",
                    "close editor",
                    "save current file",
                    "find and replace",
                    "format code",
                    "check syntax",
                ]
            )

        elif context_type == "tool_creation":
            suggestions.extend(self.suggestion_categories["custom_functions"])
            suggestions.extend(
                [
                    "create custom tool",
                    "function builder",
                    "tool wizard",
                    "save my function",
                    "test custom function",
                    "debug my tool",
                ]
            )

        elif self._is_general_help_request(input_text):
            # Oxford dictionary style help
            suggestions.extend(
                [
                    "define term <word>",
                    "lookup definition <concept>",
                    "explain concept <idea>",
                    "dictionary search <term>",
                    "word meaning <word>",
                    "term definition <phrase>",
                ]
            )

        # Enhanced intent-based suggestions
        intent = self.detect_intent(input_text)

        if intent == "filesystem":
            suggestions.extend(self.suggestion_categories["filesystem_tools"])
            suggestions.extend(
                [
                    "navigate directories",
                    "file search",
                    "batch file operations",
                    "file permissions",
                    "disk usage",
                    "file monitoring",
                ]
            )

        elif intent == "technical":
            suggestions.extend(self.suggestion_categories["codebase_components"])
            suggestions.extend(self.suggestion_categories["editor_commands"])
            suggestions.extend(
                [
                    "code analysis",
                    "performance optimization",
                    "debug assistant",
                    "code review",
                    "refactoring help",
                    "testing strategies",
                ]
            )

        elif intent == "command":
            suggestions.extend(self.suggestion_categories["basic_commands"])
            suggestions.extend(self.suggestion_categories["openai_commands"])
            suggestions.extend(self.suggestion_categories["context_commands"])

        elif intent == "analysis":
            suggestions.extend(
                [
                    "analyze conversation",
                    "performance metrics",
                    "usage statistics",
                    "error analysis",
                    "trend analysis",
                    "comparative analysis",
                ]
            )

        elif intent == "question":
            # Enhanced question suggestions based on context
            if any("file" in ctx.lower() for ctx in conversation_context[-3:]):
                suggestions.extend(
                    [
                        "what file operations are available",
                        "how do I manage files",
                        "what file formats are supported",
                        "file system capabilities",
                    ]
                )
            elif any("code" in ctx.lower() for ctx in conversation_context[-3:]):
                suggestions.extend(
                    [
                        "what code analysis features exist",
                        "how to debug code",
                        "code documentation options",
                        "programming help",
                    ]
                )
            else:
                suggestions.extend(
                    [
                        "what are your capabilities",
                        "how does the memory system work",
                        "what models are available",
                        "explain your features",
                    ]
                )

        # Add context-aware suggestions from conversation history
        if conversation_context:
            recent_topics = self._extract_recent_topics(conversation_context)
            suggestions.extend(self._get_topic_suggestions(recent_topics))

        # Add file/directory specific suggestions if input contains paths
        if self._contains_file_reference(input_text):
            suggestions.extend(
                [
                    "read_file <filename>",
                    "write_file <filename> <content>",
                    "edit_file <filename>",
                    "get_file_info <filename>",
                    "list_files <directory>",
                    "grep_file <pattern> <filename>",
                ]
            )

        # Add cross-platform editor suggestions
        if self._contains_editor_reference(input_text):
            suggestions.extend(
                [
                    "edit_with_nano <filename>",
                    "edit_with_vim <filename>",
                    "edit_with_notepad <filename>",
                    "open_with_default <filename>",
                    "run_python_script <filename>",
                    "set_executable <filename>",
                ]
            )

        # Remove duplicates and limit suggestions
        unique_suggestions = list(dict.fromkeys(suggestions))  # Preserve order
        return unique_suggestions[:15]  # Return top 15 suggestions

    def _detect_specific_context(
        self, input_text: str, conversation_context: list[str]
    ) -> str:
        """Detect specific context types from input and conversation."""
        text_to_check = input_text.lower()

        # Check filesystem help context
        for pattern in self.context_patterns["filesystem_help"]:
            if re.search(pattern, text_to_check):
                return "filesystem_help"

        # Check code help context
        for pattern in self.context_patterns["code_help"]:
            if re.search(pattern, text_to_check):
                return "code_help"

        # Check editor help context
        for pattern in self.context_patterns["editor_help"]:
            if re.search(pattern, text_to_check):
                return "editor_help"

        # Check tool creation context
        for pattern in self.context_patterns["tool_creation"]:
            if re.search(pattern, text_to_check):
                return "tool_creation"

        # Check conversation context for patterns
        if conversation_context:
            recent_text = " ".join(conversation_context[-3:]).lower()

            # File-related conversation
            if any(
                word in recent_text
                for word in ["file", "directory", "folder", "filesystem"]
            ):
                if any(
                    word in text_to_check for word in ["help", "how", "what", "can"]
                ):
                    return "filesystem_help"

            # Code-related conversation
            if any(
                word in recent_text for word in ["code", "program", "function", "class"]
            ):
                if any(
                    word in text_to_check for word in ["help", "how", "what", "can"]
                ):
                    return "code_help"

        return None

    def _is_general_help_request(self, input_text: str) -> bool:
        """Check if input is a general help/definition request."""
        text = input_text.lower()
        for pattern in self.general_help_patterns:
            if re.search(pattern, text):
                return True
        return False

    def _extract_recent_topics(self, conversation_context: list[str]) -> list[str]:
        """Extract recent topics from conversation context."""
        topics = []
        text = " ".join(conversation_context[-5:]).lower()

        topic_keywords = {
            "filesystem": [
                "file",
                "directory",
                "folder",
                "path",
                "read",
                "write",
                "edit",
            ],
            "programming": [
                "code",
                "function",
                "class",
                "method",
                "algorithm",
                "debug",
            ],
            "editor": ["edit", "modify", "save", "open", "write", "create"],
            "tools": ["tool", "function", "custom", "create", "build"],
            "analysis": ["analyze", "examine", "check", "review", "metrics"],
            "session": ["session", "history", "context", "memory"],
        }

        for topic, keywords in topic_keywords.items():
            if any(keyword in text for keyword in keywords):
                topics.append(topic)

        return topics

    def _get_topic_suggestions(self, topics: list[str]) -> list[str]:
        """Get suggestions based on detected topics."""
        suggestions = []

        topic_suggestions = {
            "filesystem": [
                "show file operations",
                "directory navigation help",
                "file search tutorial",
                "batch file operations",
            ],
            "programming": [
                "show code structure",
                "function documentation",
                "debug assistance",
                "code analysis tools",
            ],
            "editor": [
                "editor commands",
                "file editing help",
                "save and load operations",
                "text manipulation",
            ],
            "tools": [
                "create custom tool",
                "function builder",
                "tool management",
                "debug my function",
            ],
            "analysis": [
                "analyze conversation",
                "performance metrics",
                "usage statistics",
                "trend analysis",
            ],
            "session": [
                "show conversation context",
                "export session data",
                "view conversation history",
                "memory management",
            ],
        }

        for topic in topics:
            suggestions.extend(topic_suggestions.get(topic, []))

        return suggestions

    def _contains_file_reference(self, input_text: str) -> bool:
        """Check if input contains file references."""
        file_indicators = [".", "/", "\\", ".txt", ".py", ".js", ".md", ".json", ".csv"]
        text = input_text.lower()
        return any(indicator in text for indicator in file_indicators)

    def _contains_editor_reference(self, input_text: str) -> bool:
        """Check if input contains editor references."""
        editor_indicators = ["edit", "nano", "vim", "notepad", "open", "save", "write"]
        text = input_text.lower()
        return any(indicator in text for indicator in editor_indicators)


class AdvancedHistoryManager:
    """Advanced conversation history with search, filtering, and threading."""

    def __init__(self, assistant_instance):
        self.assistant = assistant_instance
        self.history_index = defaultdict(list)  # Word -> message indices
        self.conversation_threads = []  # Threaded conversation groups
        self.bookmarks = {}  # Message ID -> bookmark label
        self.tags = defaultdict(set)  # Message ID -> set of tags

    def build_search_index(self):
        """Build search index for conversation history."""
        self.history_index.clear()

        for i, msg in enumerate(self.assistant.conversation_history):
            words = msg.content.lower().split()
            for word in set(words):  # Unique words per message
                self.history_index[word].append(i)

    def search_history(self, query: str, limit: int = 10) -> list[dict[str, Any]]:
        """Search conversation history with fuzzy matching."""
        if not self.history_index:
            self.build_search_index()

        query_words = query.lower().split()
        message_scores = defaultdict(int)

        for word in query_words:
            # Exact matches
            if word in self.history_index:
                for msg_idx in self.history_index[word]:
                    message_scores[msg_idx] += 2

            # Fuzzy matches (simple implementation)
            for indexed_word in self.history_index:
                if word in indexed_word or indexed_word in word:
                    for msg_idx in self.history_index[indexed_word]:
                        message_scores[msg_idx] += 1

        # Sort by score and return top results
        sorted_messages = sorted(
            message_scores.items(), key=lambda x: x[1], reverse=True
        )
        results = []

        for msg_idx, score in sorted_messages[:limit]:
            msg = self.assistant.conversation_history[msg_idx]
            results.append(
                {
                    "message": msg,
                    "index": msg_idx,
                    "score": score,
                    "preview": (
                        msg.content[:100] + "..."
                        if len(msg.content) > 100
                        else msg.content
                    ),
                }
            )

        return results

    def filter_history(self, filters: dict[str, Any]) -> list[dict[str, Any]]:
        """Filter history by various criteria."""
        filtered = []

        for i, msg in enumerate(self.assistant.conversation_history):
            include = True

            # Role filter
            if "role" in filters and msg.role != filters["role"]:
                include = False

            # Date range filter
            if "date_from" in filters and hasattr(msg, "timestamp"):
                if msg.timestamp < filters["date_from"]:
                    include = False

            if "date_to" in filters and hasattr(msg, "timestamp"):
                if msg.timestamp > filters["date_to"]:
                    include = False

            # Tag filter
            if "tags" in filters and i in self.tags:
                if not any(tag in self.tags[i] for tag in filters["tags"]):
                    include = False

            # Length filter
            if "min_length" in filters and len(msg.content) < filters["min_length"]:
                include = False

            if "max_length" in filters and len(msg.content) > filters["max_length"]:
                include = False

            if include:
                filtered.append(
                    {
                        "message": msg,
                        "index": i,
                        "preview": (
                            msg.content[:100] + "..."
                            if len(msg.content) > 100
                            else msg.content
                        ),
                    }
                )

        return filtered

    def add_bookmark(self, message_index: int, label: str):
        """Add bookmark to a specific message."""
        if 0 <= message_index < len(self.assistant.conversation_history):
            self.bookmarks[message_index] = label
            return True
        return False

    def add_tag(self, message_index: int, tag: str):
        """Add tag to a specific message."""
        if 0 <= message_index < len(self.assistant.conversation_history):
            self.tags[message_index].add(tag)
            return True
        return False

    def get_threaded_view(self) -> list[dict[str, Any]]:
        """Get conversation organized in threads."""
        # Simple threading implementation - groups by topic similarity
        threads = []
        current_thread = []

        for i, msg in enumerate(self.assistant.conversation_history):
            if not current_thread:
                current_thread.append({"message": msg, "index": i})
            else:
                # Check if this message continues the thread
                last_msg = current_thread[-1]["message"].content.lower()
                current_msg = msg.content.lower()

                # Simple similarity check
                common_words = set(last_msg.split()) & set(current_msg.split())
                if len(common_words) > 2 or msg.role == "user":
                    current_thread.append({"message": msg, "index": i})
                else:
                    # Start new thread
                    threads.append(current_thread)
                    current_thread = [{"message": msg, "index": i}]

        if current_thread:
            threads.append(current_thread)

        return threads

    def jump_to_last_unanswered(self) -> int | None:
        """Find the last message that might need a response."""
        for i in range(len(self.assistant.conversation_history) - 1, -1, -1):
            msg = self.assistant.conversation_history[i]
            if msg.role == "user" and "?" in msg.content:
                return i
        return None


class VisualContextManager:
    """Visual context visualization with relationship mapping and timelines."""

    def __init__(self, assistant_instance):
        self.assistant = assistant_instance
        self.entity_graph = nx.DiGraph()
        self.topic_timeline = []
        self.relationships = defaultdict(list)

    def build_context_graph(self):
        """Build relationship graph from conversation context."""
        self.entity_graph.clear()

        # Extract entities and relationships from conversation
        for i, msg in enumerate(self.assistant.conversation_history):
            # Add message nodes
            self.entity_graph.add_node(
                f"msg_{i}",
                type="message",
                role=msg.role,
                content=msg.content[:50] + "...",
                timestamp=getattr(msg, "timestamp", datetime.now()),
            )

            # Extract entities (simple keyword-based)
            entities = self._extract_entities(msg.content)
            for entity in entities:
                entity_id = f"entity_{entity.lower().replace(' ', '_')}"
                if entity_id not in self.entity_graph:
                    self.entity_graph.add_node(entity_id, type="entity", label=entity)

                # Connect message to entity
                self.entity_graph.add_edge(f"msg_{i}", entity_id, type="mentions")

    def _extract_entities(self, text: str) -> list[str]:
        """Extract entities from text (simple implementation)."""
        # Common technical and business terms
        entity_patterns = [
            r"\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b",  # Proper nouns
            r"\b(?:AI|ML|API|JSON|XML|HTML|CSS|SQL|NoSQL)\b",  # Tech terms
            r"\b(?:model|algorithm|function|class|method)\b",  # Programming terms
            r"\b(?:analysis|research|data|insight|report)\b",  # Business terms
        ]

        entities = []
        for pattern in entity_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            entities.extend(matches)

        # Remove duplicates and common words
        stop_words = {
            "the",
            "and",
            "or",
            "but",
            "in",
            "on",
            "at",
            "to",
            "for",
            "of",
            "with",
            "by",
        }
        entities = [
            e for e in set(entities) if e.lower() not in stop_words and len(e) > 2
        ]

        return entities[:5]  # Limit to top 5 entities

    def generate_timeline(self) -> list[dict[str, Any]]:
        """Generate timeline of conversation topics and events."""
        timeline = []

        for i, msg in enumerate(self.assistant.conversation_history):
            # Analyze message for topic and sentiment
            topic = self._classify_topic(msg.content)
            sentiment = self._analyze_sentiment(msg.content)

            timeline.append(
                {
                    "index": i,
                    "timestamp": getattr(msg, "timestamp", datetime.now()),
                    "role": msg.role,
                    "topic": topic,
                    "sentiment": sentiment,
                    "length": len(msg.content),
                    "preview": (
                        msg.content[:100] + "..."
                        if len(msg.content) > 100
                        else msg.content
                    ),
                }
            )

        self.topic_timeline = timeline
        return timeline

    def _classify_topic(self, text: str) -> str:
        """Classify message topic (simple implementation)."""
        topic_keywords = {
            "technical": [
                "code",
                "function",
                "algorithm",
                "implement",
                "debug",
                "programming",
            ],
            "analysis": [
                "analyze",
                "explain",
                "compare",
                "evaluate",
                "data",
                "research",
            ],
            "creative": ["create", "write", "design", "imagine", "brainstorm", "story"],
            "business": ["strategy", "market", "customer", "revenue", "growth", "plan"],
            "personal": ["feel", "think", "help", "advice", "personal", "opinion"],
            "general": [],
        }

        text_lower = text.lower()
        topic_scores = {}

        for topic, keywords in topic_keywords.items():
            score = sum(1 for keyword in keywords if keyword in text_lower)
            topic_scores[topic] = score

        return max(topic_scores, key=topic_scores.get)

    def _analyze_sentiment(self, text: str) -> str:
        """Simple sentiment analysis."""
        positive_words = [
            "good",
            "great",
            "excellent",
            "amazing",
            "wonderful",
            "love",
            "perfect",
        ]
        negative_words = [
            "bad",
            "terrible",
            "awful",
            "horrible",
            "hate",
            "wrong",
            "problem",
        ]

        text_lower = text.lower()
        pos_count = sum(1 for word in positive_words if word in text_lower)
        neg_count = sum(1 for word in negative_words if word in text_lower)

        if pos_count > neg_count:
            return "positive"
        elif neg_count > pos_count:
            return "negative"
        else:
            return "neutral"

    def display_visual_context(self):
        """Display visual context representation in terminal."""
        print("\n" + "=" * 80)
        print("üé® VISUAL CONTEXT ANALYSIS")
        print("=" * 80)

        # Build and display timeline
        timeline = self.generate_timeline()

        print(f"\nüìÖ CONVERSATION TIMELINE ({len(timeline)} messages):")
        print("-" * 60)

        # Group by topic
        topic_groups = defaultdict(list)
        for event in timeline:
            topic_groups[event["topic"]].append(event)

        for topic, events in topic_groups.items():
            print(f"\nüè∑Ô∏è  {topic.upper()} ({len(events)} messages):")
            for event in events[-3:]:  # Show last 3 events per topic
                time_str = (
                    event["timestamp"].strftime("%H:%M:%S")
                    if hasattr(event["timestamp"], "strftime")
                    else "Unknown"
                )
                sentiment_icon = {"positive": "üòä", "negative": "üòü", "neutral": "üòê"}[
                    event["sentiment"]
                ]
                role_icon = "üë§" if event["role"] == "user" else "ü§ñ"
                print(f"   {time_str} {role_icon} {sentiment_icon} {event['preview']}")

        # Display entity relationships
        self.build_context_graph()

        print("\nüï∏Ô∏è  ENTITY RELATIONSHIP MAP:")
        print("-" * 60)

        # Show top entities by connections
        entity_connections = defaultdict(int)
        for node in self.entity_graph.nodes():
            if self.entity_graph.nodes[node].get("type") == "entity":
                entity_connections[node] = self.entity_graph.degree(node)

        top_entities = sorted(
            entity_connections.items(), key=lambda x: x[1], reverse=True
        )[:5]

        for entity_id, connections in top_entities:
            label = self.entity_graph.nodes[entity_id].get("label", entity_id)
            print(f"   ‚Ä¢ {label} (mentioned in {connections} messages)")

        # Display conversation flow
        print("\nüåä CONVERSATION FLOW:")
        print("-" * 60)

        if len(timeline) > 1:
            topic_transitions = []
            for i in range(1, len(timeline)):
                if timeline[i]["topic"] != timeline[i - 1]["topic"]:
                    topic_transitions.append(
                        f"{timeline[i-1]['topic']} ‚Üí {timeline[i]['topic']}"
                    )

            if topic_transitions:
                print("   Topic transitions:")
                for transition in topic_transitions[-5:]:  # Show last 5 transitions
                    print(f"   ‚Ä¢ {transition}")
            else:
                print("   Focused on a single topic")

        # Display insights
        print("\nüí° CONTEXT INSIGHTS:")
        print("-" * 60)

        # Most active topic
        if topic_groups:
            most_active = max(topic_groups.items(), key=lambda x: len(x[1]))
            print(
                f"   ‚Ä¢ Primary topic: {most_active[0]} ({len(most_active[1])} messages)"
            )

        # Conversation balance
        user_msgs = sum(
            1 for msg in self.assistant.conversation_history if msg.role == "user"
        )
        ai_msgs = len(self.assistant.conversation_history) - user_msgs
        print(
            f"   ‚Ä¢ Conversation balance: {user_msgs} user messages, {ai_msgs} AI responses"
        )

        # Average message length
        avg_length = sum(
            len(msg.content) for msg in self.assistant.conversation_history
        ) / len(self.assistant.conversation_history)
        print(f"   ‚Ä¢ Average message length: {avg_length:.0f} characters")

        print("\n" + "=" * 80)

    def export_context_data(self) -> dict[str, Any]:
        """Export context data for external visualization."""
        self.build_context_graph()
        timeline = self.generate_timeline()

        # Convert networkx graph to serializable format
        graph_data = {
            "nodes": [
                {
                    "id": node,
                    "label": self.entity_graph.nodes[node].get("label", node),
                    "type": self.entity_graph.nodes[node].get("type", "unknown"),
                    "data": self.entity_graph.nodes[node],
                }
                for node in self.entity_graph.nodes()
            ],
            "edges": [
                {
                    "source": edge[0],
                    "target": edge[1],
                    "type": self.entity_graph.edges[edge].get("type", "related"),
                }
                for edge in self.entity_graph.edges()
            ],
        }

        return {
            "timeline": timeline,
            "graph": graph_data,
            "insights": {
                "total_messages": len(self.assistant.conversation_history),
                "topics": list(set(event["topic"] for event in timeline)),
                "entities": len(
                    [
                        n
                        for n in self.entity_graph.nodes()
                        if self.entity_graph.nodes[n].get("type") == "entity"
                    ]
                ),
            },
        }


class APILoggingDashboard:
    """Comprehensive API logging with dashboards and error aggregation."""

    def __init__(self):
        self.log_dir = Path("logs")
        self.log_dir.mkdir(exist_ok=True)
        self.api_log_file = self.log_dir / "api_detailed.log"
        self.error_log_file = self.log_dir / "errors.log"
        self.metrics_file = self.log_dir / "metrics.json"

        # Initialize metrics
        self.metrics = {
            "total_requests": 0,
            "successful_requests": 0,
            "failed_requests": 0,
            "average_response_time": 0.0,
            "model_usage": defaultdict(int),
            "error_types": defaultdict(int),
            "hourly_requests": defaultdict(int),
            "daily_requests": defaultdict(int),
        }

        self._load_metrics()

    def log_api_call(
        self,
        endpoint: str,
        model: str,
        request_data: dict[str, Any],
        response_data: dict[str, Any],
        response_time: float,
        success: bool,
        error_message: str = None,
    ):
        """Log detailed API call information."""

        timestamp = datetime.now()
        log_entry = {
            "timestamp": timestamp.isoformat(),
            "endpoint": endpoint,
            "model": model,
            "request_size": len(str(request_data)),
            "response_size": len(str(response_data)) if success else 0,
            "response_time": response_time,
            "success": success,
            "error": error_message,
            "hour": timestamp.hour,
            "date": timestamp.date().isoformat(),
        }

        # Write to detailed log
        with open(self.api_log_file, "a") as f:
            f.write(json.dumps(log_entry) + "\n")

        # Update metrics
        self._update_metrics(log_entry)

        # Log errors separately
        if not success and error_message:
            with open(self.error_log_file, "a") as f:
                error_entry = {
                    "timestamp": timestamp.isoformat(),
                    "model": model,
                    "error": error_message,
                    "request_preview": str(request_data)[:200],
                }
                f.write(json.dumps(error_entry) + "\n")

    def _update_metrics(self, log_entry: dict[str, Any]):
        """Update internal metrics."""
        self.metrics["total_requests"] += 1

        if log_entry["success"]:
            self.metrics["successful_requests"] += 1
        else:
            self.metrics["failed_requests"] += 1
            if log_entry["error"]:
                self.metrics["error_types"][log_entry["error"]] += 1

        # Update response time average
        total_time = self.metrics["average_response_time"] * (
            self.metrics["total_requests"] - 1
        )
        self.metrics["average_response_time"] = (
            total_time + log_entry["response_time"]
        ) / self.metrics["total_requests"]

        # Update model usage
        self.metrics["model_usage"][log_entry["model"]] += 1

        # Update time-based metrics
        self.metrics["hourly_requests"][log_entry["hour"]] += 1
        self.metrics["daily_requests"][log_entry["date"]] += 1

        # Save metrics
        self._save_metrics()

    def _load_metrics(self):
        """Load metrics from file."""
        if self.metrics_file.exists():
            try:
                with open(self.metrics_file) as f:
                    loaded_metrics = json.load(f)
                    # Convert defaultdicts back
                    for key, value in loaded_metrics.items():
                        if key in [
                            "model_usage",
                            "error_types",
                            "hourly_requests",
                            "daily_requests",
                        ]:
                            self.metrics[key] = defaultdict(int, value)
                        else:
                            self.metrics[key] = value
            except Exception as e:
                print(f"Warning: Could not load metrics: {e}")

    def _save_metrics(self):
        """Save metrics to file."""
        try:
            # Convert defaultdicts to regular dicts for JSON serialization
            serializable_metrics = {}
            for key, value in self.metrics.items():
                if isinstance(value, defaultdict):
                    serializable_metrics[key] = dict(value)
                else:
                    serializable_metrics[key] = value

            with open(self.metrics_file, "w") as f:
                json.dump(serializable_metrics, f, indent=2)
        except Exception as e:
            print(f"Warning: Could not save metrics: {e}")

    def display_dashboard(self):
        """Display comprehensive API dashboard."""
        print("\n" + "=" * 80)
        print("üìä API LOGGING DASHBOARD")
        print("=" * 80)

        # Overview metrics
        success_rate = (
            (self.metrics["successful_requests"] / self.metrics["total_requests"] * 100)
            if self.metrics["total_requests"] > 0
            else 0
        )

        print("\nüìà OVERVIEW:")
        print(f"   ‚Ä¢ Total Requests: {self.metrics['total_requests']}")
        print(f"   ‚Ä¢ Success Rate: {success_rate:.1f}%")
        print(
            f"   ‚Ä¢ Average Response Time: {self.metrics['average_response_time']:.3f}s"
        )
        print(f"   ‚Ä¢ Failed Requests: {self.metrics['failed_requests']}")

        # Model usage
        if self.metrics["model_usage"]:
            print("\nü§ñ MODEL USAGE:")
            for model, count in sorted(
                self.metrics["model_usage"].items(), key=lambda x: x[1], reverse=True
            ):
                percentage = (
                    (count / self.metrics["total_requests"] * 100)
                    if self.metrics["total_requests"] > 0
                    else 0
                )
                print(f"   ‚Ä¢ {model}: {count} requests ({percentage:.1f}%)")

        # Error analysis
        if self.metrics["error_types"]:
            print("\n‚ùå ERROR ANALYSIS:")
            for error_type, count in sorted(
                self.metrics["error_types"].items(), key=lambda x: x[1], reverse=True
            ):
                print(f"   ‚Ä¢ {error_type}: {count} occurrences")

        # Time-based analysis
        if self.metrics["hourly_requests"]:
            print("\n‚è∞ HOURLY DISTRIBUTION:")
            for hour in sorted(self.metrics["hourly_requests"].keys()):
                count = self.metrics["hourly_requests"][hour]
                bar = "‚ñà" * (count // 2)  # Simple bar chart
                print(f"   {hour:02d}:00 {count:3d} {bar}")

        # Recent errors
        if self.error_log_file.exists():
            print("\nüö® RECENT ERRORS:")
            try:
                with open(self.error_log_file) as f:
                    error_lines = f.readlines()[-5:]  # Last 5 errors

                for line in error_lines:
                    if line.strip():
                        error_data = json.loads(line)
                        timestamp = error_data["timestamp"][:19]
                        print(f"   ‚Ä¢ {timestamp} - {error_data['error']}")
            except Exception as e:
                print(f"   Could not read error log: {e}")

        print("\n" + "=" * 80)

    def export_logs(self, format: str = "json") -> str:
        """Export logs in specified format."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        if format.lower() == "json":
            export_file = self.log_dir / f"api_export_{timestamp}.json"

            # Combine all log data
            export_data = {"metrics": dict(self.metrics), "api_calls": [], "errors": []}

            # Read API calls
            if self.api_log_file.exists():
                with open(self.api_log_file) as f:
                    for line in f:
                        if line.strip():
                            export_data["api_calls"].append(json.loads(line))

            # Read errors
            if self.error_log_file.exists():
                with open(self.error_log_file) as f:
                    for line in f:
                        if line.strip():
                            export_data["errors"].append(json.loads(line))

            with open(export_file, "w") as f:
                json.dump(export_data, f, indent=2)

        elif format.lower() == "csv":
            export_file = self.log_dir / f"api_export_{timestamp}.csv"

            # Simple CSV export
            with open(export_file, "w") as f:
                f.write("timestamp,model,response_time,success,error\n")
                if self.api_log_file.exists():
                    with open(self.api_log_file) as api_f:
                        for line in api_f:
                            if line.strip():
                                entry = json.loads(line)
                                f.write(
                                    f"{entry['timestamp']},{entry['model']},{entry['response_time']},{entry['success']},{entry.get('error', '')}\n"
                                )

        return str(export_file)


class MultimodalMemoryManager:
    """Multimodal memory system with attachments and search capabilities."""

    def __init__(self, assistant_instance):
        self.assistant = assistant_instance
        self.attachments_dir = Path("attachments")
        self.attachments_dir.mkdir(exist_ok=True)
        self.memory_index = {}
        self.attachment_metadata = {}

    def add_attachment(
        self, file_path: str, description: str = "", tags: list[str] = None
    ) -> dict[str, Any]:
        """Add file attachment to memory with metadata."""
        try:
            source_path = Path(file_path)
            if not source_path.exists():
                return {"success": False, "error": "File not found"}

            # Generate unique attachment ID
            attachment_id = f"att_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{len(self.attachment_metadata)}"

            # Copy file to attachments directory
            dest_path = self.attachments_dir / f"{attachment_id}_{source_path.name}"
            import shutil

            shutil.copy2(source_path, dest_path)

            # Extract metadata
            metadata = {
                "id": attachment_id,
                "original_path": str(source_path),
                "stored_path": str(dest_path),
                "filename": source_path.name,
                "file_size": dest_path.stat().st_size,
                "file_type": source_path.suffix.lower(),
                "description": description,
                "tags": tags or [],
                "timestamp": datetime.now().isoformat(),
                "content_preview": self._generate_content_preview(dest_path),
                "extracted_text": self._extract_text_content(dest_path),
                "indexed": False,
            }

            self.attachment_metadata[attachment_id] = metadata

            # Index for search
            self._index_attachment(attachment_id)

            return {
                "success": True,
                "attachment_id": attachment_id,
                "metadata": metadata,
            }

        except Exception as e:
            return {"success": False, "error": str(e)}

    def _generate_content_preview(self, file_path: Path) -> str:
        """Generate content preview based on file type."""
        try:
            file_type = file_path.suffix.lower()

            if file_type in [".txt", ".md", ".py", ".js", ".html", ".css"]:
                # Text files - read first few lines
                with open(file_path, encoding="utf-8", errors="ignore") as f:
                    preview = f.read(200)
                    return preview + "..." if len(preview) >= 200 else preview

            elif file_type in [".jpg", ".jpeg", ".png", ".gif", ".bmp"]:
                # Image files - basic info
                try:
                    from PIL import Image

                    with Image.open(file_path) as img:
                        return (
                            f"Image {img.size[0]}x{img.size[1]}, Format: {img.format}"
                        )
                except:
                    return f"Image file: {file_path.name}"

            elif file_type in [".pdf"]:
                # PDF files - try to extract first page text
                try:
                    import PyPDF2

                    with open(file_path, "rb") as f:
                        reader = PyPDF2.PdfReader(f)
                        if len(reader.pages) > 0:
                            text = reader.pages[0].extract_text()[:200]
                            return text + "..." if len(text) >= 200 else text
                except:
                    return f"PDF document: {file_path.name}"

            else:
                return (
                    f"Binary file: {file_path.name} ({file_path.stat().st_size} bytes)"
                )

        except Exception:
            return f"File: {file_path.name}"

    def _extract_text_content(self, file_path: Path) -> str:
        """Extract full text content for search indexing."""
        try:
            file_type = file_path.suffix.lower()

            if file_type in [".txt", ".md", ".py", ".js", ".html", ".css"]:
                with open(file_path, encoding="utf-8", errors="ignore") as f:
                    return f.read()

            elif file_type == ".pdf":
                try:
                    import PyPDF2

                    text_content = []
                    with open(file_path, "rb") as f:
                        reader = PyPDF2.PdfReader(f)
                        for page in reader.pages:
                            text_content.append(page.extract_text())
                    return " ".join(text_content)
                except:
                    return ""

            elif file_type in [".jpg", ".jpeg", ".png"]:
                # OCR for images (if available)
                try:
                    import pytesseract
                    from PIL import Image

                    text = pytesseract.image_to_string(Image.open(file_path))
                    return text
                except:
                    return ""

            return ""

        except Exception:
            return ""

    def _index_attachment(self, attachment_id: str):
        """Index attachment for search functionality."""
        if attachment_id not in self.attachment_metadata:
            return

        metadata = self.attachment_metadata[attachment_id]

        # Create searchable text
        searchable_text = " ".join(
            [
                metadata["filename"],
                metadata["description"],
                " ".join(metadata["tags"]),
                metadata["content_preview"],
                metadata["extracted_text"],
            ]
        ).lower()

        # Index words
        words = searchable_text.split()
        for word in set(words):  # Unique words
            if word not in self.memory_index:
                self.memory_index[word] = []
            self.memory_index[word].append(attachment_id)

        metadata["indexed"] = True

    def search_attachments(self, query: str, limit: int = 10) -> list[dict[str, Any]]:
        """Search attachments by content, filename, or metadata."""
        query_words = query.lower().split()
        attachment_scores = defaultdict(int)

        for word in query_words:
            if word in self.memory_index:
                for attachment_id in self.memory_index[word]:
                    attachment_scores[attachment_id] += 1

        # Sort by score and return results
        results = []
        for attachment_id, score in sorted(
            attachment_scores.items(), key=lambda x: x[1], reverse=True
        ):
            if attachment_id in self.attachment_metadata:
                metadata = self.attachment_metadata[attachment_id].copy()
                metadata["search_score"] = score
                results.append(metadata)

        return results[:limit]

    def list_attachments(
        self, filter_type: str = None, tags: list[str] = None
    ) -> list[dict[str, Any]]:
        """List attachments with optional filtering."""
        results = []

        for attachment_id, metadata in self.attachment_metadata.items():
            # Type filter
            if filter_type and not metadata["file_type"].startswith(filter_type):
                continue

            # Tags filter
            if tags and not any(tag in metadata["tags"] for tag in tags):
                continue

            results.append(metadata)

        # Sort by timestamp (newest first)
        results.sort(key=lambda x: x["timestamp"], reverse=True)
        return results

    def get_attachment(self, attachment_id: str) -> dict[str, Any]:
        """Get attachment metadata by ID."""
        return self.attachment_metadata.get(attachment_id, {})

    def remove_attachment(self, attachment_id: str) -> bool:
        """Remove attachment from memory."""
        try:
            if attachment_id not in self.attachment_metadata:
                return False

            metadata = self.attachment_metadata[attachment_id]

            # Remove file
            file_path = Path(metadata["stored_path"])
            if file_path.exists():
                file_path.unlink()

            # Remove from index
            for word, attachment_ids in self.memory_index.items():
                if attachment_id in attachment_ids:
                    attachment_ids.remove(attachment_id)
                if not attachment_ids:
                    del self.memory_index[word]

            # Remove metadata
            del self.attachment_metadata[attachment_id]

            return True

        except Exception:
            return False

    def update_attachment_tags(self, attachment_id: str, tags: list[str]) -> bool:
        """Update attachment tags."""
        if attachment_id not in self.attachment_metadata:
            return False

        old_tags = self.attachment_metadata[attachment_id]["tags"]
        self.attachment_metadata[attachment_id]["tags"] = tags

        # Re-index if tags changed
        if set(old_tags) != set(tags):
            self._index_attachment(attachment_id)

        return True

    def get_memory_stats(self) -> dict[str, Any]:
        """Get memory system statistics."""
        total_size = sum(
            meta["file_size"] for meta in self.attachment_metadata.values()
        )

        file_types = defaultdict(int)
        for meta in self.attachment_metadata.values():
            file_types[meta["file_type"]] += 1

        return {
            "total_attachments": len(self.attachment_metadata),
            "total_size_mb": total_size / (1024 * 1024),
            "indexed_words": len(self.memory_index),
            "file_types": dict(file_types),
            "storage_directory": str(self.attachments_dir),
        }

    def export_memory_index(self) -> str:
        """Export memory index for backup."""
        export_data = {
            "timestamp": datetime.now().isoformat(),
            "attachments": self.attachment_metadata,
            "memory_index": {
                word: ids for word, ids in self.memory_index.items() if ids
            },
        }

        export_file = (
            self.attachments_dir
            / f"memory_index_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        )
        with open(export_file, "w") as f:
            json.dump(export_data, f, indent=2)

        return str(export_file)

    def import_memory_index(self, index_file: str) -> bool:
        """Import memory index from backup."""
        try:
            with open(index_file) as f:
                import_data = json.load(f)

            self.attachment_metadata.update(import_data.get("attachments", {}))
            self.memory_index.update(import_data.get("memory_index", {}))

            return True

        except Exception:
            return False


class SelfDiagnosisAndRecovery:
    """Self-diagnosis and recovery system with proactive health checks."""

    def __init__(self, assistant_instance):
        self.assistant = assistant_instance
        self.health_status = {
            "overall": "healthy",
            "openai_connection": "unknown",
            "memory_system": "unknown",
            "tool_system": "unknown",
            "logging_system": "unknown",
            "session_management": "unknown",
        }
        self.diagnostic_history = []
        self.auto_recovery_enabled = True

    def run_comprehensive_health_check(self) -> dict[str, Any]:
        """Run comprehensive health check on all systems."""
        print("\n" + "=" * 80)
        print("üîç ECHOES SELF-DIAGNOSIS SYSTEM")
        print("=" * 80)

        results = {
            "timestamp": datetime.now().isoformat(),
            "checks": {},
            "overall_status": "healthy",
            "issues_found": [],
            "recommendations": [],
        }

        # Check OpenAI connection
        openai_status = self._check_openai_connection()
        results["checks"]["openai_connection"] = openai_status
        self.health_status["openai_connection"] = openai_status["status"]

        # Check memory system
        memory_status = self._check_memory_system()
        results["checks"]["memory_system"] = memory_status
        self.health_status["memory_system"] = memory_status["status"]

        # Check tool system
        tool_status = self._check_tool_system()
        results["checks"]["tool_system"] = tool_status
        self.health_status["tool_system"] = tool_status["status"]

        # Check logging system
        logging_status = self._check_logging_system()
        results["checks"]["logging_system"] = logging_status
        self.health_status["logging_system"] = logging_status["status"]

        # Check session management
        session_status = self._check_session_management()
        results["checks"]["session_management"] = session_status
        self.health_status["session_management"] = session_status["status"]

        # Check system resources
        resource_status = self._check_system_resources()
        results["checks"]["system_resources"] = resource_status

        # Determine overall status
        failed_checks = [
            name
            for name, check in results["checks"].items()
            if check["status"] == "error"
        ]
        warning_checks = [
            name
            for name, check in results["checks"].items()
            if check["status"] == "warning"
        ]

        if failed_checks:
            results["overall_status"] = "error"
            self.health_status["overall"] = "error"
        elif warning_checks:
            results["overall_status"] = "warning"
            self.health_status["overall"] = "warning"
        else:
            results["overall_status"] = "healthy"
            self.health_status["overall"] = "healthy"

        # Collect issues and recommendations
        for name, check in results["checks"].items():
            if check.get("issues"):
                results["issues_found"].extend(
                    [f"{name}: {issue}" for issue in check["issues"]]
                )
            if check.get("recommendations"):
                results["recommendations"].extend(
                    [f"{name}: {rec}" for rec in check["recommendations"]]
                )

        # Store diagnostic history
        self.diagnostic_history.append(results)
        if len(self.diagnostic_history) > 10:
            self.diagnostic_history.pop(0)  # Keep last 10 diagnostics

        # Display results
        self._display_health_check_results(results)

        # Auto-recovery if enabled
        if self.auto_recovery_enabled and failed_checks:
            print("\nüîß ATTEMPTING AUTO-RECOVERY...")
            recovery_results = self._attempt_auto_recovery(failed_checks)
            results["recovery_attempts"] = recovery_results

        return results

    def _check_openai_connection(self) -> dict[str, Any]:
        """Check OpenAI connection and model availability."""
        result = {"status": "healthy", "issues": [], "recommendations": []}

        try:
            if not self.assistant.openai_enabled:
                result["status"] = "warning"
                result["issues"].append("OpenAI integration is disabled")
                result["recommendations"].append(
                    "Enable OpenAI with 'enable openai <api-key>'"
                )
                return result

            # Test API connectivity
            if hasattr(self.assistant, "client") and self.assistant.client:
                try:
                    # Simple model list test
                    models = self.assistant.client.models.list()
                    if len(models.data) > 0:
                        result[
                            "details"
                        ] = f"Connected - {len(models.data)} models available"
                    else:
                        result["status"] = "error"
                        result["issues"].append("No models available from OpenAI")
                        result["recommendations"].append(
                            "Check API key and permissions"
                        )
                except Exception as e:
                    result["status"] = "error"
                    result["issues"].append(f"API connection failed: {str(e)}")
                    result["recommendations"].append(
                        "Check internet connection and API key"
                    )
            else:
                result["status"] = "error"
                result["issues"].append("OpenAI client not initialized")
                result["recommendations"].append("Restart OpenAI integration")

        except Exception as e:
            result["status"] = "error"
            result["issues"].append(f"OpenAI check failed: {str(e)}")

        return result

    def _check_memory_system(self) -> dict[str, Any]:
        """Check memory and personality systems."""
        result = {"status": "healthy", "issues": [], "recommendations": []}

        try:
            # Check conversation history
            if not hasattr(self.assistant, "conversation_history"):
                result["status"] = "error"
                result["issues"].append("Conversation history not found")
                return result

            history_size = len(self.assistant.conversation_history)
            result["details"] = f"Conversation history: {history_size} messages"

            # Check personality memory
            if not hasattr(self.assistant, "personality_memory"):
                result["status"] = "warning"
                result["issues"].append("Personality memory not initialized")
                result["recommendations"].append(
                    "Personality memory will be created on next interaction"
                )
            else:
                memory_size = len(self.assistant.personality_memory)
                result["details"] += f", Personality memory: {memory_size} items"

            # Check context analyzer
            if not hasattr(self.assistant, "context_analyzer"):
                result["status"] = "warning"
                result["issues"].append("Context analyzer not found")
                result["recommendations"].append("Context analysis may be limited")

        except Exception as e:
            result["status"] = "error"
            result["issues"].append(f"Memory system check failed: {str(e)}")

        return result

    def _check_tool_system(self) -> dict[str, Any]:
        """Check runtime tool system."""
        result = {"status": "healthy", "issues": [], "recommendations": []}

        try:
            # Check if tool manager exists
            if not hasattr(self.assistant, "tool_manager"):
                result["status"] = "warning"
                result["issues"].append("Tool manager not initialized")
                result["recommendations"].append("Tool functionality may be limited")
                return result

            tool_manager = self.assistant.tool_manager
            tools_count = len(tool_manager.user_tools)
            result["details"] = f"Runtime tools: {tools_count} available"

            # Test tool directory
            if not tool_manager.tools_dir.exists():
                result["status"] = "warning"
                result["issues"].append("Tool directory not found")
                result["recommendations"].append(
                    "Tool directory will be created when needed"
                )

            # Test tool execution with a simple function
            try:
                test_result = tool_manager.execute_tool("_health_check", lambda: "ok")
                if test_result != "ok":
                    result["status"] = "warning"
                    result["issues"].append("Tool execution test failed")
            except:
                # Expected since _health_check doesn't exist
                pass

        except Exception as e:
            result["status"] = "error"
            result["issues"].append(f"Tool system check failed: {str(e)}")

        return result

    def _check_logging_system(self) -> dict[str, Any]:
        """Check logging system functionality."""
        result = {"status": "healthy", "issues": [], "recommendations": []}

        try:
            # Check log directory
            log_dir = Path("logs")
            if not log_dir.exists():
                result["status"] = "warning"
                result["issues"].append("Log directory not found")
                result["recommendations"].append(
                    "Log directory will be created when needed"
                )
            else:
                log_files = list(log_dir.glob("*.log"))
                result["details"] = f"Log files: {len(log_files)} found"

            # Test logger functionality
            if not hasattr(self.assistant, "logger"):
                result["status"] = "warning"
                result["issues"].append("Main logger not found")
                result["recommendations"].append("Logging may be limited")

            # Check API logger
            if not hasattr(self.assistant, "api_logger"):
                result["status"] = "warning"
                result["issues"].append("API logger not found")
                result["recommendations"].append("API logging may be limited")

        except Exception as e:
            result["status"] = "error"
            result["issues"].append(f"Logging system check failed: {str(e)}")

        return result

    def _check_session_management(self) -> dict[str, Any]:
        """Check session management capabilities."""
        result = {"status": "healthy", "issues": [], "recommendations": []}

        try:
            # Check session directory
            session_dir = Path("sessions")
            if not session_dir.exists():
                result["status"] = "warning"
                result["issues"].append("Session directory not found")
                result["recommendations"].append(
                    "Session directory will be created when needed"
                )
            else:
                session_files = list(session_dir.glob("*.json"))
                result["details"] = f"Session files: {len(session_files)} found"

            # Test session export functionality
            if not hasattr(self.assistant, "conversation_history"):
                result["status"] = "error"
                result["issues"].append("Cannot access conversation history")
                return result

            # Test JSON serialization
            try:
                test_data = {"test": "session_data"}
                json.dumps(test_data)  # Test JSON functionality
            except Exception as e:
                result["status"] = "error"
                result["issues"].append(f"JSON serialization failed: {str(e)}")

        except Exception as e:
            result["status"] = "error"
            result["issues"].append(f"Session management check failed: {str(e)}")

        return result

    def _check_system_resources(self) -> dict[str, Any]:
        """Check system resources and performance."""
        result = {"status": "healthy", "issues": [], "recommendations": []}

        try:
            import psutil

            # Memory usage
            memory = psutil.virtual_memory()
            memory_percent = memory.percent
            result["details"] = f"Memory usage: {memory_percent:.1f}%"

            if memory_percent > 90:
                result["status"] = "warning"
                result["issues"].append("High memory usage")
                result["recommendations"].append(
                    "Consider clearing conversation history"
                )
            elif memory_percent > 95:
                result["status"] = "error"
                result["issues"].append("Critical memory usage")
                result["recommendations"].append("Restart application recommended")

            # Disk space
            disk = psutil.disk_usage(".")
            disk_percent = (disk.used / disk.total) * 100
            result["details"] += f", Disk usage: {disk_percent:.1f}%"

            if disk_percent > 90:
                result["status"] = "warning"
                result["issues"].append("Low disk space")
                result["recommendations"].append("Clean up old logs and sessions")

        except ImportError:
            result["status"] = "warning"
            result["issues"].append("psutil not available for resource monitoring")
            result["recommendations"].append(
                "Install psutil for detailed resource monitoring"
            )
        except Exception as e:
            result["status"] = "warning"
            result["issues"].append(f"Resource check failed: {str(e)}")

        return result

    def _display_health_check_results(self, results: dict[str, Any]):
        """Display health check results in a formatted way."""
        print(f"\nüìä OVERALL STATUS: {results['overall_status'].upper()}")
        print("-" * 60)

        status_icons = {"healthy": "‚úÖ", "warning": "‚ö†Ô∏è", "error": "‚ùå"}

        for check_name, check_result in results["checks"].items():
            icon = status_icons.get(check_result["status"], "‚ùì")
            print(
                f"{icon} {check_name.replace('_', ' ').title()}: {check_result['status']}"
            )

            if check_result.get("details"):
                print(f"   {check_result['details']}")

            if check_result.get("issues"):
                for issue in check_result["issues"]:
                    print(f"   ‚ö†Ô∏è  {issue}")

        if results["issues_found"]:
            print(f"\nüö® ISSUES FOUND ({len(results['issues_found'])}):")
            for issue in results["issues_found"]:
                print(f"   ‚Ä¢ {issue}")

        if results["recommendations"]:
            print(f"\nüí° RECOMMENDATIONS ({len(results['recommendations'])}):")
            for rec in results["recommendations"]:
                print(f"   ‚Ä¢ {rec}")

        print("\n" + "=" * 80)

    def _attempt_auto_recovery(self, failed_checks: list[str]) -> dict[str, Any]:
        """Attempt automatic recovery for failed systems."""
        recovery_results = {}

        for check_name in failed_checks:
            try:
                if check_name == "openai_connection":
                    recovery_results[check_name] = self._recover_openai_connection()
                elif check_name == "memory_system":
                    recovery_results[check_name] = self._recover_memory_system()
                elif check_name == "tool_system":
                    recovery_results[check_name] = self._recover_tool_system()
                elif check_name == "logging_system":
                    recovery_results[check_name] = self._recover_logging_system()
                elif check_name == "session_management":
                    recovery_results[check_name] = self._recover_session_management()
                else:
                    recovery_results[check_name] = {
                        "success": False,
                        "message": "No recovery method available",
                    }
            except Exception as e:
                recovery_results[check_name] = {
                    "success": False,
                    "message": f"Recovery failed: {str(e)}",
                }

        # Display recovery results
        print("\nüîß RECOVERY RESULTS:")
        for check_name, result in recovery_results.items():
            status = "‚úÖ Success" if result["success"] else "‚ùå Failed"
            print(f"   {check_name}: {status}")
            if not result["success"] and result.get("message"):
                print(f"      {result['message']}")

        return recovery_results

    def _recover_openai_connection(self) -> dict[str, Any]:
        """Attempt to recover OpenAI connection."""
        try:
            # Try to reinitialize OpenAI client if API key is available
            if (
                hasattr(self.assistant, "openai_api_key")
                and self.assistant.openai_api_key
            ):
                self.assistant.client = OpenAI(api_key=self.assistant.openai_api_key)
                # Test connection
                models = self.assistant.client.models.list()
                return {"success": True, "message": "OpenAI connection restored"}
            else:
                return {
                    "success": False,
                    "message": "No API key available for recovery",
                }
        except Exception as e:
            return {"success": False, "message": f"Recovery failed: {str(e)}"}

    def _recover_memory_system(self) -> dict[str, Any]:
        """Attempt to recover memory system."""
        try:
            if not hasattr(self.assistant, "personality_memory"):
                self.assistant.personality_memory = {}
            if not hasattr(self.assistant, "conversation_history"):
                self.assistant.conversation_history = []
            return {"success": True, "message": "Memory system reinitialized"}
        except Exception as e:
            return {"success": False, "message": f"Recovery failed: {str(e)}"}

    def _recover_tool_system(self) -> dict[str, Any]:
        """Attempt to recover tool system."""
        try:
            # Reinitialize tool manager
            if hasattr(self.assistant, "tool_manager"):
                self.assistant.tool_manager = RuntimeToolManager(self.assistant)
            return {"success": True, "message": "Tool system reinitialized"}
        except Exception as e:
            return {"success": False, "message": f"Recovery failed: {str(e)}"}

    def _recover_logging_system(self) -> dict[str, Any]:
        """Attempt to recover logging system."""
        try:
            # Create log directory
            log_dir = Path("logs")
            log_dir.mkdir(exist_ok=True)
            return {"success": True, "message": "Logging system restored"}
        except Exception as e:
            return {"success": False, "message": f"Recovery failed: {str(e)}"}

    def _recover_session_management(self) -> dict[str, Any]:
        """Attempt to recover session management."""
        try:
            # Create session directory
            session_dir = Path("sessions")
            session_dir.mkdir(exist_ok=True)
            return {"success": True, "message": "Session management restored"}
        except Exception as e:
            return {"success": False, "message": f"Recovery failed: {str(e)}"}

    def get_diagnostic_history(self, limit: int = 5) -> list[dict[str, Any]]:
        """Get recent diagnostic history."""
        return self.diagnostic_history[-limit:]

    def enable_auto_recovery(self):
        """Enable automatic recovery."""
        self.auto_recovery_enabled = True
        print("‚úÖ Auto-recovery enabled")

    def disable_auto_recovery(self):
        """Disable automatic recovery."""
        self.auto_recovery_enabled = False
        print("‚ùå Auto-recovery disabled")


class RuntimeToolManager:
    """Manages user-defined runtime tools for the Echoes Assistant."""

    def __init__(self, assistant_instance):
        self.assistant = assistant_instance
        self.user_tools = {}
        self.tools_dir = Path("user_tools")
        self.tools_dir.mkdir(exist_ok=True)

        # Setup tool logger
        self.logger = logging.getLogger("RuntimeTools")

    def add_tool(self, name: str, description: str, function_code: str) -> bool:
        """Add a user-defined tool at runtime."""

        try:
            # Create safe execution environment
            tool_namespace = {
                "assistant": self.assistant,
                "np": np,
                "json": json,
                "datetime": datetime,
                "time": time,
                "logger": self.logger,
            }

            # Execute the function code
            exec(function_code, tool_namespace)

            # Extract the function from namespace
            if name not in tool_namespace or not callable(tool_namespace[name]):
                print(f"‚ùå Function '{name}' not found in provided code")
                return False

            tool_func = tool_namespace[name]

            # Store tool with metadata
            self.user_tools[name] = {
                "function": tool_func,
                "description": description,
                "code": function_code,
                "created_at": datetime.now().isoformat(),
                "usage_count": 0,
            }

            # Save tool to file
            self._save_tool_to_file(name, description, function_code)

            print(f"‚úÖ Tool '{name}' added successfully!")
            print(f"   Description: {description}")
            print(f"   Usage: call {name} [arguments]")

            self.logger.info(f"User tool '{name}' added")
            return True

        except Exception as e:
            print(f"‚ùå Failed to add tool '{name}': {e}")
            self.logger.error(f"Tool addition failed: {e}")
            return False

    def execute_tool(self, name: str, *args, **kwargs) -> Any:
        """Execute a user-defined tool."""

        if name not in self.user_tools:
            print(f"‚ùå Tool '{name}' not found")
            return None

        try:
            tool = self.user_tools[name]
            result = tool["function"](*args, **kwargs)

            # Update usage statistics
            tool["usage_count"] += 1

            # Log execution
            self.logger.info(f"Tool '{name}' executed successfully")

            return result

        except Exception as e:
            print(f"‚ùå Tool execution failed: {e}")
            self.logger.error(f"Tool execution failed: {e}")
            return None

    def list_tools(self) -> dict[str, dict[str, Any]]:
        """List all available user tools."""

        return {
            name: {
                "description": tool["description"],
                "created_at": tool["created_at"],
                "usage_count": tool["usage_count"],
            }
            for name, tool in self.user_tools.items()
        }

    def remove_tool(self, name: str) -> bool:
        """Remove a user-defined tool."""

        if name not in self.user_tools:
            print(f"‚ùå Tool '{name}' not found")
            return False

        try:
            # Remove from memory
            del self.user_tools[name]

            # Remove file
            tool_file = self.tools_dir / f"{name}.py"
            if tool_file.exists():
                tool_file.unlink()

            print(f"‚úÖ Tool '{name}' removed successfully")
            self.logger.info(f"User tool '{name}' removed")
            return True

        except Exception as e:
            print(f"‚ùå Failed to remove tool '{name}': {e}")
            self.logger.error(f"Tool removal failed: {e}")
            return False

    def _save_tool_to_file(self, name: str, description: str, code: str):
        """Save tool code to file for persistence."""

        tool_file = self.tools_dir / f"{name}.py"

        header = f'''"""
User-defined tool: {name}
Description: {description}
Created: {datetime.now().isoformat()}
"""

'''

        with open(tool_file, "w") as f:
            f.write(header + code)

    def load_tools_from_directory(self):
        """Load existing tools from the tools directory."""

        if not self.tools_dir.exists():
            return

        for tool_file in self.tools_dir.glob("*.py"):
            try:
                with open(tool_file) as f:
                    content = f.read()

                # Extract tool name from filename
                tool_name = tool_file.stem

                # Extract description from header
                lines = content.split("\n")
                description = "User-defined tool"
                for line in lines:
                    if line.strip().startswith("Description:"):
                        description = line.replace("Description:", "").strip()
                        break

                # Find the function code (skip header)
                code_start = 0
                for i, line in enumerate(lines):
                    if line.strip() == '"""' and i > 0:
                        code_start = i + 1
                        break

                function_code = "\n".join(lines[code_start:])

                # Load the tool
                if self.add_tool(tool_name, description, function_code):
                    print(f"üìù Loaded existing tool: {tool_name}")

            except Exception as e:
                self.logger.error(f"Failed to load tool {tool_file}: {e}")


class EnhancedCLI:
    """Enhanced CLI with autocomplete, history, and context visualization."""

    def __init__(self, assistant_instance):
        self.assistant = assistant_instance
        self.session_dir = Path("sessions")
        self.session_dir.mkdir(exist_ok=True)

        # Initialize advanced systems
        self.conversational_autocomplete = ConversationalAutocomplete(
            assistant_instance
        )
        self.history_manager = AdvancedHistoryManager(assistant_instance)
        self.visual_context = VisualContextManager(assistant_instance)
        self.api_dashboard = APILoggingDashboard()
        self.self_diagnosis = SelfDiagnosisAndRecovery(assistant_instance)
        self.multimodal_memory = MultimodalMemoryManager(assistant_instance)

        # Initialize runtime tool manager
        self.tool_manager = RuntimeToolManager(assistant_instance)

        # Command definitions for autocomplete
        self.commands = {
            "basic": ["help", "stats", "reset", "train", "exit", "quit", "bye"],
            "openai": [
                "enable openai",
                "disable openai",
                "set model",
                "refresh models",
            ],
            "dynamic": [
                "enable dynamic",
                "disable dynamic",
                "enable cost",
                "disable cost",
            ],
            "context": [
                "show context",
                "show history",
                "show memory",
                "export session",
                "import session",
                "show visual context",
            ],
            "logs": [
                "show logs",
                "clear logs",
                "export logs",
                "api dashboard",
                "export api logs",
            ],
            "tools": ["list tools", "add tool", "remove tool", "tool info", "call"],
            "history": [
                "search history",
                "filter history",
                "threaded view",
                "jump unanswered",
                "add bookmark",
                "add tag",
            ],
            "diagnosis": [
                "health check",
                "diagnostic history",
                "enable auto recovery",
                "disable auto recovery",
            ],
            "memory": [
                "add attachment",
                "search attachments",
                "list attachments",
                "memory stats",
                "remove attachment",
            ],
            "filesystem": [
                "read_file",
                "write_file",
                "edit_file",
                "remove_file",
                "grep_file",
                "list_files",
                "create_directory",
                "remove_directory",
                "move_file",
                "copy_file",
                "get_file_info",
                "change_directory",
                "current_directory",
                "parent_directory",
                "file_exists",
            ],
            "editor": [
                "open_in_editor",
                "save_file",
                "save_as",
                "close_editor",
                "find_in_file",
                "replace_in_file",
                "goto_line",
                "format_code",
                "check_syntax",
                "run_linter",
            ],
            "cross_platform": [
                "edit_with_nano",
                "edit_with_vim",
                "edit_with_notepad",
                "open_with_default",
                "run_python_script",
                "execute_command",
                "set_executable",
                "get_file_permissions",
                "change_permissions",
                "open_terminal",
                "run_shell_command",
            ],
            "custom_functions": [
                "create_function",
                "call_function",
                "list_user_functions",
                "delete_function",
                "function_help",
                "save_function",
                "load_function",
                "test_function",
                "debug_function",
                "profile_function",
                "export_functions",
            ],
            "codebase": [
                "show functions",
                "list classes",
                "find definitions",
                "show imports",
                "analyze dependencies",
                "code documentation",
                "function signatures",
                "class hierarchy",
                "module structure",
                "api endpoints",
            ],
        }

        # Flatten all commands for completer
        self.all_commands = []
        for category, cmds in self.commands.items():
            self.all_commands.extend(cmds)

        # Initialize enhanced CLI components
        if ENHANCED_CLI_AVAILABLE:
            self.completer = WordCompleter(self.all_commands, ignore_case=True)
            self.history = FileHistory(str(self.session_dir / "cli_history"))
            self.key_bindings = self._create_key_bindings()

        # Setup logging
        self._setup_logging()

    def _create_key_bindings(self):
        """Create custom key bindings for enhanced navigation."""
        kb = KeyBindings()

        @kb.add("c-c")
        def _(event):
            """Handle Ctrl+C gracefully."""
            event.app.exit()

        @kb.add("f1")
        def _(event):
            """Show context help."""
            self._show_context_help()

        return kb

    def _setup_logging(self):
        """Setup structured logging for API calls and errors."""
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)

        # Configure logging
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
            handlers=[
                logging.FileHandler(log_dir / "echoes.log"),
                logging.StreamHandler(),
            ],
        )

        self.logger = logging.getLogger("EchoesAssistant")

        # Create API call logger
        self.api_logger = logging.getLogger("EchoesAPI")
        api_handler = logging.FileHandler(log_dir / "api_calls.log")
        api_handler.setFormatter(logging.Formatter("%(asctime)s - %(message)s"))
        self.api_logger.addHandler(api_handler)
        self.api_logger.setLevel(logging.INFO)

    def get_user_input(self, prompt_text="üí¨ You: "):
        """Get user input with enhanced CLI features and dynamic suggestions."""
        if ENHANCED_CLI_AVAILABLE:
            try:
                # Get conversation context for suggestions
                conversation_context = [
                    msg.content for msg in self.assistant.conversation_history[-5:]
                ]

                # Create dynamic completer
                dynamic_suggestions = (
                    self.conversational_autocomplete.get_dynamic_suggestions(
                        "", conversation_context
                    )
                )
                all_suggestions = self.all_commands + dynamic_suggestions
                completer = FuzzyWordCompleter(list(set(all_suggestions)))

                return prompt(
                    prompt_text,
                    completer=completer,
                    history=self.history,
                    key_bindings=self.key_bindings,
                    multiline=False,
                    auto_suggest=AutoSuggestFromHistory(),
                )
            except KeyboardInterrupt:
                return "\x03"  # Ctrl+C
            except EOFError:
                return "exit"
        else:
            # Fallback to basic input
            return input(prompt_text).strip()

    def show_context_visualization(self):
        """Display current context, memory, and conversation state."""
        stats = self.assistant.get_stats()

        print("\n" + "=" * 60)
        print("üß† ECHOES CONTEXT VISUALIZATION")
        print("=" * 60)

        # Conversation Overview
        print("\nüìä CONVERSATION OVERVIEW:")
        print(f"   Total Messages: {len(self.assistant.conversation_history)}")
        print(f"   Session ID: {stats['session_id']}")
        print(f"   Intelligence Source: {stats['intelligence_source']}")
        print(f"   Current Model: {stats['current_model']}")

        # Personality & Memory
        personality = stats["personality_insights"]
        print("\nüé® PERSONALITY & MEMORY:")
        print(f"   Dominant Style: {personality['dominant_personality']}")
        print(f"   Preferred Domain: {personality['preferred_domain']}")
        print(f"   Domains Explored: {personality['total_domains_explored']}")
        print(f"   Support Sessions: {personality['emotional_support_sessions']}")

        # Recent Conversation (last 5 messages)
        print("\nüí¨ RECENT CONVERSATION:")
        recent_messages = self.assistant.conversation_history[-5:]
        for i, msg in enumerate(recent_messages, 1):
            role_icon = "üë§" if msg.role == "user" else "ü§ñ"
            content_preview = (
                msg.content[:100] + "..." if len(msg.content) > 100 else msg.content
            )
            print(f"   {i}. {role_icon} {content_preview}")

        # Platform Integration Status
        if stats["platform_integration"]["platform_sync"] == "active":
            print("\nüåê PLATFORM INTEGRATION:")
            print(
                f"   Available Models: {stats['platform_integration']['total_available_models']}"
            )
            print(
                f"   Last Refresh: {datetime.fromtimestamp(stats['platform_integration']['last_model_refresh'] or 0)}"
            )

        # Context Window Info
        if hasattr(self.assistant, "_last_analysis") and self.assistant._last_analysis:
            analysis = self.assistant._last_analysis
            print("\nüîç LAST ANALYSIS:")
            print(f"   Detected Domain: {analysis.get('domain', 'unknown')}")
            print(f"   Complexity: {analysis.get('complexity', 'unknown')}")
            print(f"   Personality: {analysis.get('personality', 'unknown')}")
            print(
                f"   Recommended Model: {analysis.get('recommended_model', 'unknown')}"
            )
            print(f"   Confidence: {analysis.get('confidence', 0):.2f}")

        print("\n" + "=" * 60)

    def export_session(self, filename=None):
        """Export current session state to file."""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"echoes_session_{timestamp}.json"

        session_data = {
            "timestamp": datetime.now().isoformat(),
            "session_id": self.assistant.session_id,
            "conversation_history": [
                {
                    "role": msg.role,
                    "content": msg.content,
                    "timestamp": msg.timestamp if hasattr(msg, "timestamp") else None,
                }
                for msg in self.assistant.conversation_history
            ],
            "stats": self.assistant.get_stats(),
            "openai_settings": {
                "enabled": self.assistant.openai_enabled,
                "model_preference": self.assistant.model_preference,
                "dynamic_switching": self.assistant.dynamic_model_switching,
                "cost_optimization": self.assistant.cost_optimization,
            },
            "last_analysis": getattr(self.assistant, "_last_analysis", {}),
            "personality_memory": getattr(self.assistant, "personality_memory", {}),
            "emotional_history": getattr(self.assistant, "emotional_history", []),
        }

        filepath = self.session_dir / filename
        with open(filepath, "w") as f:
            json.dump(session_data, f, indent=2)

        print(f"‚úÖ Session exported to: {filepath}")
        self.logger.info(f"Session exported to {filepath}")
        return str(filepath)

    def import_session(self, filename):
        """Import session state from file."""
        filepath = self.session_dir / filename
        if not filepath.exists():
            print(f"‚ùå Session file not found: {filepath}")
            return False

        try:
            with open(filepath) as f:
                session_data = json.load(f)

            # Restore conversation history
            self.assistant.conversation_history = []
            for msg_data in session_data.get("conversation_history", []):
                msg = ChatMessage(role=msg_data["role"], content=msg_data["content"])
                if msg_data.get("timestamp"):
                    msg.timestamp = datetime.fromisoformat(msg_data["timestamp"])
                self.assistant.conversation_history.append(msg)

            # Restore settings
            openai_settings = session_data.get("openai_settings", {})
            self.assistant.dynamic_model_switching = openai_settings.get(
                "dynamic_switching", True
            )
            self.assistant.cost_optimization = openai_settings.get(
                "cost_optimization", True
            )

            # Restore memory
            if "last_analysis" in session_data:
                self.assistant._last_analysis = session_data["last_analysis"]
            if "personality_memory" in session_data:
                self.assistant.personality_memory = session_data["personality_memory"]
            if "emotional_history" in session_data:
                self.assistant.emotional_history = session_data["emotional_history"]

            print(f"‚úÖ Session imported from: {filepath}")
            print(f"   Restored {len(self.assistant.conversation_history)} messages")
            print(f"   Session ID: {session_data.get('session_id', 'unknown')}")

            self.logger.info(f"Session imported from {filepath}")
            return True

        except Exception as e:
            print(f"‚ùå Failed to import session: {e}")
            self.logger.error(f"Session import failed: {e}")
            return False

    def show_logs(self, lines=20):
        """Display recent API logs."""
        log_file = Path("logs") / "api_calls.log"
        if not log_file.exists():
            print("üìù No logs found.")
            return

        try:
            with open(log_file) as f:
                log_lines = f.readlines()

            print(f"\nüìù RECENT API LOGS (last {lines} lines):")
            print("-" * 60)
            for line in log_lines[-lines:]:
                print(line.strip())
            print("-" * 60)

        except Exception as e:
            print(f"‚ùå Failed to read logs: {e}")

    def _show_context_help(self):
        """Show context-sensitive help."""
        print("\nüìö CONTEXT HELP:")
        print("Available commands:")
        for category, cmds in self.commands.items():
            print(f"  {category.upper()}: {', '.join(cmds)}")
        print("\nPress F1 anytime to show this help.")
        print("Use Tab to autocomplete commands.")
        print("Use ‚Üë/‚Üì arrows to navigate history.")


# ============================================================================
# Core Data Models
# ============================================================================


@dataclass
class ChatMessage:
    """Chat message with metadata."""

    role: str  # 'user', 'assistant', 'system'
    content: str
    timestamp: str
    message_id: str


@dataclass
class ConversationContext:
    """Conversation context for continuity."""

    session_id: str
    messages: list[ChatMessage]
    context_summary: str
    last_updated: str


class ContextAnalyzer:
    """Analyzes conversation context to determine optimal model selection with personality and empathy."""

    def __init__(self):
        # Enhanced keywords and patterns for different domains
        self.domain_keywords = {
            "scientific": [
                "experiment",
                "hypothesis",
                "research",
                "scientific",
                "biology",
                "chemistry",
                "physics",
                "quantum",
                "molecular",
                "genetic",
                "analysis",
                "methodology",
                "peer review",
                "empirical",
                "theory",
                "laboratory",
                "algorithm",
                "computation",
                "simulation",
                "data",
                "statistics",
                "prove",
                "demonstrate",
                "validate",
                "test",
                "measure",
                "observe",
            ],
            "mathematical": [
                "calculate",
                "equation",
                "formula",
                "theorem",
                "proof",
                "algebra",
                "calculus",
                "geometry",
                "statistics",
                "probability",
                "integral",
                "derivative",
                "matrix",
                "vector",
                "function",
                "variable",
                "constant",
                "solve",
                "compute",
                "optimize",
                "maximize",
                "minimize",
                "graph",
            ],
            "virtualization": [
                "simulate",
                "virtual",
                "model",
                "environment",
                "scenario",
                "sandbox",
                "emulation",
                "digital twin",
                "virtualization",
                "container",
                "vm",
                "simulation",
                "modeling",
                "prototype",
                "virtual reality",
                "augmented",
                "system",
                "architecture",
                "framework",
                "platform",
                "infrastructure",
            ],
            "complex_reasoning": [
                "analyze",
                "reason",
                "logic",
                "critical thinking",
                "problem solving",
                "strategy",
                "optimization",
                "decision making",
                "complex",
                "intricate",
                "nuanced",
                "multifaceted",
                "systematic",
                "comprehensive",
                "evaluate",
                "compare",
                "contrast",
                "synthesize",
                "integrate",
                "perspective",
            ],
            "emotional_support": [
                "feel",
                "feeling",
                "sad",
                "happy",
                "angry",
                "worried",
                "anxious",
                "depressed",
                "stressed",
                "overwhelmed",
                "confused",
                "lost",
                "help",
                "advice",
                "guidance",
                "support",
                "comfort",
                "understand",
                "listen",
                "empathy",
                "caring",
                "concern",
                "emotional",
                "mental health",
            ],
            "creative": [
                "create",
                "design",
                "write",
                "compose",
                "paint",
                "draw",
                "invent",
                "imagine",
                "brainstorm",
                "story",
                "poem",
                "art",
                "music",
                "novel",
                "inspire",
                "innovate",
                "original",
                "unique",
                "artistic",
                "expressive",
            ],
            "personal_development": [
                "learn",
                "grow",
                "improve",
                "develop",
                "skill",
                "habit",
                "goal",
                "motivation",
                "productivity",
                "focus",
                "discipline",
                "routine",
                "career",
                "success",
                "failure",
                "challenge",
                "overcome",
                "achieve",
            ],
        }

        # Enhanced complexity indicators
        self.complexity_indicators = {
            "high": [
                "explain in detail",
                "comprehensive analysis",
                "deep dive",
                "thorough",
                "exhaustive",
                "complete",
                "step by step",
                "detailed explanation",
                "in depth",
                "extensive",
                "systematic",
                "methodical",
                "academic",
                "research",
                "scholarly",
                "technical",
                "advanced",
                "sophisticated",
            ],
            "medium": [
                "explain",
                "describe",
                "analyze",
                "compare",
                "evaluate",
                "discuss",
                "how to",
                "what is",
                "why does",
                "when should",
                "where can",
                "who",
            ],
            "low": [
                "what",
                "how",
                "quick",
                "simple",
                "basic",
                "overview",
                "summary",
                "brief",
                "short",
                "quickly",
                "simply",
                "basics",
                "fundamentals",
            ],
        }

        # Personality indicators
        self.personality_indicators = {
            "analytical": [
                "logic",
                "data",
                "facts",
                "evidence",
                "analysis",
                "statistics",
                "research",
                "study",
                "experiment",
                "prove",
                "verify",
                "objective",
            ],
            "creative": [
                "imagine",
                "create",
                "design",
                "innovate",
                "artistic",
                "expressive",
                "colorful",
                "vibrant",
                "inspire",
                "original",
                "unique",
                "vision",
            ],
            "practical": [
                "practical",
                "realistic",
                "actionable",
                "useful",
                "applicable",
                "hands on",
                "step by step",
                "implementation",
                "execution",
                "results",
            ],
            "philosophical": [
                "meaning",
                "purpose",
                "why",
                "existence",
                "consciousness",
                "ethics",
                "morality",
                "philosophy",
                "wisdom",
                "truth",
                "knowledge",
                "understanding",
            ],
            "social": [
                "people",
                "relationship",
                "team",
                "collaborate",
                "communicate",
                "connect",
                "community",
                "social",
                "interpersonal",
                "empathy",
                "support",
            ],
        }

        # Memory and context tracking
        self.conversation_patterns = {}
        self.user_preferences = {}
        self.emotional_state = "neutral"

    def analyze_prompt(
        self, message: str, conversation_history: list[ChatMessage]
    ) -> dict[str, Any]:
        """Enhanced prompt analysis with personality, memory, and empathy."""

        message_lower = message.lower()

        # Detect domain with enhanced scoring
        detected_domain = self._detect_domain(message_lower)

        # Assess complexity with nuance
        complexity = self._assess_complexity(message_lower)

        # Analyze personality indicators
        personality = self._detect_personality(message_lower)

        # Detect emotional state and empathy needs
        emotional_analysis = self._analyze_emotional_content(message_lower)

        # Check conversation context and memory
        context_depth = len(conversation_history)
        is_followup = self._is_followup_question(message_lower, conversation_history)

        # Update conversation patterns for memory
        self._update_conversation_patterns(
            detected_domain, personality, emotional_analysis
        )

        # Determine optimal model with enhanced logic
        recommended_model = self._recommend_model_enhanced(
            detected_domain,
            complexity,
            personality,
            emotional_analysis,
            context_depth,
            is_followup,
        )

        return {
            "domain": detected_domain,
            "complexity": complexity,
            "personality": personality,
            "emotional_analysis": emotional_analysis,
            "context_depth": context_depth,
            "is_followup": is_followup,
            "recommended_model": recommended_model,
            "confidence": self._calculate_confidence_enhanced(
                detected_domain, complexity, personality, emotional_analysis
            ),
            "memory_context": self._get_memory_context(),
        }

    def _detect_domain(self, message: str) -> str:
        """Enhanced domain detection with weighted scoring."""

        domain_scores = {}
        for domain, keywords in self.domain_keywords.items():
            # Weight exact matches higher than partial matches
            exact_matches = sum(
                1 for keyword in keywords if f" {keyword} " in f" {message} "
            )
            partial_matches = sum(1 for keyword in keywords if keyword in message)

            # Calculate weighted score
            domain_scores[domain] = (exact_matches * 2 + partial_matches * 1) / len(
                keywords
            )

        if max(domain_scores.values()) < 0.1:  # Threshold for "no clear domain"
            return "general"

        return max(domain_scores, key=domain_scores.get)

    def _assess_complexity(self, message: str) -> str:
        """Assess the complexity of the user's message."""

        complexity_indicators = {
            "low": [
                "simple",
                "basic",
                "easy",
                "quick",
                "what is",
                "tell me",
                "explain",
            ],
            "medium": [
                "how",
                "why",
                "compare",
                "analyze",
                "discuss",
                "evaluate",
                "example",
            ],
            "high": [
                "complex",
                "advanced",
                "detailed",
                "comprehensive",
                "in-depth",
                "multi-step",
                "intricate",
                "nuanced",
                "sophisticated",
            ],
            "expert": [
                "research",
                "thesis",
                "paper",
                "academic",
                "scientific",
                "mathematical proof",
                "algorithm",
                "optimization",
            ],
        }

        complexity_scores = {}
        for level, indicators in complexity_indicators.items():
            score = sum(1 for indicator in indicators if indicator in message)
            complexity_scores[level] = score

        # Check for specific complexity patterns
        question_count = message.count("?")
        word_count = len(message.split())

        # Adjust based on message characteristics
        if word_count > 50 or question_count > 3:
            complexity_scores["high"] += 1
        elif word_count > 100:
            complexity_scores["expert"] += 1
        elif word_count < 10:
            complexity_scores["low"] += 1

        # Check for semantic complexity indicators
        complex_keywords = [
            "quantum",
            "algorithm",
            "mathematical",
            "theoretical",
            "computational",
            "statistical",
            "probabilistic",
            "cryptographic",
            "optimization",
            "machine learning",
            "artificial intelligence",
            "neural network",
            "blockchain",
            "distributed",
            "scalability",
            "architecture",
        ]

        detail_keywords = [
            "detail",
            "detailed",
            "comprehensive",
            "thorough",
            "in-depth",
            "explain",
            "analyze",
            "breakdown",
            "elaborate",
            "expand",
        ]

        # Check for complex topics
        if any(keyword.lower() in message.lower() for keyword in complex_keywords):
            complexity_scores["high"] += 2  # Increase weight for complex topics
            complexity_scores["medium"] += 1
            # Force high complexity for scientific topics with detail requests
            if any(keyword.lower() in message.lower() for keyword in detail_keywords):
                complexity_scores["high"] += 2
                complexity_scores["low"] = max(0, complexity_scores["low"] - 1)

        # Check for detail requests
        if any(keyword.lower() in message.lower() for keyword in detail_keywords):
            complexity_scores["medium"] += 1
            if word_count < 20:  # Short but wants detail
                complexity_scores["high"] += 1

        return max(complexity_scores, key=complexity_scores.get)

    def _detect_personality(self, message: str) -> str:
        """Detect user's personality style from message."""

        personality_scores = {}
        for personality, indicators in self.personality_indicators.items():
            score = sum(1 for indicator in indicators if indicator in message)
            personality_scores[personality] = score

        if max(personality_scores.values()) == 0:
            return "balanced"

        return max(personality_scores, key=personality_scores.get)

    def _is_followup_question(
        self, message: str, conversation_history: list[ChatMessage]
    ) -> bool:
        """Determine if the current message is a follow-up question."""

        if not conversation_history:
            return False

        # Check for follow-up indicators
        followup_indicators = [
            "what about",
            "how about",
            "and",
            "also",
            "additionally",
            "furthermore",
            "moreover",
            "but",
            "however",
            "what if",
            "can you",
            "could you",
            "would you",
            "should i",
        ]

        message_lower = message.lower()
        has_followup_indicator = any(
            indicator in message_lower for indicator in followup_indicators
        )

        # Check if it references previous conversation
        last_message = conversation_history[-1].content.lower()
        shared_words = set(message_lower.split()) & set(last_message.split())
        references_previous = len(shared_words) > 2

        # Check for questions that continue previous topics
        is_question = "?" in message or any(
            word in message_lower for word in ["what", "how", "why", "when", "where"]
        )

        return has_followup_indicator or (references_previous and is_question)

    def _analyze_emotional_content(self, message: str) -> dict[str, Any]:
        """Analyze emotional content for empathy and support needs."""

        emotional_keywords = {
            "negative": [
                "sad",
                "angry",
                "frustrated",
                "worried",
                "anxious",
                "depressed",
                "stressed",
                "overwhelmed",
                "confused",
                "lost",
                "help",
                "problem",
            ],
            "positive": [
                "happy",
                "excited",
                "grateful",
                "optimistic",
                "proud",
                "confident",
                "successful",
                "achievement",
                "progress",
                "good",
                "great",
            ],
            "seeking_support": [
                "help",
                "advice",
                "guidance",
                "support",
                "comfort",
                "understand",
                "listen",
                "empathy",
                "suggestion",
            ],
            "seeking_information": [
                "learn",
                "understand",
                "know",
                "explain",
                "tell me",
                "what",
                "how",
                "why",
                "when",
                "where",
            ],
        }

        emotional_state = "neutral"
        support_needed = False
        empathy_level = 0.0

        for emotion, keywords in emotional_keywords.items():
            matches = sum(1 for keyword in keywords if keyword in message)
            if matches > 0:
                if emotion == "negative":
                    emotional_state = "concerned"
                    empathy_level = min(1.0, matches * 0.3)
                elif emotion == "positive":
                    emotional_state = "positive"
                elif emotion == "seeking_support":
                    support_needed = True
                    empathy_level = min(1.0, empathy_level + 0.4)

        return {
            "emotional_state": emotional_state,
            "support_needed": support_needed,
            "empathy_level": empathy_level,
            "emotional_keywords_found": matches if "matches" in locals() else 0,
        }

    def _update_conversation_patterns(
        self, domain: str, personality: str, emotional: dict[str, Any]
    ):
        """Update conversation patterns for memory and learning."""

        if domain not in self.conversation_patterns:
            self.conversation_patterns[domain] = {
                "count": 0,
                "personality": {},
                "emotional_states": [],
            }

        self.conversation_patterns[domain]["count"] += 1

        if personality not in self.conversation_patterns[domain]["personality"]:
            self.conversation_patterns[domain]["personality"][personality] = 0
        self.conversation_patterns[domain]["personality"][personality] += 1

        self.conversation_patterns[domain]["emotional_states"].append(
            emotional["emotional_state"]
        )

        # Update user preferences based on patterns
        self._update_user_preferences()

    def _update_user_preferences(self):
        """Learn user preferences from conversation patterns."""

        # Find most common domain
        domain_counts = {
            domain: data["count"] for domain, data in self.conversation_patterns.items()
        }
        if domain_counts:
            self.user_preferences["preferred_domain"] = max(
                domain_counts, key=domain_counts.get
            )

        # Find dominant personality style
        personality_counts = {}
        for domain_data in self.conversation_patterns.values():
            for personality, count in domain_data["personality"].items():
                personality_counts[personality] = (
                    personality_counts.get(personality, 0) + count
                )

        if personality_counts:
            self.user_preferences["dominant_personality"] = max(
                personality_counts, key=personality_counts.get
            )

    def _get_memory_context(self) -> dict[str, Any]:
        """Get memory context for personalization."""

        return {
            "conversation_patterns": self.conversation_patterns,
            "user_preferences": self.user_preferences,
            "total_domains_explored": len(self.conversation_patterns),
            "most_common_domain": self.user_preferences.get(
                "preferred_domain", "general"
            ),
            "dominant_personality": self.user_preferences.get(
                "dominant_personality", "balanced"
            ),
        }

    def _recommend_model_enhanced(
        self,
        domain: str,
        complexity: str,
        personality: str,
        emotional: dict[str, Any],
        context_depth: int,
        is_followup: bool,
    ) -> str:
        """Enhanced model recommendation with personality and emotional considerations."""

        # High empathy needs prioritize models with better emotional intelligence
        if emotional["empathy_level"] > 0.6:
            if complexity in ["high", "medium"]:
                return "gpt-4"  # Best for emotional support and complex reasoning
            else:
                return "gpt-3.5-turbo"  # Good for emotional support, cost effective

        # Creative personality gets more creative models
        if personality == "creative":
            if complexity == "high":
                return "gpt-4-turbo-preview"  # Latest creative capabilities
            else:
                return "gpt-3.5-turbo"  # Good creative support

        # Analytical personality gets precision models
        if personality == "analytical":
            if domain in ["scientific", "mathematical"]:
                return (
                    "gpt-4" if complexity == "high" else "gpt-3.5-turbo"
                )  # Avoid o1-mini due to system message incompatibility
            else:
                return "gpt-4" if complexity == "high" else "gpt-3.5-turbo"

        # Domain-specific recommendations (original logic preserved)
        if domain == "scientific":
            return (
                "gpt-4" if complexity == "high" else "gpt-3.5-turbo"
            )  # Avoid o1-mini due to system message incompatibility

        elif domain == "mathematical":
            return (
                "gpt-4" if complexity == "high" else "gpt-3.5-turbo"
            )  # Avoid o1-mini due to system message incompatibility

        elif domain == "virtualization":
            return (
                "gpt-4" if complexity == "high" else "gpt-3.5-turbo"
            )  # Use available models

        elif domain == "complex_reasoning":
            return "gpt-4" if complexity == "high" else "gpt-3.5-turbo"

        # Personal development and emotional support
        elif domain in ["emotional_support", "personal_development"]:
            return "gpt-4" if emotional["support_needed"] else "gpt-3.5-turbo"

        # Creative domain
        elif domain == "creative":
            return "gpt-4-turbo-preview" if complexity == "high" else "gpt-3.5-turbo"

        # General recommendations based on complexity
        if complexity == "high":
            return "gpt-4-turbo-preview"
        elif complexity == "medium":
            return "gpt-3.5-turbo"
        else:
            return "gpt-3.5-turbo"

    def _calculate_confidence_enhanced(
        self, domain: str, complexity: str, personality: str, emotional: dict[str, Any]
    ) -> float:
        """Enhanced confidence calculation considering personality and emotional factors."""

        base_confidence = 0.7

        # Higher confidence for clear domain matches
        if domain != "general":
            base_confidence += 0.2

        # Higher confidence for clear personality indicators
        if personality != "balanced":
            base_confidence += 0.1

        # Higher confidence for emotional clarity
        if emotional["empathy_level"] > 0.3:
            base_confidence += 0.1

        # Higher confidence for clear complexity indicators
        if complexity in ["high", "low"]:
            base_confidence += 0.1

        # Memory boost - confidence increases with pattern recognition
        if self.user_preferences.get("preferred_domain") == domain:
            base_confidence += 0.1

        return min(1.0, base_confidence)


# ============================================================================
# Advanced Assistant Implementation
# ============================================================================


class IntelligentAssistant:
    """
    Advanced AI Assistant with sophisticated capabilities while maintaining simplicity.
    """

    def __init__(self, openai_api_key: str | None = None):
        # Core initialization
        self.values = {"respect", "accuracy", "helpfulness", "clarity", "adaptability"}
        self.behaviors = []
        self.session_id = str(uuid.uuid4())

        # Advanced features
        self.conversation_history = []
        self.context_memory = {}
        self.learning_data = []
        self.tools_enabled = True

        # Performance metrics
        self.response_times = []
        self.success_rate = 0.0
        self.total_interactions = 0

        # OpenAI/ChatGPT Integration
        self.openai_enabled = False
        self.openai_client = None
        self.model_preference = "gpt-3.5-turbo"
        self.fallback_enabled = True

        # Dynamic model switching based on context
        self.dynamic_model_switching = True
        self.cost_optimization = True
        self.context_analyzer = ContextAnalyzer()

        # Personality and memory tracking
        self._last_analysis = {}
        self._last_used_model = None  # Track the actual model used
        self.personality_memory = {}
        self.emotional_history = []

        # OpenAI Platform Integration
        self.available_models = {}
        self.model_info_cache = {}
        self.last_model_refresh = None

        # Model capabilities mapping (will be updated dynamically)
        self.model_capabilities = {
            "gpt-3.5-turbo": {
                "strengths": ["general", "conversation", "quick_response"],
                "cost_tier": "low",
                "speed": "fast",
                "complexity": "basic",
            },
            "gpt-4": {
                "strengths": ["reasoning", "analysis", "complex_tasks"],
                "cost_tier": "high",
                "speed": "moderate",
                "complexity": "advanced",
            },
            "gpt-4-turbo-preview": {
                "strengths": ["latest", "performance", "balanced"],
                "cost_tier": "medium",
                "speed": "fast",
                "complexity": "advanced",
            },
            "o1-preview": {
                "strengths": ["scientific", "mathematical", "research"],
                "cost_tier": "very_high",
                "speed": "slow",
                "complexity": "expert",
            },
            "o1-mini": {
                "strengths": ["scientific", "mathematical", "cost_efficient"],
                "cost_tier": "medium",
                "speed": "moderate",
                "complexity": "intermediate",
            },
            "gpt-4o": {  # Add gpt-4o capabilities
                "strengths": ["general", "complex_reasoning", "multimodal"],
                "cost_tier": "high",
                "speed": "fast",
                "complexity": "expert",
            },
        }

        # Initialize OpenAI if available
        if OPENAI_AVAILABLE:
            self._initialize_openai(openai_api_key)

        print("üß† Echoes AI Assistant initialized")
        if self.openai_enabled:
            print(f"ü§ñ ChatGPT integration enabled with {self.model_preference}")
        else:
            print("‚ú® Using advanced local intelligence")
        print("üéØ Ready for intelligent conversation")

    def _initialize_openai(self, api_key: str | None = None):
        """Initialize OpenAI client with platform integration."""

        if not OPENAI_AVAILABLE:
            print("‚ö†Ô∏è OpenAI package not installed. Install with: pip install openai")
            return

        # Get API key from parameter or environment
        api_key = api_key or os.getenv("OPENAI_API_KEY")

        if not api_key:
            print(
                "‚ö†Ô∏è No OpenAI API key provided. Set OPENAI_API_KEY environment variable or use enable_openai command."
            )
            return

        try:
            # Initialize OpenAI client
            self.openai_client = OpenAI(api_key=api_key)
            self.openai_enabled = True

            # Fetch available models from OpenAI platform
            self._refresh_available_models()

            # Set best available model as default
            self._set_optimal_default_model()

            print(
                f"‚úÖ OpenAI connected. Available models: {', '.join(list(self.available_models.keys())[:5])}"
            )

        except Exception as e:
            print(f"‚ùå Failed to initialize OpenAI: {e}")
            self.openai_enabled = False

    def _refresh_available_models(self):
        """Fetch available models directly from OpenAI platform."""

        if not self.openai_client:
            return

        try:
            # Get models from OpenAI API
            models = self.openai_client.models.list()

            # Process and categorize models
            self.available_models = {}
            self.model_info_cache = {}

            for model in models.data:
                model_id = model.id

                # Filter for chat models we want to use
                if self._is_chat_model(model_id):
                    # Store basic model info
                    self.model_info_cache[model_id] = {
                        "id": model_id,
                        "created": model.created,
                        "owned_by": model.owned_by,
                        "object": model.object,
                    }

                    # Determine model capabilities based on model ID
                    capabilities = self._infer_model_capabilities(model_id)
                    self.available_models[model_id] = capabilities
                    self.model_capabilities[model_id] = capabilities

            self.last_model_refresh = time.time()
            print(
                f"üîÑ Refreshed {len(self.available_models)} models from OpenAI platform"
            )

        except Exception as e:
            print(f"‚ö†Ô∏è Failed to refresh models from OpenAI: {e}")
            # Keep existing models as fallback

    def _is_chat_model(self, model_id: str) -> bool:
        """Determine if model is suitable for chat completion."""

        # Include GPT models, O1, O3 series
        chat_patterns = [
            "gpt-3.5",
            "gpt-4",
            "gpt-4o",
            "gpt-4-turbo",
            "o1-",
            "o3-",
            "chatgpt-",
            "gpt-",
        ]

        # Exclude fine-tuned models and older versions
        exclude_patterns = [
            "-fine",
            "-tuned",
            "-instruct",
            "davinci",
            "curie",
            "babbage",
            "ada",
        ]

        model_lower = model_id.lower()

        # Check if it matches chat patterns
        is_chat = any(pattern in model_lower for pattern in chat_patterns)

        # Check if it should be excluded
        should_exclude = any(pattern in model_lower for pattern in exclude_patterns)

        return is_chat and not should_exclude

    def _infer_model_capabilities(self, model_id: str) -> dict[str, Any]:
        """Infer model capabilities based on model ID pattern."""

        model_lower = model_id.lower()

        # Default capabilities
        capabilities = {
            "strengths": ["general"],
            "cost_tier": "medium",
            "speed": "moderate",
            "complexity": "basic",
        }

        # GPT-4 models
        if "gpt-4" in model_lower:
            if "turbo" in model_lower or "4o" in model_lower:
                capabilities.update(
                    {
                        "strengths": ["latest", "performance", "balanced"],
                        "cost_tier": "medium",
                        "speed": "fast",
                        "complexity": "advanced",
                    }
                )
            else:
                capabilities.update(
                    {
                        "strengths": ["reasoning", "analysis", "complex_tasks"],
                        "cost_tier": "high",
                        "speed": "moderate",
                        "complexity": "advanced",
                    }
                )

        # GPT-3.5 models
        elif "gpt-3.5" in model_lower:
            capabilities.update(
                {
                    "strengths": ["general", "conversation", "quick_response"],
                    "cost_tier": "low",
                    "speed": "fast",
                    "complexity": "basic",
                }
            )

        # O1 models (scientific/mathematical)
        elif "o1-" in model_lower:
            if "preview" in model_lower or "o1-" == model_lower[:3]:
                capabilities.update(
                    {
                        "strengths": ["scientific", "mathematical", "research"],
                        "cost_tier": "very_high",
                        "speed": "slow",
                        "complexity": "expert",
                    }
                )
            elif "mini" in model_lower:
                capabilities.update(
                    {
                        "strengths": ["scientific", "mathematical", "cost_efficient"],
                        "cost_tier": "medium",
                        "speed": "moderate",
                        "complexity": "intermediate",
                    }
                )

        # O3 models (virtualization/complex reasoning)
        elif "o3-" in model_lower:
            if "preview" in model_lower or "o3-" == model_lower[:3]:
                capabilities.update(
                    {
                        "strengths": [
                            "virtualization",
                            "complex_reasoning",
                            "simulation",
                        ],
                        "cost_tier": "very_high",
                        "speed": "slow",
                        "complexity": "expert",
                    }
                )
            elif "mini" in model_lower:
                capabilities.update(
                    {
                        "strengths": ["virtualization", "reasoning", "balanced"],
                        "cost_tier": "medium",
                        "speed": "moderate",
                        "complexity": "intermediate",
                    }
                )

        # GPT-4o models
        elif "gpt-4o" in model_lower:
            capabilities.update(
                {
                    "strengths": ["multimodal", "vision", "latest", "performance"],
                    "cost_tier": "medium",
                    "speed": "fast",
                    "complexity": "advanced",
                }
            )

        return capabilities

    def _set_optimal_default_model(self):
        """Set the best available model as default."""

        if not self.available_models:
            return

        # Priority order for default model selection
        preferred_order = [
            "gpt-4o",  # Latest and most capable
            "gpt-4-turbo-preview",
            "gpt-4",
            "gpt-4o-mini",
            "gpt-3.5-turbo",
        ]

        # Find the best available model
        for preferred in preferred_order:
            for available in self.available_models.keys():
                if preferred in available:
                    self.model_preference = available
                    print(f"üéØ Set optimal default model: {available}")
                    return

        # Fallback to first available model
        self.model_preference = list(self.available_models.keys())[0]
        print(f"üéØ Set default model: {self.model_preference}")

    def refresh_models(self):
        """Manually refresh available models from OpenAI platform."""

        if not self.openai_enabled:
            print("‚ö†Ô∏è OpenAI not enabled. Use 'enable openai' first.")
            return False

        print("üîÑ Refreshing models from OpenAI platform...")
        self._refresh_available_models()
        self._set_optimal_default_model()

        return True

    def list_available_models(self) -> dict[str, Any]:
        """List all available models with their capabilities."""

        if not self.openai_enabled:
            return {"error": "OpenAI not enabled"}

        # Auto-refresh if models are stale (more than 1 hour old)
        if self.last_model_refresh and (time.time() - self.last_model_refresh) > 3600:
            self._refresh_available_models()

        return {
            "total_models": len(self.available_models),
            "last_refresh": self.last_model_refresh,
            "models": self.available_models,
            "current_default": self.model_preference,
        }

    def enable_openai(self, api_key: str):
        """Enable OpenAI integration with provided API key."""
        if not OPENAI_AVAILABLE:
            print("‚ùå OpenAI not installed. Install with: pip install openai")
            return False

        self._initialize_openai(api_key)
        return self.openai_enabled

    def disable_openai(self):
        """Disable OpenAI integration and use local intelligence."""
        self.openai_enabled = False
        self.openai_client = None
        print("ü§ñ ChatGPT integration disabled. Using local intelligence.")

    def set_model(self, model_name: str):
        """Set preferred OpenAI model using dynamic platform models."""
        if not self.openai_enabled:
            print("‚ö†Ô∏è OpenAI not enabled. Use 'enable openai' first.")
            return False

        # Check if model is available from platform
        if model_name in self.available_models:
            old_model = self.model_preference
            self.model_preference = model_name
            capabilities = self.model_capabilities.get(model_name, {})

            print(f"ü§ñ Model switched from {old_model} to {model_name}")
            print(
                f"   Strengths: {', '.join(capabilities.get('strengths', ['general']))}"
            )
            print(f"   Cost Tier: {capabilities.get('cost_tier', 'unknown')}")
            print(f"   Speed: {capabilities.get('speed', 'unknown')}")
            print(f"   Complexity: {capabilities.get('complexity', 'unknown')}")
            return True
        else:
            available_models = list(self.available_models.keys())[:10]  # Show first 10
            print(f"‚ö†Ô∏è Model '{model_name}' not available.")
            print(f"   Available models: {', '.join(available_models)}")
            if len(self.available_models) > 10:
                print(f"   ... and {len(self.available_models) - 10} more models")
            print("   Use 'refresh models' to get latest model list from OpenAI")
            return False

    def enable_dynamic_switching(self):
        """Enable intelligent model switching based on context."""
        self.dynamic_model_switching = True
        print(
            "üß† Dynamic model switching enabled - I'll choose the best model for each request!"
        )

    def disable_dynamic_switching(self):
        """Disable dynamic model switching and use current model."""
        self.dynamic_model_switching = False
        print(
            f"üîí Dynamic switching disabled - Using {self.model_preference} for all requests."
        )

    def enable_cost_optimization(self):
        """Enable cost optimization for model selection."""
        self.cost_optimization = True
        print("üí∞ Cost optimization enabled - I'll prioritize cost-effective models!")

    def disable_cost_optimization(self):
        """Disable cost optimization."""
        self.cost_optimization = False
        print("üí∏ Cost optimization disabled - I'll prioritize quality over cost!")

    def get_last_used_model(self) -> str:
        """Get the actual model used in the last response."""
        return self._last_used_model or self.model_preference

    def get_optimal_model(self, message: str) -> str:
        """Get the optimal model for the given message."""

        if not self.openai_enabled or not self.dynamic_model_switching:
            return self.model_preference

        # Analyze the message
        analysis = self.context_analyzer.analyze_prompt(
            message, self.conversation_history
        )
        recommended_model = analysis["recommended_model"]

        # Apply cost optimization if enabled
        if self.cost_optimization:
            recommended_model = self._apply_cost_optimization(
                recommended_model, analysis
            )

        return recommended_model

    def _apply_cost_optimization(
        self, recommended_model: str, analysis: dict[str, Any]
    ) -> str:
        """Apply cost optimization to model selection."""

        # If confidence is low, use cheaper model
        if analysis["confidence"] < 0.8:
            if recommended_model in ["o1-preview", "gpt-4"]:
                # Switch to mini/cheaper versions
                if recommended_model == "o1-preview":
                    return "gpt-3.5-turbo"  # Avoid o1-mini due to system message incompatibility
                elif recommended_model == "gpt-4":
                    return "gpt-4-turbo-preview"

        # For low complexity tasks, always use cheaper models
        if analysis["complexity"] == "low":
            if recommended_model in ["o1-preview", "gpt-4"]:
                return "gpt-3.5-turbo"

        return recommended_model

    def chat(self, message: str, stream: bool = False) -> str | Iterator[str]:
        """
        Main chat interface - simple on the outside, powerful on the inside.

        Args:
            message: User message
            stream: Whether to stream response

        Returns:
            Response string or iterator for streaming
        """
        start_time = time.time()

        # Create message record
        user_message = ChatMessage(
            role="user",
            content=message,
            timestamp=datetime.now(UTC).isoformat(),
            message_id=str(uuid.uuid4()),
        )

        # Add to conversation history
        self.conversation_history.append(user_message)

        # Process message with advanced AI
        if stream:
            return self._stream_response(message, start_time)
        else:
            response = self._generate_response(message)

            # Record metrics
            response_time = time.time() - start_time
            self.response_times.append(response_time)
            self.total_interactions += 1

            return response

    def _generate_response(self, message: str) -> str:
        """Generate intelligent response using ChatGPT or local AI."""

        # Check for OpenAI commands first
        if message.lower().startswith("enable openai"):
            parts = message.split(" ", 2)
            if len(parts) > 2:
                return (
                    "‚úÖ OpenAI enabled!"
                    if self.enable_openai(parts[2])
                    else "‚ùå Failed to enable OpenAI"
                )
            else:
                return "Please provide API key: 'enable openai your-api-key-here'"

        elif message.lower() == "disable openai":
            self.disable_openai()
            return "‚úÖ OpenAI disabled. Using local intelligence."

        elif message.lower().startswith("set model"):
            parts = message.split(" ", 2)
            if len(parts) > 2:
                return (
                    "‚úÖ Model updated!"
                    if self.set_model(parts[2])
                    else "‚ùå Failed to set model"
                )
            else:
                return f"Current model: {self.model_preference}. Available: gpt-3.5-turbo, gpt-4, gpt-4-turbo-preview, gpt-4o, o1-preview"

        # Dynamic switching commands
        elif message.lower() == "enable dynamic":
            self.enable_dynamic_switching()
            return "‚úÖ Dynamic model switching enabled!"

        elif message.lower() == "disable dynamic":
            self.disable_dynamic_switching()
            return "üîí Dynamic model switching disabled."

        elif message.lower() == "enable cost":
            self.enable_cost_optimization()
            return "üí∞ Cost optimization enabled!"

        elif message.lower() == "disable cost":
            self.disable_cost_optimization()
            return "üí∏ Cost optimization disabled."

        # Use ChatGPT if enabled and available
        if self.openai_enabled and self.openai_client:
            try:
                return self._generate_chatgpt_response(message)
            except Exception as e:
                print(f"‚ö†Ô∏è ChatGPT error: {e}. Falling back to local intelligence...")
                if not self.fallback_enabled:
                    return "Sorry, I'm having trouble connecting to ChatGPT. Please try again later."

        # Fallback to local intelligence
        return self._generate_local_response(message)

    def _generate_chatgpt_response(self, message: str) -> str:
        """Generate response using ChatGPT with dynamic model selection."""

        # Get optimal model for this message
        optimal_model = self.get_optimal_model(message)

        # Store analysis for personalization
        analysis = self.context_analyzer.analyze_prompt(
            message, self.conversation_history
        )
        self._last_analysis = analysis
        self._last_used_model = optimal_model  # Track the actual model used

        # Prepare conversation history for ChatGPT
        messages = []

        # Add enhanced system prompt based on model and personality
        system_prompt = self._generate_system_prompt(optimal_model)
        messages.append({"role": "system", "content": system_prompt})

        # Add recent conversation history (last 10 messages, but manage context length)
        recent_history = self.conversation_history[-10:]

        # Calculate approximate tokens (rough estimate: 1 token ‚âà 4 characters)
        total_chars = (
            sum(len(msg.content) for msg in recent_history)
            + len(message)
            + len(system_prompt)
        )
        estimated_tokens = total_chars // 4

        # If estimated tokens are high, reduce history
        max_history = 10
        if estimated_tokens > 6000:  # Leave room for response
            max_history = 5
        if estimated_tokens > 7000:
            max_history = 3

        recent_history = recent_history[-max_history:]

        for msg in recent_history:
            messages.append({"role": msg.role, "content": msg.content})

        # Add current message
        messages.append({"role": "user", "content": message})

        try:
            # Generate response with optimal model
            response = self.openai_client.chat.completions.create(
                model=optimal_model,
                messages=messages,
                max_tokens=self._get_max_tokens(optimal_model),
                temperature=self._get_temperature(optimal_model),
                top_p=1,
                frequency_penalty=0,
                presence_penalty=0,
            )

            chatgpt_response = response.choices[0].message.content.strip()

            # Add intelligence indicator and model info
            model_info = self.model_capabilities.get(optimal_model, {})
            response_suffix = self._generate_response_suffix_enhanced(
                optimal_model, model_info, analysis
            )

            # If dynamic switching is enabled, show model selection info
            if self.dynamic_model_switching:
                selection_info = f"\n\nüß† *Auto-selected {optimal_model} for {analysis['domain']} {analysis['complexity']} complexity task*"
            else:
                selection_info = ""

            return chatgpt_response + response_suffix + selection_info

        except Exception as e:
            raise Exception(f"ChatGPT API error with {optimal_model}: {e}")

    def _generate_response_suffix_enhanced(
        self, model: str, capabilities: dict[str, Any], analysis: dict[str, Any]
    ) -> str:
        """Generate enhanced response suffix with personality and emotional indicators."""

        # Base model indicator
        if model.startswith("o1"):
            base_suffix = "\n\nüî¨ *Powered by O1 scientific reasoning model*"
        elif model.startswith("o3"):
            base_suffix = "\n\nüåê *Powered by O3 virtualization reasoning model*"
        elif model == "gpt-4":
            base_suffix = "\n\nüß† *Powered by GPT-4 advanced intelligence*"
        elif model == "gpt-4-turbo-preview":
            base_suffix = "\n\n‚ö° *Powered by GPT-4 Turbo (latest)*"
        else:
            base_suffix = "\n\nü§ñ *Powered by ChatGPT*"

        # Add personality indicator
        personality = analysis.get("personality", "balanced")
        if personality != "balanced":
            personality_emoji = {
                "analytical": "üìä",
                "creative": "üé®",
                "practical": "üõ†Ô∏è",
                "philosophical": "ü§î",
                "social": "ü§ù",
            }
            base_suffix += f" {personality_emoji.get(personality, 'üéØ')} *Adapted to your {personality} style*"

        # Add emotional support indicator
        emotional = analysis.get("emotional_analysis", {})
        if emotional.get("support_needed", False):
            base_suffix += " üíô *Here to support you with empathy and care*"
        elif emotional.get("emotional_state") == "positive":
            base_suffix += " üòä *Glad to share in your positive energy*"

        # Add memory indicator for established conversations
        memory = analysis.get("memory_context", {})
        if memory.get("total_domains_explored", 0) > 5:
            base_suffix += f" üß© *Drawing on our shared experience across {memory['total_domains_explored']} domains*"

        return base_suffix

    def _generate_system_prompt(self, model: str) -> str:
        """Generate system prompt optimized for specific model with personality and empathy."""

        base_prompt = f"""You are Echoes AI Assistant, an intelligent and helpful assistant with these core values: {', '.join(self.values)}.

Guidelines:
- Be respectful, accurate, and helpful
- Provide clear and concise responses
- Adapt your communication style to the user
- If you don't know something, admit it gracefully
- Maintain conversation context and continuity
- Use emojis occasionally to be friendly and engaging

Current session: {self.session_id}
Total interactions so far: {self.total_interactions}"""

        # Get context analysis for personalization
        if hasattr(self, "_last_analysis"):
            analysis = self._last_analysis
            personality = analysis.get("personality", "balanced")
            emotional = analysis.get("emotional_analysis", {})
            memory = analysis.get("memory_context", {})

            # Add personality adaptations
            if personality == "analytical":
                base_prompt += "\n\nPersonality Adaptation: Be precise, logical, and data-driven. Provide evidence-based answers with clear reasoning."
            elif personality == "creative":
                base_prompt += "\n\nPersonality Adaptation: Be imaginative, expressive, and inspiring. Use creative language and think outside the box."
            elif personality == "practical":
                base_prompt += "\n\nPersonality Adaptation: Be actionable, realistic, and results-oriented. Focus on practical steps and real-world applications."
            elif personality == "philosophical":
                base_prompt += "\n\nPersonality Adaptation: Be thoughtful, reflective, and deep. Explore meaning and purpose while providing wisdom."
            elif personality == "social":
                base_prompt += "\n\nPersonality Adaptation: Be warm, empathetic, and relationship-focused. Consider human connections and emotional impact."

            # Add emotional support adaptations
            if emotional.get("support_needed", False):
                base_prompt += "\n\nEmotional Support: Be extra empathetic, patient, and supportive. Acknowledge feelings and provide gentle guidance. Use warm, caring language."

            # Add memory context
            if memory.get("total_domains_explored", 0) > 3:
                base_prompt += f"\n\nMemory Context: I've learned you prefer {memory.get('dominant_personality', 'balanced')} communication and often explore {memory.get('most_common_domain', 'general')} topics. I'll tailor my responses accordingly."

        # Model-specific optimizations
        if model.startswith("o1"):
            base_prompt += "\n\nSpecial Instructions: You are optimized for scientific and mathematical reasoning. Provide precise, analytical responses with clear logical progression. Show your work for complex problems."

        elif model.startswith("o3"):
            base_prompt += "\n\nSpecial Instructions: You are optimized for virtualization and complex reasoning. Think step-by-step and consider multiple perspectives. Provide detailed analysis of complex scenarios."

        elif model == "gpt-4":
            base_prompt += "\n\nSpecial Instructions: You are an advanced AI with superior reasoning capabilities. Provide nuanced, comprehensive responses that consider multiple angles and implications."

        elif model == "gpt-4-turbo-preview":
            base_prompt += "\n\nSpecial Instructions: You represent the latest in AI capabilities. Provide cutting-edge insights and analysis with confidence and clarity."

        return base_prompt

    def _get_max_tokens(self, model: str) -> int:
        """Get optimal max tokens for model."""

        if model.startswith("o1") or model.startswith("o3"):
            return 2000  # More tokens for complex reasoning
        elif model == "gpt-4":
            return 1500
        else:
            return 1000

    def _get_temperature(self, model: str) -> float:
        """Get optimal temperature for model."""

        if model.startswith("o1") or model.startswith("o3"):
            return 0.1  # Lower temperature for analytical tasks
        elif model == "gpt-4":
            return 0.7
        else:
            return 0.7

    def _generate_response_suffix(
        self, model: str, capabilities: dict[str, Any]
    ) -> str:
        """Generate response suffix based on model used."""

        if model.startswith("o1"):
            return "\n\nüî¨ *Powered by O1 scientific reasoning model*"
        elif model.startswith("o3"):
            return "\n\nüåê *Powered by O3 virtualization reasoning model*"
        elif model == "gpt-4":
            return "\n\nüß† *Powered by GPT-4 advanced intelligence*"
        elif model == "gpt-4-turbo-preview":
            return "\n\n‚ö° *Powered by GPT-4 Turbo (latest)*"
        else:
            return "\n\nü§ñ *Powered by ChatGPT*"

    def _generate_local_response(self, message: str) -> str:
        """Generate response using local intelligence (original implementation)."""

        # Analyze message intent and context
        intent = self._analyze_intent(message)
        context = self._get_conversation_context()

        # Generate base response
        if intent == "question":
            response = self._handle_question(message, context)
        elif intent == "task":
            response = self._handle_task(message, context)
        elif intent == "conversation":
            response = self._handle_conversation(message, context)
        else:
            response = self._handle_general(message, context)

        # Add personality and learning
        response = self._enhance_response(response, intent, context)

        # Add local intelligence indicator
        response += "\n\n‚ú® *Powered by advanced local intelligence*"

        return response

    def _stream_response(self, message: str, start_time: float) -> Iterator[str]:
        """Stream response in real-time."""

        # Generate response chunks
        full_response = self._generate_response(message)

        # Stream word by word for real-time feel
        words = full_response.split()
        buffer = ""

        for i, word in enumerate(words):
            buffer += word + " "

            # Send chunks of 3-5 words
            if (i + 1) % 3 == 0 or i == len(words) - 1:
                yield buffer.strip()
                buffer = ""
                time.sleep(0.1)  # Natural typing delay

        # Record final metrics
        response_time = time.time() - start_time
        self.response_times.append(response_time)
        self.total_interactions += 1

    def _analyze_intent(self, message: str) -> str:
        """Analyze user intent using simple patterns."""

        question_patterns = [
            r"\?",
            r"what|how|why|when|where|who",
            r"can you|could you|would you",
        ]

        task_patterns = [
            r"help me|please|can you",
            r"create|make|write|generate",
            r"analyze|calculate|compute|process",
            r"refactor|simplify|optimize",
        ]

        message_lower = message.lower()

        # Check for code-related tasks first
        if any(
            word in message_lower
            for word in ["code", "refactor", "function", "chat", "assistant"]
        ):
            return "task"  # Treat code-related as tasks

        for pattern in question_patterns:
            if re.search(pattern, message_lower):
                return "question"

        for pattern in task_patterns:
            if re.search(pattern, message_lower):
                return "task"

        return "conversation"

    def _get_conversation_context(self) -> str:
        """Get summarized conversation context."""

        if len(self.conversation_history) < 3:
            return "new_conversation"

        # Get last few messages for context
        recent_messages = self.conversation_history[-6:]
        context_summary = " ".join([msg.content for msg in recent_messages])

        # Keep it brief
        if len(context_summary) > 200:
            context_summary = context_summary[:200] + "..."

        return context_summary

    def _handle_question(self, message: str, context: str) -> str:
        """Handle questions with intelligent responses."""

        # Simulate knowledge retrieval
        if "weather" in message.lower():
            return "I don't have access to real-time weather data, but I can help you understand weather patterns or suggest weather resources!"

        elif "calculate" in message.lower() or "math" in message.lower():
            return "I can help with calculations! Please provide the specific math problem you'd like me to solve."

        else:
            return f"That's an interesting question! Based on our conversation context ({context[:50]}...), I'd say this deserves careful consideration. What specific aspect would you like to explore further?"

    def _handle_task(self, message: str, context: str) -> str:
        """Handle task requests with helpful responses."""

        message_lower = message.lower()

        # Code-related tasks
        if any(
            word in message_lower
            for word in ["code", "refactor", "simplify", "optimize"]
        ):
            return "I'd be happy to help you refactor or simplify your code! However, I need to see the actual code first. Please paste the Python code you want me to refactor, and I'll provide specific suggestions for making it more readable, efficient, and logically structured."

        if any(word in message_lower for word in ["function", "chat", "assistant"]):
            return "Those sound like important components of a chat system! To help you understand or refactor them, please share the actual code. I'll analyze the structure and provide specific improvements for clarity and simplicity."

        if "write" in message_lower or "create" in message_lower:
            return "I'd be happy to help you create something! Could you provide more details about what you'd like to write or create?"

        else:
            return "I can help with various tasks! Please provide more details about what you'd like me to do, and if it involves code, please share the code you'd like me to work with."

    def _handle_conversation(self, message: str, context: str) -> str:
        """Handle general conversation."""

        responses = [
            "That's fascinating! Tell me more about your perspective on this.",
            "I appreciate you sharing that with me. How does this connect to your goals?",
            "Interesting point! I'm curious to hear more about your thoughts.",
            "Thank you for sharing. What would you like to explore next?",
        ]

        # Choose response based on conversation flow
        if len(self.conversation_history) > 5:
            return (
                f"Building on our conversation, {np.random.choice(responses).lower()}"
            )
        else:
            return np.random.choice(responses)

    def _handle_general(self, message: str, context: str) -> str:
        """Handle general messages with better context awareness."""

        # Check if this is about code refactoring (from the user's previous message)
        if "code" in message.lower() and "refactor" in message.lower():
            return "I'd be happy to help you refactor code! However, I don't see the code you'd like me to simplify. Could you please provide the Python code you want me to refactor for better simplicity and understanding?"

        # Check if this is about explaining functions
        if any(
            word in message.lower()
            for word in ["chat", "assistant", "main loop", "function"]
        ):
            return "Those sound like important components of a chat assistant system. To help you understand or refactor them, could you share the actual code? I can then provide specific simplification suggestions and structural improvements."

        # Default helpful response
        return "I'm here to help! Could you please provide more details about what you'd like assistance with? If you have code to refactor, please share it and I'll help simplify it."

    def _enhance_response(self, response: str, intent: str, context: str) -> str:
        """Enhance response with personality and learning."""

        # Add contextual awareness
        if len(self.conversation_history) > 10:
            response += "\n\nüí≠ *I'm learning from our ongoing conversation and adapting to your communication style.*"

        # Add helpful suggestions
        if intent == "question":
            response += (
                "\n\nüí° *Feel free to ask follow-up questions if you need more details!*"
            )

        return response

    def _learn_from_interaction(self, user_input: str, response: str) -> None:
        """Learn from each interaction."""

        # Calculate engagement score
        engagement_score = self._calculate_engagement(user_input, response)

        # Store learning data
        learning_point = {
            "user_input": user_input,
            "response": response,
            "engagement_score": engagement_score,
            "timestamp": datetime.now(UTC).isoformat(),
            "context_length": len(self.conversation_history),
        }

        self.learning_data.append(learning_point)

        # Update success rate
        if engagement_score > 0.7:
            self.success_rate = (self.success_rate * 0.8) + (engagement_score * 0.2)

    def _calculate_engagement(self, user_input: str, response: str) -> float:
        """Calculate engagement score for learning."""

        # Simple engagement metrics
        user_length = len(user_input.split())
        response_length = len(response.split())

        # Prefer balanced conversations
        length_ratio = min(user_length, response_length) / max(
            user_length, response_length
        )

        # Add some randomness for learning simulation
        base_score = length_ratio * 0.7 + np.random.rand() * 0.3

        return min(1.0, base_score)

    def update_values(self, new_values: set) -> None:
        """Update core values with validation."""

        # Validate new values
        valid_values = {
            "respect",
            "accuracy",
            "helpfulness",
            "clarity",
            "adaptability",
            "creativity",
            "empathy",
            "efficiency",
            "learning",
        }

        for value in new_values:
            if value in valid_values:
                self.values.add(value)
            else:
                print(f"‚ö†Ô∏è  Value '{value}' not recognized. Using standard values.")

    def learn_behavior(self, user_input: str, response: str, feedback: float) -> None:
        """Learn from feedback using reinforcement learning."""

        reward = self.evaluate_response(response) * feedback
        self.behaviors.append((user_input, response, reward))

        # Keep only recent behaviors for memory efficiency
        if len(self.behaviors) > 100:
            self.behaviors = self.behaviors[-50:]

    def evaluate_response(self, response: str) -> float:
        """Evaluate response quality using multiple metrics."""

        # Base score from length and complexity
        words = response.split()
        length_score = min(1.0, len(words) / 20)  # Prefer ~20 word responses

        # Complexity score (simple heuristic)
        complexity_score = min(1.0, len(set(words)) / len(words))

        # Values alignment score
        values_score = self._check_values_alignment(response)

        # Combined score
        total_score = length_score * 0.3 + complexity_score * 0.3 + values_score * 0.4

        return total_score

    def _check_values_alignment(self, response: str) -> float:
        """Check if response aligns with core values."""

        value_keywords = {
            "respect": ["please", "thank you", "appreciate", "understand"],
            "accuracy": ["precise", "exact", "correct", "accurate"],
            "helpfulness": ["help", "assist", "support", "guide"],
            "clarity": ["clear", "simple", "easy", "understandable"],
        }

        response_lower = response.lower()
        alignment_score = 0.0

        for value, keywords in value_keywords.items():
            if value in self.values:
                matches = sum(1 for keyword in keywords if keyword in response_lower)
                alignment_score += matches / len(keywords)

        return min(1.0, alignment_score / len(self.values))

    def get_stats(self) -> dict[str, Any]:
        """Get assistant statistics including platform model information."""

        avg_response_time = np.mean(self.response_times) if self.response_times else 0

        # Get personality and memory insights
        memory_context = self.context_analyzer._get_memory_context()

        # Get platform model information
        platform_info = {}
        if self.openai_enabled:
            platform_info = {
                "total_available_models": len(self.available_models),
                "last_model_refresh": self.last_model_refresh,
                "model_info_cached": len(self.model_info_cache),
                "platform_sync": "active",
            }
        else:
            platform_info = {"platform_sync": "disabled"}

        return {
            "total_interactions": self.total_interactions,
            "success_rate": self.success_rate,
            "average_response_time": avg_response_time,
            "conversation_length": len(self.conversation_history),
            "learned_behaviors": len(self.behaviors),
            "core_values": list(self.values),
            "session_id": self.session_id,
            "openai_enabled": self.openai_enabled,
            "current_model": self.model_preference if self.openai_enabled else "local",
            "intelligence_source": "ChatGPT" if self.openai_enabled else "Local AI",
            "fallback_enabled": self.fallback_enabled,
            "dynamic_model_switching": self.dynamic_model_switching,
            "cost_optimization": self.cost_optimization,
            "available_models": (
                list(self.available_models.keys()) if self.openai_enabled else ["local"]
            ),
            # Platform integration info
            "platform_integration": platform_info,
            # Personality and memory insights
            "personality_insights": {
                "dominant_personality": memory_context.get(
                    "dominant_personality", "balanced"
                ),
                "preferred_domain": memory_context.get("most_common_domain", "general"),
                "total_domains_explored": memory_context.get(
                    "total_domains_explored", 0
                ),
                "conversation_patterns": memory_context.get(
                    "conversation_patterns", {}
                ),
                "emotional_support_sessions": len(
                    [
                        e
                        for e in self.emotional_history
                        if e.get("support_needed", False)
                    ]
                ),
            },
        }

    def reset_conversation(self) -> None:
        """Reset conversation while preserving learning."""

        self.conversation_history = []
        print("üîÑ Conversation reset. Learning and behaviors preserved.")


# ============================================================================
# Training and Analytics Functions
# ============================================================================


def train_assistant(data: dict[str, Any]) -> float:
    """
    Train assistant using provided data.

    Args:
        data: Training data with 'X' and 'y' keys

    Returns:
        Training accuracy score
    """

    # Simulate training process
    print("üéì Training assistant with provided data...")

    # Simple training simulation
    X_size = len(data.get("X", []))
    y_size = len(data.get("y", []))

    if X_size == 0 or y_size == 0:
        print("‚ö†Ô∏è  No training data provided")
        return 0.0

    # Simulate training accuracy
    accuracy = min(0.95, 0.5 + (X_size / 1000))

    print(f"‚úÖ Training completed with {accuracy:.2f} accuracy")
    return accuracy


def analyze_performance(assistant: IntelligentAssistant) -> dict[str, Any]:
    """Analyze assistant performance and provide insights."""

    stats = assistant.get_stats()

    # Performance analysis
    insights = {
        "performance_grade": (
            "A"
            if stats["success_rate"] > 0.8
            else "B"
            if stats["success_rate"] > 0.6
            else "C"
        ),
        "engagement_level": (
            "High"
            if stats["total_interactions"] > 10
            else "Medium"
            if stats["total_interactions"] > 5
            else "Low"
        ),
        "learning_progress": (
            "Excellent"
            if len(assistant.behaviors) > 20
            else "Good"
            if len(assistant.behaviors) > 10
            else "Developing"
        ),
    }

    return {**stats, **insights}


# ============================================================================
# Main Interface
# ============================================================================


def main():
    """Main interface for interactive use with enhanced CLI features."""

    print("üöÄ Echoes AI Assistant - Enhanced Implementation with Platform Integration")
    print("=" * 80)

    # Initialize assistant
    assistant = IntelligentAssistant()

    # Initialize enhanced CLI
    cli = EnhancedCLI(assistant)

    # Show CLI status
    if ENHANCED_CLI_AVAILABLE:
        print(
            "‚úÖ Enhanced CLI enabled - Tab completion, history navigation, and context visualization"
        )
        print("üí° Press F1 for context help, Tab to autocomplete, ‚Üë/‚Üì for history")
    else:
        print("‚ö†Ô∏è Basic CLI mode - Install prompt_toolkit for enhanced features")

    # Interactive loop
    while True:
        try:
            user_input = cli.get_user_input("\nüí¨ You: ")

            # Handle Ctrl+C
            if user_input == "\x03":
                print("\nüëã Session interrupted. Goodbye!")
                break

            # Handle exit commands
            if user_input.lower() in ["exit", "quit", "bye"]:
                print("üëã Goodbye! It was great assisting you!")
                break

            # Handle empty input
            if not user_input.strip():
                continue

            # Enhanced CLI commands
            if user_input.lower() == "help":
                cli._show_context_help()
                continue

            elif user_input.lower() == "show context":
                cli.show_context_visualization()
                continue

            elif user_input.lower() == "show history":
                print(
                    f"\nüí¨ CONVERSATION HISTORY ({len(assistant.conversation_history)} messages):"
                )
                for i, msg in enumerate(assistant.conversation_history[-10:], 1):
                    role_icon = "üë§" if msg.role == "user" else "ü§ñ"
                    timestamp = (
                        msg.timestamp[:19] if hasattr(msg, "timestamp") else "Unknown"
                    )
                    print(
                        f"   {i}. [{timestamp}] {role_icon}: {msg.content[:100]}{'...' if len(msg.content) > 100 else ''}"
                    )
                continue

            elif user_input.lower() == "show memory":
                memory_context = assistant.context_analyzer._get_memory_context()
                print("\nüß© MEMORY CONTEXT:")
                print(
                    f"   Preferred Domain: {memory_context.get('most_common_domain', 'general')}"
                )
                print(
                    f"   Dominant Personality: {memory_context.get('dominant_personality', 'balanced')}"
                )
                print(
                    f"   Total Domains Explored: {memory_context.get('total_domains_explored', 0)}"
                )
                print(
                    f"   Conversation Patterns: {memory_context.get('conversation_patterns', {})}"
                )
                continue

            elif user_input.lower() == "show visual context":
                cli.visual_context.display_visual_context()
                continue

            # Advanced history commands
            elif user_input.lower().startswith("search history"):
                parts = user_input.split(" ", 3)
                if len(parts) < 3:
                    print("Usage: search history <query> [limit]")
                    continue

                query = parts[2]
                limit = int(parts[3]) if len(parts) > 3 else 10

                results = cli.history_manager.search_history(query, limit)
                if results:
                    print(f"\nüîç SEARCH RESULTS for '{query}' ({len(results)} found):")
                    for i, result in enumerate(results, 1):
                        msg = result["message"]
                        score = result["score"]
                        preview = result["preview"]
                        role_icon = "üë§" if msg.role == "user" else "ü§ñ"
                        print(f"   {i}. [{score}] {role_icon}: {preview}")
                else:
                    print(f"üîç No results found for '{query}'")
                continue

            elif user_input.lower().startswith("filter history"):
                print("\nüîç HISTORY FILTER OPTIONS:")
                print("Available filters:")
                print("  role: user|assistant")
                print("  min_length: <number>")
                print("  max_length: <number>")
                print("Example: filter history role user min_length 10")
                continue

            elif user_input.lower() == "threaded view":
                threads = cli.history_manager.get_threaded_view()
                print(f"\nüßµ THREADED CONVERSATION VIEW ({len(threads)} threads):")
                for i, thread in enumerate(threads, 1):
                    print(f"\n   Thread {i} ({len(thread)} messages):")
                    for msg_data in thread[:3]:  # Show first 3 messages
                        msg = msg_data["message"]
                        role_icon = "üë§" if msg.role == "user" else "ü§ñ"
                        preview = (
                            msg.content[:80] + "..."
                            if len(msg.content) > 80
                            else msg.content
                        )
                        print(f"      {role_icon}: {preview}")
                    if len(thread) > 3:
                        print(f"      ... and {len(thread) - 3} more messages")
                continue

            elif user_input.lower() == "jump unanswered":
                unanswered_idx = cli.history_manager.jump_to_last_unanswered()
                if unanswered_idx is not None:
                    msg = assistant.conversation_history[unanswered_idx]
                    print(f"\nüéØ LAST UNANSWERED QUESTION (message {unanswered_idx}):")
                    print(f"   üë§: {msg.content}")
                else:
                    print("\nüéØ No unanswered questions found")
                continue

            elif user_input.lower().startswith("add bookmark"):
                parts = user_input.split(" ", 3)
                if len(parts) < 4:
                    print("Usage: add bookmark <message_index> <label>")
                    continue

                try:
                    msg_idx = int(parts[2])
                    label = parts[3]
                    if cli.history_manager.add_bookmark(msg_idx, label):
                        print(f"‚úÖ Bookmark added: '{label}' at message {msg_idx}")
                    else:
                        print(f"‚ùå Invalid message index: {msg_idx}")
                except ValueError:
                    print("‚ùå Message index must be a number")
                continue

            elif user_input.lower().startswith("add tag"):
                parts = user_input.split(" ", 3)
                if len(parts) < 4:
                    print("Usage: add tag <message_index> <tag>")
                    continue

                try:
                    msg_idx = int(parts[2])
                    tag = parts[3]
                    if cli.history_manager.add_tag(msg_idx, tag):
                        print(f"‚úÖ Tag added: '{tag}' to message {msg_idx}")
                    else:
                        print(f"‚ùå Invalid message index: {msg_idx}")
                except ValueError:
                    print("‚ùå Message index must be a number")
                continue

            # API dashboard commands
            elif user_input.lower() == "api dashboard":
                cli.api_dashboard.display_dashboard()
                continue

            elif user_input.lower().startswith("export api logs"):
                parts = user_input.split(" ", 3)
                format_type = parts[3] if len(parts) > 3 else "json"

                if format_type not in ["json", "csv"]:
                    print("Format must be 'json' or 'csv'")
                    continue

                export_file = cli.api_dashboard.export_logs(format_type)
                print(f"‚úÖ API logs exported to: {export_file}")
                continue

            # Self-diagnosis commands
            elif user_input.lower() == "health check":
                cli.self_diagnosis.run_comprehensive_health_check()
                continue

            elif user_input.lower() == "diagnostic history":
                history = cli.self_diagnosis.get_diagnostic_history()
                if history:
                    print(f"\nüìã RECENT DIAGNOSTIC HISTORY ({len(history)} checks):")
                    for i, diagnostic in enumerate(history, 1):
                        timestamp = diagnostic["timestamp"][:19]
                        status = diagnostic["overall_status"].upper()
                        issues = len(diagnostic.get("issues_found", []))
                        print(
                            f"   {i}. [{timestamp}] Status: {status} ({issues} issues)"
                        )
                else:
                    print("\nüìã No diagnostic history available")
                continue

            elif user_input.lower() == "enable auto recovery":
                cli.self_diagnosis.enable_auto_recovery()
                continue

            elif user_input.lower() == "disable auto recovery":
                cli.self_diagnosis.disable_auto_recovery()
                continue

            # Multimodal memory commands
            elif user_input.lower().startswith("add attachment"):
                parts = user_input.split(" ", 3)
                if len(parts) < 3:
                    print("Usage: add attachment <file_path> [description]")
                    continue

                file_path = parts[2]
                description = parts[3] if len(parts) > 3 else ""

                result = cli.multimodal_memory.add_attachment(file_path, description)
                if result["success"]:
                    print(f"‚úÖ Attachment added: {result['attachment_id']}")
                    print(f"   File: {result['metadata']['filename']}")
                    print(f"   Size: {result['metadata']['file_size']} bytes")
                    print(
                        f"   Preview: {result['metadata']['content_preview'][:100]}..."
                    )
                else:
                    print(f"‚ùå Failed to add attachment: {result['error']}")
                continue

            elif user_input.lower().startswith("search attachments"):
                parts = user_input.split(" ", 3)
                if len(parts) < 3:
                    print("Usage: search attachments <query> [limit]")
                    continue

                query = parts[2]
                limit = int(parts[3]) if len(parts) > 3 else 10

                results = cli.multimodal_memory.search_attachments(query, limit)
                if results:
                    print(
                        f"\nüîç ATTACHMENT SEARCH RESULTS for '{query}' ({len(results)} found):"
                    )
                    for i, result in enumerate(results, 1):
                        print(
                            f"   {i}. [{result['search_score']}] {result['filename']}"
                        )
                        print(
                            f"      Type: {result['file_type']}, Size: {result['file_size']} bytes"
                        )
                        print(f"      Preview: {result['content_preview'][:80]}...")
                        if result["tags"]:
                            print(f"      Tags: {', '.join(result['tags'])}")
                else:
                    print(f"üîç No attachments found for '{query}'")
                continue

            elif user_input.lower() == "list attachments":
                attachments = cli.multimodal_memory.list_attachments()
                if attachments:
                    print(f"\nüìé ATTACHMENTS ({len(attachments)} total):")
                    for i, attachment in enumerate(attachments, 1):
                        print(
                            f"   {i}. {attachment['filename']} ({attachment['file_size']} bytes)"
                        )
                        print(
                            f"      ID: {attachment['id']}, Type: {attachment['file_type']}"
                        )
                        print(f"      Added: {attachment['timestamp'][:19]}")
                        if attachment["tags"]:
                            print(f"      Tags: {', '.join(attachment['tags'])}")
                else:
                    print("üìé No attachments found")
                continue

            elif user_input.lower() == "memory stats":
                stats = cli.multimodal_memory.get_memory_stats()
                print("\nüìä MULTIMODAL MEMORY STATISTICS:")
                print(f"   ‚Ä¢ Total Attachments: {stats['total_attachments']}")
                print(f"   ‚Ä¢ Total Size: {stats['total_size_mb']:.2f} MB")
                print(f"   ‚Ä¢ Indexed Words: {stats['indexed_words']}")
                print(f"   ‚Ä¢ Storage Directory: {stats['storage_directory']}")
                if stats["file_types"]:
                    print("   ‚Ä¢ File Types:")
                    for file_type, count in stats["file_types"].items():
                        print(f"      - {file_type}: {count} files")
                continue

            elif user_input.lower().startswith("remove attachment"):
                parts = user_input.split(" ", 3)
                if len(parts) < 3:
                    print("Usage: remove attachment <attachment_id>")
                    continue

                attachment_id = parts[2]
                if cli.multimodal_memory.remove_attachment(attachment_id):
                    print(f"‚úÖ Attachment {attachment_id} removed successfully")
                else:
                    print(f"‚ùå Failed to remove attachment {attachment_id}")
                continue

            elif user_input.lower().startswith("export session"):
                parts = user_input.split(" ", 2)
                filename = parts[2] if len(parts) > 2 else None
                cli.export_session(filename)
                continue

            elif user_input.lower().startswith("import session"):
                parts = user_input.split(" ", 2)
                if len(parts) > 2:
                    cli.import_session(parts[2])
                else:
                    print("Usage: import session <filename>")
                continue

            elif user_input.lower() == "show logs":
                cli.show_logs()
                continue

            elif user_input.lower() == "clear logs":
                try:
                    log_file = Path("logs") / "api_calls.log"
                    if log_file.exists():
                        log_file.unlink()
                        print("‚úÖ Logs cleared")
                    else:
                        print("üìù No logs to clear")
                except Exception as e:
                    print(f"‚ùå Failed to clear logs: {e}")
                continue

            # Original commands
            elif user_input.lower() == "stats":
                stats = assistant.get_stats()
                print("\nüìä Echoes AI Assistant Statistics")
                print("=" * 50)

                # Basic stats
                print("üî¢ Basic Stats:")
                print(f"   ‚Ä¢ Total Interactions: {stats['total_interactions']}")
                print(f"   ‚Ä¢ Success Rate: {stats['success_rate']:.1%}")
                print(
                    f"   ‚Ä¢ Average Response Time: {stats['average_response_time']:.3f}s"
                )
                print(
                    f"   ‚Ä¢ Conversation Length: {stats['conversation_length']} messages"
                )

                # Intelligence
                print("\nüß† Intelligence:")
                print(f"   ‚Ä¢ Source: {stats['intelligence_source']}")
                print(f"   ‚Ä¢ Current Model: {stats['current_model']}")
                print(f"   ‚Ä¢ Available Models: {len(stats['available_models'])} models")
                print(f"   ‚Ä¢ Fallback Enabled: {stats['fallback_enabled']}")

                # Platform integration
                platform = stats["platform_integration"]
                print("\nüåê Platform Integration:")
                print(f"   ‚Ä¢ Sync Status: {platform['platform_sync']}")
                if platform["platform_sync"] == "active":
                    print(f"   ‚Ä¢ Total Models: {platform['total_available_models']}")
                    print(
                        f"   ‚Ä¢ Last Refresh: {datetime.fromtimestamp(platform['last_model_refresh'] or 0)}"
                    )

                # Smart features
                print("\nüéØ Smart Features:")
                print(
                    f"   ‚Ä¢ Dynamic Switching: {'Enabled' if stats['dynamic_model_switching'] else 'Disabled'}"
                )
                print(
                    f"   ‚Ä¢ Cost Optimization: {'Enabled' if stats['cost_optimization'] else 'Disabled'}"
                )

                # Personality insights
                personality = stats["personality_insights"]
                print("\nüé® Personality & Memory:")
                print(f"   ‚Ä¢ Dominant Style: {personality['dominant_personality']}")
                print(f"   ‚Ä¢ Preferred Domain: {personality['preferred_domain']}")
                print(f"   ‚Ä¢ Domains Explored: {personality['total_domains_explored']}")
                print(
                    f"   ‚Ä¢ Support Sessions: {personality['emotional_support_sessions']}"
                )
                continue

            elif user_input.lower() == "reset":
                assistant.reset_conversation()
                continue

            elif user_input.lower() == "train":
                # Simulate training with dummy data
                dummy_data = {"X": list(range(100)), "y": list(range(100))}
                accuracy = train_assistant(dummy_data)
                print(f"üéØ Training accuracy: {accuracy:.2f}")
                continue

            # Parse OpenAI commands
            elif user_input.lower().startswith("enable openai"):
                parts = user_input.split(" ", 2)
                if len(parts) >= 3:
                    api_key = parts[2]
                    start_time = time.time()
                    success = assistant.enable_openai(api_key)
                    elapsed = time.time() - start_time

                    # Log API call
                    cli.api_logger.info(
                        f"OPENAI_ENABLE - Success: {success}, Time: {elapsed:.3f}s"
                    )

                    if success:
                        print("‚úÖ ChatGPT integration enabled successfully!")
                        print(
                            f"üìä Found {len(assistant.available_models)} models from OpenAI platform"
                        )
                        print(f"üéØ Optimal default model: {assistant.model_preference}")
                    else:
                        print("‚ùå Failed to enable ChatGPT integration.")
                else:
                    print("Usage: enable openai <your-api-key>")
                continue

            elif user_input.lower() == "disable openai":
                assistant.disable_openai()
                cli.api_logger.info("OPENAI_DISABLE - Disabled OpenAI integration")
                continue

            elif user_input.lower().startswith("set model"):
                parts = user_input.split(" ", 2)
                if len(parts) == 2:
                    # Show current model and available models
                    print(f"üìä Current model: {assistant.model_preference}")
                    if assistant.openai_enabled:
                        models_info = assistant.list_available_models()
                        print(
                            f"üåê Total available models: {models_info['total_models']}"
                        )
                        print(f"üîÑ Last refresh: {models_info['last_refresh']}")
                        print("üìã Available models (first 10):")
                        for i, model in enumerate(
                            list(models_info["models"].keys())[:10]
                        ):
                            capabilities = models_info["models"][model]
                            print(f"   {i+1}. {model}")
                            print(
                                f"      Strengths: {', '.join(capabilities['strengths'])}"
                            )
                            print(
                                f"      Cost: {capabilities['cost_tier']}, Speed: {capabilities['speed']}"
                            )
                        if models_info["total_models"] > 10:
                            print(
                                f"   ... and {models_info['total_models'] - 10} more models"
                            )
                    else:
                        print(
                            "‚ö†Ô∏è OpenAI not enabled. Use 'enable openai <api-key>' first."
                        )
                elif len(parts) >= 3:
                    model_name = parts[2]
                    start_time = time.time()
                    success = assistant.set_model(model_name)
                    elapsed = time.time() - start_time

                    # Log API call
                    cli.api_logger.info(
                        f"MODEL_SET - Model: {model_name}, Success: {success}, Time: {elapsed:.3f}s"
                    )

                    if not success:
                        print(
                            "üí° Use 'refresh models' to get the latest model list from OpenAI"
                        )
                continue

            elif user_input.lower() == "refresh models":
                start_time = time.time()
                success = assistant.refresh_models()
                elapsed = time.time() - start_time

                # Log API call
                cli.api_logger.info(
                    f"MODELS_REFRESH - Success: {success}, Time: {elapsed:.3f}s"
                )

                if success:
                    print("‚úÖ Models refreshed successfully!")
                    print(
                        f"üìä Total models available: {len(assistant.available_models)}"
                    )
                    print(f"üéØ Current default: {assistant.model_preference}")
                continue

            elif user_input.lower() == "enable dynamic":
                assistant.enable_dynamic_switching()
                cli.logger.info("Dynamic switching enabled")
                continue

            elif user_input.lower() == "disable dynamic":
                assistant.disable_dynamic_switching()
                cli.logger.info("Dynamic switching disabled")
                continue

            elif user_input.lower() == "enable cost":
                assistant.enable_cost_optimization()
                cli.logger.info("Cost optimization enabled")
                continue

            elif user_input.lower() == "disable cost":
                assistant.disable_cost_optimization()
                cli.logger.info("Cost optimization disabled")
                continue

            # Tool management commands
            elif user_input.lower() == "list tools":
                tools = cli.tool_manager.list_tools()
                if not tools:
                    print(
                        "üîß No user tools defined. Use 'add tool <name>' to create one."
                    )
                else:
                    print(f"\nüîß USER-DEFINED TOOLS ({len(tools)} total):")
                    for name, info in tools.items():
                        print(f"   ‚Ä¢ {name}")
                        print(f"     Description: {info['description']}")
                        print(f"     Created: {info['created_at'][:19]}")
                        print(f"     Usage: {info['usage_count']} times")
                continue

            elif user_input.lower().startswith("add tool"):
                parts = user_input.split(" ", 2)
                if len(parts) < 3:
                    print("Usage: add tool <name>")
                    print("Then provide the function code when prompted")
                    continue

                tool_name = parts[2]
                print(f"\nüîß Adding tool '{tool_name}'...")
                print("Enter tool description:")
                description = cli.get_user_input("Description: ").strip()

                print(
                    "\nEnter Python function code (use '<<<END' on a new line to finish):"
                )
                print("Example:")
                print("def my_tool():")
                print("    return 'Hello from my tool!'")
                print("<<<END")

                function_lines = []
                while True:
                    line = cli.get_user_input()
                    if line.strip() == "<<<END":
                        break
                    function_lines.append(line)

                function_code = "\n".join(function_lines)

                if cli.tool_manager.add_tool(tool_name, description, function_code):
                    print(f"‚úÖ Tool '{tool_name}' added successfully!")
                continue

            elif user_input.lower().startswith("remove tool"):
                parts = user_input.split(" ", 2)
                if len(parts) < 3:
                    print("Usage: remove tool <name>")
                    continue

                tool_name = parts[2]
                cli.tool_manager.remove_tool(tool_name)
                continue

            elif user_input.lower().startswith("tool info"):
                parts = user_input.split(" ", 2)
                if len(parts) < 3:
                    print("Usage: tool info <name>")
                    continue

                tool_name = parts[2]
                tools = cli.tool_manager.list_tools()
                if tool_name in tools:
                    info = tools[tool_name]
                    print(f"\nüîß TOOL INFO: {tool_name}")
                    print(f"   Description: {info['description']}")
                    print(f"   Created: {info['created_at']}")
                    print(f"   Usage Count: {info['usage_count']}")
                else:
                    print(f"‚ùå Tool '{tool_name}' not found")
                continue

            elif user_input.lower().startswith("call "):
                parts = user_input.split(" ", 2)
                if len(parts) < 2:
                    print("Usage: call <tool_name> [args...]")
                    continue

                tool_name = parts[1]
                args = parts[2].split() if len(parts) > 2 else []

                # Execute tool
                result = cli.tool_manager.execute_tool(tool_name, *args)
                if result is not None:
                    print(f"\nüîß Tool Result: {result}")
                continue

            # Get response from assistant
            start_time = time.time()
            response = assistant.chat(user_input)
            elapsed = time.time() - start_time

            # Log the interaction
            cli.api_logger.info(
                f"CHAT_INPUT - Length: {len(user_input)}, Model: {assistant.get_last_used_model()}"
            )
            cli.api_logger.info(
                f"CHAT_RESPONSE - Length: {len(response)}, Time: {elapsed:.3f}s"
            )

            print(f"\nü§ñ Assistant: {response}")

        except KeyboardInterrupt:
            print("\n\nüëã Session interrupted. Goodbye!")
            break
        except Exception as e:
            print(f"\n‚ö†Ô∏è  Error: {e}")
            cli.logger.error(f"Main loop error: {e}")
            print("Please try again or type 'help' for commands.")


if __name__ == "__main__":
    main()
