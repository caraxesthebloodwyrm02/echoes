I'll create a battle-tested Python terminal assistant that combines the best features from both files while being focused on practical terminal operations. Let me build a streamlined but powerful assistant. <function_calls> terminal_assistant.py #!/usr/bin/env python3 """ Battle-Tested Terminal Assistant

A sophisticated terminal-based AI assistant with experience, values, and principles. Combines intelligent communication, reasoning, search, and action capabilities.

Features:

Advanced CLI with command parsing and autocomplete
Intelligent file system operations
Web search and information retrieval
Code execution and analysis
Memory and context management
Tool integration framework
Progress indicators and status updates
Error handling and auto-recovery
Session persistence and learning
Author: Battle-Tested AI Version: 1.0 """

import os import sys import json import time import re import hashlib import subprocess import threading from datetime import datetime, timezone from pathlib import Path from typing import Dict, List, Any, Optional, Iterator, Tuple, Union from collections import defaultdict, Counter from dataclasses import dataclass, field from enum import Enum import argparse import asyncio import logging

Core dependencies with fallbacks
try: import requests REQUESTS_AVAILABLE = True except ImportError: REQUESTS_AVAILABLE = False print("âš ï¸ requests not available. Install with: pip install requests")

try: from googlesearch import search GOOGLE_SEARCH_AVAILABLE = True except ImportError: GOOGLE_SEARCH_AVAILABLE = False print("âš ï¸ google-search not available. Install with: pip install google-search")

try: import psutil PSUTIL_AVAILABLE = True except ImportError: PSUTIL_AVAILABLE = False print("âš ï¸ psutil not available. Install with: pip install psutil")

Configure logging
logging.basicConfig( level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', handlers=[ logging.FileHandler("assistant_logs.log"), logging.StreamHandler() ] ) logger = logging.getLogger("TerminalAssistant")

class ActionType(Enum): """Types of actions the assistant can perform.""" FILE_OPERATION = "file_operation" SYSTEM_COMMAND = "system_command" SEARCH = "search" CODE_EXECUTION = "code_execution" ANALYSIS = "analysis" COMMUNICATION = "communication" LEARNING = "learning"

class ComplexityLevel(Enum): """Complexity levels for task assessment.""" SIMPLE = "simple" MODERATE = "moderate" COMPLEX = "complex" EXPERT = "expert"

@dataclass class Experience: """Represents accumulated experience and learning.""" domain: str success_rate: float = 0.0 total_attempts: int = 0 successful_attempts: int = 0 patterns: Dict[str, int] = field(default_factory=dict) best_practices: List[str] = field(default_factory=list) failure_modes: List[str] = field(default_factory=list)

def record_attempt(self, success: bool, pattern: str = "", practice: str = "", failure: str = ""):
    """Record an attempt and update experience."""
    self.total_attempts += 1
    if success:
        self.successful_attempts += 1
        if practice:
            self.best_practices.append(practice)
        if pattern:
            self.patterns[pattern] = self.patterns.get(pattern, 0) + 1
    else:
        if failure:
            self.failure_modes.append(failure)
    
    self.success_rate = self.successful_attempts / self.total_attempts
    
    # Keep only recent patterns and practices to manage memory
    if len(self.patterns) > 20:
        # Remove least common patterns
        sorted_patterns = sorted(self.patterns.items(), key=lambda x: x[1])
        for pattern, count in sorted_patterns[:len(self.patterns) - 15]:
            del self.patterns[pattern]
    
    if len(self.best_practices) > 50:
        self.best_practices = self.best_practices[-30:]
    
    if len(self.failure_modes) > 30:
        self.failure_modes = self.failure_modes[-20:]
@dataclass class CommandResult: """Result of command execution.""" success: bool output: str = "" error: str = "" execution_time: float = 0.0 complexity: ComplexityLevel = ComplexityLevel.SIMPLE confidence: float = 0.0 suggestions: List[str] = field(default_factory=list) metadata: Dict[str, Any] = field(default_factory=dict)

class TerminalAssistant: """ Battle-Tested Terminal Assistant with Experience and Intelligence.

This assistant combines practical terminal operations with intelligent
reasoning, learning from experience, and adaptive behavior.
"""

def __init__(self):
    """Initialize the terminal assistant."""
    self.version = "1.0"
    self.session_id = hashlib.md5(f"{time.time()}{os.getpid()}".encode()).hexdigest()[:8]
    
    # Core values and principles
    self.values = {
        "reliability": "Consistent, dependable performance in all conditions",
        "efficiency": "Optimal resource usage and time management",
        "intelligence": "Smart reasoning and adaptive learning",
        "transparency": "Clear communication and honest feedback",
        "safety": "Secure operations with error handling and recovery",
        "adaptability": "Learning from experience and improving over time",
        "helpfulness": "Focus on user needs and practical solutions"
    }
    
    # Experience tracking
    self.experience = {
        "file_operations": Experience("file_operations"),
        "system_commands": Experience("system_commands"),
        "web_search": Experience("web_search"),
        "code_execution": Experience("code_execution"),
        "data_analysis": Experience("data_analysis"),
        "debugging": Experience("debugging"),
        "automation": Experience("automation")
    }
    
    # Session state
    self.conversation_history = []
    self.command_history = []
    self.current_directory = Path.cwd()
    self.session_start_time = time.time()
    self.last_command_time = time.time()
    
    # Settings and configuration
    self.settings = {
        "verbose": True,
        "show_progress": True,
        "auto_retry": True,
        "learning_enabled": True,
        "context_aware": True,
        "multi_threaded": True,
        "timeout_seconds": 30,
        "max_retries": 3
    }
    
    # Tool registry
    self.tools = self._initialize_tools()
    
    # Context and memory
    self.context = {
        "user_preferences": {},
        "common_tasks": [],
        "recent_files": [],
        "search_history": [],
        "failed_patterns": []
    }
    
    # Performance metrics
    self.metrics = {
        "total_commands": 0,
        "successful_commands": 0,
        "failed_commands": 0,
        "average_execution_time": 0.0,
        "most_used_commands": Counter(),
        "success_by_category": defaultdict(int),
        "learning_progress": 0.0
    }
    
    logger.info(f"TerminalAssistant v{self.version} initialized (Session: {self.session_id})")
    print(f"ðŸ›¡ï¸ Terminal Assistant v{self.version} - Battle-Tested & Ready")
    print(f"ðŸŽ¯ Session ID: {self.session_id}")
    print(f"âš¡ Active: {len(self.tools)} tools, {len(self.experience)} experience domains")

def _initialize_tools(self) -> Dict[str, Any]:
    """Initialize available tools and utilities."""
    tools = {
        "file_operations": {
            "read": self._read_file,
            "write": self._write_file,
            "list": self._list_files,
            "search": self._search_files,
            "analyze": self._analyze_file,
            "diff": self._compare_files,
            "backup": self._backup_file,
            "compress": self._compress_file,
            "extract": self._extract_archive,
            "permissions": self._change_permissions
        },
        "system_commands": {
            "execute": self._execute_command,
            "process_info": self._get_process_info,
            "disk_usage": self._get_disk_usage,
            "network_status": self._check_network,
            "system_info": self._get_system_info,
            "install": self._install_package,
            "service_control": self._control_service,
            "log_analysis": self._analyze_logs
        },
        "web_operations": {
            "search": self._web_search,
            "download": self._download_file,
            "http_status": self._check_http_status,
            "curl": self._curl_request,
            "api_call": self._make_api_call
        },
        "code_operations": {
            "run": self._run_code,
            "lint": self._lint_code,
            "format": self._format_code,
            "test": self._run_tests,
            "debug": self._debug_code,
            "profile": self._profile_code,
            "docstring": self._generate_docstring
        },
        "data_operations": {
            "parse_csv": self._parse_csv,
            "parse_json": self._parse_json,
            "parse_xml": self._parse_xml,
            "database_query": self._query_database,
            "statistical_analysis": self._analyze_data,
            "visualize": self._create_visualization,
            "clean_data": self._clean_data
        },
        "analysis_operations": {
            "code_review": self._review_code,
            "security_scan": self._security_scan,
            "performance_analyze": self._analyze_performance,
            "dependency_check": self._check_dependencies,
            "architecture_review": self._review_architecture,
            "compliance_check": self._check_compliance
        },
        "automation_operations": {
            "schedule_task": self._schedule_task,
            "monitor_process": self._monitor_process,
            "create_script": self._create_script,
            "workflow_automation": self._automate_workflow,
            "alert_system": self._setup_alerts,
            "backup_automation": self._automate_backup
        }
    }
    
    logger.info(f"Initialized {sum(len(category) for category in tools.values())} tools across {len(tools)} categories")
    return tools

def _get_complexity_assessment(self, command: str, args: List[str]) -> ComplexityLevel:
    """Assess the complexity of a command."""
    # Simple commands
    simple_keywords = ["help", "status", "list", "read", "info", "show", "get"]
    moderate_keywords = ["search", "write", "create", "run", "execute", "download"]
    complex_keywords = ["analyze", "review", "automate", "optimize", "migrate", "transform"]
    expert_keywords = ["debug", "profile", "security", "architecture", "compliance"]
    
    command_lower = command.lower()
    args_text = " ".join(args).lower()
    
    # Check for expert-level operations
    if any(keyword in command_lower or keyword in args_text for keyword in expert_keywords):
        return ComplexityLevel.EXPERT
    
    # Check for complex operations
    if any(keyword in command_lower or keyword in args_text for keyword in complex_keywords):
        return ComplexityLevel.COMPLEX
    
    # Check for moderate operations
    if any(keyword in command_lower or keyword in args_text for keyword in moderate_keywords):
        return ComplexityLevel.MODERATE
    
    return ComplexityLevel.SIMPLE

def _estimate_execution_time(self, command: str, args: List[str], complexity: ComplexityLevel) -> float:
    """Estimate execution time for a command."""
    base_times = {
        ComplexityLevel.SIMPLE: 0.5,
        ComplexityLevel.MODERATE: 2.0,
        ComplexityLevel.COMPLEX: 10.0,
        ComplexityLevel.EXPERT: 30.0
    }
    
    base_time = base_times.get(complexity, 5.0)
    
    # Adjust based on command type
    command_lower = command.lower()
    if "search" in command_lower and not GOOGLE_SEARCH_AVAILABLE:
        base_time *= 2  # Slower without Google search
    elif "analysis" in command_lower:
        base_time *= 3  # Analysis takes longer
    elif "debug" in command_lower:
        base_time *= 4  # Debugging is intensive
    elif "automate" in command_lower:
        base_time *= 5  # Automation can be complex
    
    return base_time

def _assess_confidence(self, command: str, args: List[str]) -> float:
    """Assess confidence level for command execution."""
    confidence = 0.8  # Base confidence
    
    # Check if command exists in tools
    tool_found = False
    for category, tools in self.tools.items():
        if command in tools:
            tool_found = True
            # Higher confidence for well-tested tools
            domain = f"{category.split('_')[0]}_{command}"
            if domain in self.experience:
                exp = self.experience[domain]
                confidence += (exp.success_rate - 0.5) * 0.3  # Scale by experience
    
    if not tool_found:
        confidence -= 0.5
    
    # Check for system dependencies
    if not REQUESTS_AVAILABLE and ("search" in command.lower() or "download" in command.lower()):
        confidence -= 0.3
    
    if not GOOGLE_SEARCH_AVAILABLE and "search" in command.lower():
        confidence -= 0.2
    
    # Adjust for complexity
    complexity = self._get_complexity_assessment(command, args)
    complexity_penalty = {
        ComplexityLevel.SIMPLE: 0.1,
        ComplexityLevel.MODERATE: 0.05,
        ComplexityLevel.COMPLEX: -0.1,
        ComplexityLevel.EXPERT: -0.2
    }
    confidence += complexity_penalty.get(complexity, 0)
    
    return max(0.0, min(1.0, confidence))

def _generate_suggestions(self, command: str, args: List[str], result: CommandResult) -> List[str]:
    """Generate suggestions based on execution results."""
    suggestions = []
    
    # Success-based suggestions
    if result.success:
        # Suggest related commands
        if command in self.tools.get("file_operations", {}):
            suggestions.extend([
                "Consider automating this file operation",
                "You might want to create a backup first",
                "Try searching for similar patterns in other files"
            ])
        elif command in self.tools.get("code_operations", {}):
            suggestions.extend([
                "Run tests to verify changes",
                "Consider linting for code quality",
                "Document any changes made"
            ])
        elif command in self.tools.get("web_operations", {}):
            suggestions.extend([
                "Save important information for future reference",
                "Check for security implications",
                "Consider batch operations for multiple items"
            ])
    
    # Failure-based suggestions
    else:
        error_lower = result.error.lower()
        if "permission" in error_lower:
            suggestions.extend([
                "Check file permissions and ownership",
                "Run with appropriate privileges if needed",
                "Verify write access to the target location"
            ])
        elif "not found" in error_lower:
            suggestions.extend([
                "Verify the file path or command name",
                "Check if the required package is installed",
                "Use 'list' or 'search' commands to locate resources"
            ])
        elif "timeout" in error_lower:
            suggestions.extend([
                "Increase timeout settings",
                "Break down the operation into smaller steps",
                "Check network connectivity if applicable"
            ])
        elif "memory" in error_lower or "out of memory" in error_lower:
            suggestions.extend([
                "Close unnecessary applications",
                "Process smaller datasets",
                "Consider using streaming or chunked processing"
            ])
    
    # Experience-based suggestions
    domain = self._get_command_domain(command)
    if domain in self.experience:
        exp = self.experience[domain]
        if exp.success_rate < 0.7:
            suggestions.append(f"Consider reviewing {domain.replace('_', ' ')} patterns")
        
        # Suggest best practices
        if exp.best_practices:
            best_practice = exp.best_practices[-1]
            suggestions.append(f"Best practice: {best_practice}")
    
    return suggestions[:3]  # Limit to 3 suggestions

def _get_command_domain(self, command: str) -> str:
    """Get the domain for a command."""
    for category in self.tools.keys():
        if command in self.tools[category]:
            return f"{category.split('_')[0]}_{command}"
    return "unknown"

def _record_experience(self, command: str, args: List[str], result: CommandResult):
    """Record execution experience for learning."""
    if not self.settings["learning_enabled"]:
        return
    
    domain = self._get_command_domain(command)
    if domain in self.experience:
        pattern = f"{command} {' '.join(args[:2])}"  # First 2 args for pattern
        practice = f"Executed {command} with confidence {result.confidence:.2f}"
        failure = result.error if not result.success else ""
        
        self.experience[domain].record_attempt(
            success=result.success,
            pattern=pattern[:50],  # Limit pattern length
            practice=practice,
            failure=failure[:100]  # Limit failure description
        )
        
        # Update global metrics
        self.metrics["total_commands"] += 1
        if result.success:
            self.metrics["successful_commands"] += 1
        else:
            self.metrics["failed_commands"] += 1
        
        # Update success rate
        total = self.metrics["successful_commands"] + self.metrics["failed_commands"]
        if total > 0:
            self.metrics["success_rate"] = self.metrics["successful_commands"] / total
        
        # Update most used commands
        self.metrics["most_used_commands"][command] += 1

def _update_metrics(self, execution_time: float):
    """Update performance metrics."""
    # Update average execution time
    if self.metrics["total_commands"] > 0:
        current_avg = self.metrics["average_execution_time"]
        new_avg = (current_avg * (self.metrics["total_commands"] - 1) + execution_time) / self.metrics["total_commands"]
        self.metrics["average_execution_time"] = new_avg
    
    # Update learning progress
    learning_scores = []
    for exp in self.experience.values():
        if exp.total_attempts > 0:
            learning_scores.append(exp.success_rate)
    
    if learning_scores:
        self.metrics["learning_progress"] = sum(learning_scores) / len(learning_scores)

def _show_progress(self, message: str, start_time: float, estimated_time: float):
    """Show progress indicator for long operations."""
    if not self.settings["show_progress"]:
        return
    
    def progress_worker():
        elapsed = 0
        while elapsed < estimated_time:
            time.sleep(2)
            elapsed = time.time() - start_time
            if elapsed > 2:  # Only show progress after 2 seconds
                progress = min(elapsed / estimated_time, 1.0)
                bar_length = 20
                filled_length = int(bar_length * progress)
                bar = 'â–ˆ' * filled_length + 'â–‘' * (bar_length - filled_length)
                percentage = progress * 100
                print(f"\r{message}: [{bar}] {percentage:.1f}% ({elapsed:.1f}s)", end="", flush=True)
    
    if estimated_time > 5:  # Only show progress for operations expected to take > 5 seconds
        thread = threading.Thread(target=progress_worker)
        thread.daemon = True
        thread.start()

def _handle_error(self, command: str, args: List[str], error: Exception) -> CommandResult:
    """Handle errors with intelligent recovery suggestions."""
    error_message = str(error)
    confidence = 0.0
    
    # Common error patterns and solutions
    error_patterns = {
        "FileNotFoundError": {
            "solution": "Check file path and permissions",
            "confidence": 0.8
        },
        "PermissionError": {
            "solution": "Run with appropriate privileges or check file permissions",
            "confidence": 0.7
        },
        "subprocess.CalledProcessError": {
            "solution": "Check command syntax and dependencies",
            "confidence": 0.6
        },
        "ConnectionError": {
            "solution": "Check network connectivity and service availability",
            "confidence": 0.5
        },
        "TimeoutError": {
            "solution": "Increase timeout or break operation into smaller steps",
            "confidence": 0.4
        }
    }
    
    for error_type, pattern in error_patterns.items():
        if error_type in str(type(error)):
            confidence = pattern["confidence"]
            error_message = f"{error_message}. Suggested solution: {pattern['solution']}"
            break
    
    # Auto-retry logic
    if self.settings["auto_retry"] and confidence < 0.5:
        max_retries = self.settings["max_retries"]
        for attempt in range(1, max_retries + 1):
            logger.info(f"Auto-retrying {command} (attempt {attempt}/{max_retries})")
            time.sleep(1 * attempt)  # Exponential backoff
            try:
                result = self._execute_command_internal(command, args)
                if result.success:
                    return result
            except Exception:
                continue
    
    return CommandResult(
        success=False,
        error=error_message,
        confidence=confidence,
        complexity=self._get_complexity_assessment(command, args)
    )

def _execute_command_internal(self, command: str, args: List[str]) -> CommandResult:
    """Internal command execution with proper error handling."""
    start_time = time.time()
    
    try:
        complexity = self._get_complexity_assessment(command, args)
        confidence = self._assess_confidence(command, args)
        
        # Find and execute the command
        for category, tools in self.tools.items():
            if command in tools:
                tool_function = tools[command]
                
                # Validate arguments
                if not self._validate_arguments(command, args, tool_function):
                    return CommandResult(
                        success=False,
                        error="Invalid arguments provided",
                        complexity=complexity,
                        confidence=0.3
                    )
                
                # Execute with progress tracking
                estimated_time = self._estimate_execution_time(command, args, complexity)
                progress_thread = None
                
                if self.settings["show_progress"] and estimated_time > 3:
                    progress_thread = threading.Thread(
                        target=self._show_progress,
                        args=(f"Executing {command}", start_time, estimated_time)
                    )
                    progress_thread.daemon = True
                    progress_thread.start()
                
                # Execute the tool
                if asyncio.iscoroutinefunction(tool_function):
                    result_data = asyncio.run(tool_function(*args))
                else:
                    result_data = tool_function(*args)
                
                # Clean up progress indicator
                if progress_thread:
                    print()  # New line after progress bar
                
                execution_time = time.time() - start_time
                
                # Format result
                if isinstance(result_data, dict):
                    output = json.dumps(result_data, indent=2)
                else:
                    output = str(result_data)
                
                return CommandResult(
                    success=True,
                    output=output,
                    execution_time=execution_time,
                    complexity=complexity,
                    confidence=confidence
                )
        
        # Command not found
        return CommandResult(
            success=False,
            error=f"Command '{command}' not found",
            complexity=complexity,
            confidence=0.0
        )
        
    except Exception as e:
        return self._handle_error(command, args, e)

def _validate_arguments(self, command: str, args: List[str], tool_function) -> bool:
    """Validate command arguments before execution."""
    # Basic validation - could be enhanced based on function signature
    if command in ["read", "analyze", "backup"] and len(args) < 1:
        return False
    if command in ["write"] and len(args) < 2:
        return False
    if command in ["diff"] and len(args) < 2:
        return False
    
    return True

# ============================================================================
# File Operation Tools
# ============================================================================

def _read_file(self, filepath: str) -> str:
    """Read and return file contents."""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            return f.read()
    except UnicodeDecodeError:
        # Try with different encoding
        with open(filepath, 'r', encoding='latin-1') as f:
            return f.read()

def _write_file(self, filepath: str, content: str) -> Dict[str, Any]:
    """Write content to file."""
    # Create directory if it doesn't exist
    Path(filepath).parent.mkdir(parents=True, exist_ok=True)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(content)
    
    return {
        "success": True,
        "filepath": filepath,
        "size": len(content),
        "timestamp": datetime.now().isoformat()
    }

def _list_files(self, directory: str = ".", pattern: str = "*", recursive: bool = False) -> Dict[str, Any]:
    """List files in directory with optional filtering."""
    path = Path(directory)
    
    if not path.exists():
        return {"error": f"Directory {directory} does not exist"}
    
    files = []
    if recursive:
        for file_path in path.rglob(pattern):
            if file_path.is_file():
                stat = file_path.stat()
                files.append({
                    "path": str(file_path),
                    "size": stat.st_size,
                    "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                    "extension": file_path.suffix
                })
    else:
        for file_path in path.glob(pattern):
            if file_path.is_file():
                stat = file_path.stat()
                files.append({
                    "path": str(file_path),
                    "size": stat.st_size,
                    "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
                    "extension": file_path.suffix
                })
    
    return {
        "directory": str(path),
        "pattern": pattern,
        "recursive": recursive,
        "count": len(files),
        "files": files
    }

def _search_files(self, query: str, directory: str = ".", file_types: List[str] = None) -> Dict[str, Any]:
    """Search for files containing query text."""
    path = Path(directory)
    matches = []
    
    if file_types is None:
        file_types = ['.txt', '.py', '.js', '.json', '.md', '.yml', '.yaml', '.cfg', '.ini']
    
    search_patterns = [f"*{ext}" for ext in file_types]
    
    for pattern in search_patterns:
        for file_path in path.rglob(pattern):
            if file_path.is_file():
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                        if query.lower() in content.lower():
                            # Find line numbers for context
                            lines = content.split('\n')
                            line_numbers = []
                            for i, line in enumerate(lines):
                                if query.lower() in line.lower():
                                    line_numbers.append(i + 1)
                            
                            matches.append({
                                "file": str(file_path),
                                "matches": len(line_numbers),
                                "lines": line_numbers[:5],  # First 5 matches
                                "size": file_path.stat().st_size
                            })
                except (UnicodeDecodeError, PermissionError):
                    continue
    
    return {
        "query": query,
        "directory": str(path),
        "file_types": file_types,
        "total_matches": len(matches),
        "files": matches
    }

def _analyze_file(self, filepath: str) -> Dict[str, Any]:
    """Analyze file characteristics and provide insights."""
    path = Path(filepath)
    
    if not path.exists():
        return {"error": f"File {filepath} does not exist"}
    
    stat = path.stat()
    analysis = {
        "path": str(path),
        "size": stat.st_size,
        "created": datetime.fromtimestamp(stat.st_ctime).isoformat(),
        "modified": datetime.fromtimestamp(stat.st_mtime).isoformat(),
        "extension": path.suffix,
        "type": "unknown"
    }
    
    # File type specific analysis
    if path.suffix in ['.py']:
        analysis.update(self._analyze_python_file(path))
    elif path.suffix in ['.js', '.ts']:
        analysis.update(self._analyze_javascript_file(path))
    elif path.suffix in ['.json']:
        analysis.update(self._analyze_json_file(path))
    elif path.suffix in ['.txt', '.md']:
        analysis.update(self._analyze_text_file(path))
    
    return analysis

def _analyze_python_file(self, path: Path) -> Dict[str, Any]:
    """Analyze Python file."""
    try:
        with open(path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        lines = content.split('\n')
        analysis = {
            "type": "python",
            "lines": len(lines),
            "characters": len(content),
            "non_empty_lines": len([line for line in lines if line.strip()]),
            "functions": len(re.findall(r'def\s+(\w+)', content)),
            "classes": len(re.findall(r'class\s+(\w+)', content)),
            "imports": len(re.findall(r'^\s*(import|from)\s+', content, re.MULTILINE)),
            "docstrings": len(re.findall(r'"""', content)) // 2,
            "comments": len(re.findall(r'#.*$', content, re.MULTILINE))
        }
        
        # Complexity estimation
        complexity_indicators = [
            'if', 'elif', 'else', 'for', 'while', 'try', 'except', 'finally',
            'with', 'lambda', 'yield', 'async', 'await'
        ]
        complexity_score = sum(content.count(indicator) for indicator in complexity_indicators)
        analysis["complexity_score"] = complexity_score
        analysis["complexity_level"] = "high" if complexity_score > 50 else "medium" if complexity_score > 20 else "low"
        
        return analysis
    except Exception as e:
        return {"type": "python", "error": str(e)}

def _analyze_javascript_file(self, path: Path) -> Dict[str, Any]:
    """Analyze JavaScript/TypeScript file."""
    try:
        with open(path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        lines = content.split('\n')
        analysis = {
            "type": "javascript",
            "lines": len(lines),
            "characters": len(content),
            "non_empty_lines": len([line for line in lines if line.strip()]),
            "functions": len(re.findall(r'function\s+(\w+)|const\s+(\w+)\s*=', content)),
            "classes": len(re.findall(r'class\s+(\w+)', content)),
            "imports": len(re.findall(r'import.*from', content)),
            "exports": len(re.findall(r'export', content)),
            "comments": len(re.findall(r'//.*$|/\*.*?\*/', content, re.DOTALL | re.MULTILINE))
        }
        
        return analysis
    except Exception as e:
        return {"type": "javascript", "error": str(e)}

def _analyze_json_file(self, path: Path) -> Dict[str, Any]:
    """Analyze JSON file."""
    try:
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        def count_keys(obj, depth=0, max_depth=3):
            if depth > max_depth or not isinstance(obj, dict):
                return 0
            count = len(obj)
            for value in obj.values():
                if isinstance(value, dict):
                    count += count_keys(value, depth + 1, max_depth)
            return count
        
        analysis = {
            "type": "json",
            "size": path.stat().st_size,
            "keys_at_top_level": len(data) if isinstance(data, dict) else 0,
            "estimated_total_keys": count_keys(data),
            "data_type": type(data).__name__,
            "is_valid": True
        }
        
        if isinstance(data, dict):
            analysis["sample_keys"] = list(data.keys())[:5]
        elif isinstance(data, list):
            analysis["array_length"] = len(data)
            if data:
                analysis["sample_elements"] = str(data[:3])
        
        return analysis
    except json.JSONDecodeError as e:
        return {"type": "json", "error": f"Invalid JSON: {str(e)}", "is_valid": False}
    except Exception as e:
        return {"type": "json", "error": str(e)}

def _analyze_text_file(self, path: Path) -> Dict[str, Any]:
    """Analyze text file."""
    try:
        with open(path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        lines = content.split('\n')
        words = re.findall(r'\w+', content)
        
        analysis = {
            "type": "text",
            "lines": len(lines),
            "words": len(words),
            "characters": len(content),
            "non_empty_lines": len([line for line in lines if line.strip()]),
            "avg_words_per_line": len(words) / len(lines) if lines else 0,
            "unique_words": len(set(word.lower() for word in words)),
            "most_common_words": Counter(word.lower() for word in words).most_common(5)
        }
        
        return analysis
    except Exception as e:
        return {"type": "text", "error": str(e)}

def _compare_files(self, file1: str, file2: str) -> Dict[str, Any]:
    """Compare two files and return differences."""
    try:
        with open(file1, 'r', encoding='utf-8') as f1:
            content1 = f1.read()
        with open(file2, 'r', encoding='utf-8') as f2:
            content2 = f2.read()
        
        lines1 = content1.split('\n')
        lines2 = content2.split('\n')
        
        # Simple diff calculation
        max_lines = max(len(lines1), len(lines2))
        differences = []
        
        for i in range(max_lines):
            line1 = lines1[i] if i < len(lines1) else ""
            line2 = lines2[i] if i < len(lines2) else ""
            
            if line1 != line2:
                differences.append({
                    "line_number": i + 1,
                    "file1": line1,
                    "file2": line2
                })
        
        return {
            "file1": file1,
            "file2": file2,
            "file1_lines": len(lines1),
            "file2_lines": len(lines2),
            "total_differences": len(differences),
            "differences": differences[:20],  # Limit to first 20 differences
            "similarity_score": 1 - (len(differences) / max_lines) if max_lines > 0 else 1.0
        }
    except Exception as e:
        return {"error": str(e)}

def _backup_file(self, filepath: str, backup_dir: str = "backups") -> Dict[str, Any]:
    """Create backup of a file."""
    try:
        source_path = Path(filepath)
        if not source_path.exists():
            return {"error": f"File {filepath} does not exist"}
        
        # Create backup directory
        backup_path = Path(backup_dir)
        backup_path.mkdir(exist_ok=True)
        
        # Generate backup filename with timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_filename = f"{source_path.stem}_{timestamp}{source_path.suffix}"
        backup_filepath = backup_path / backup_filename
        
        # Copy file
        import shutil
        shutil.copy2(source_path, backup_filepath)
        
        return {
            "success": True,
            "source": str(source_path),
            "backup": str(backup_filepath),
            "backup_size": backup_filepath.stat().st_size,
            "timestamp": timestamp
        }
    except Exception as e:
        return {"error": str(e)}

def _compress_file(self, filepath: str, compression_type: str = "gzip") -> Dict[str, Any]:
    """Compress a file."""
    try:
        import gzip
        import tarfile
        
        source_path = Path(filepath)
        if not source_path.exists():
            return {"error": f"File {filepath} does not exist"}
        
        if compression_type == "gzip":
            compressed_path = source_path.with_suffix(source_path.suffix + ".gz")
            with open(source_path, 'rb') as f_in:
                with gzip.open(compressed_path, 'wb') as f_out:
                    f_out.writelines(f_in)
            
            return {
                "success": True,
                "original": str(source_path),
                "compressed": str(compressed_path),
                "original_size": source_path.stat().st_size,
                "compressed_size": compressed_path.stat().st_size,
                "compression_ratio": compressed_path.stat().st_size / source_path.stat().st_size
            }
        
        elif compression_type == "tar":
            compressed_path = source_path.with_suffix(".tar.gz")
            with tarfile.open(compressed_path, "w:gz") as tar:
                tar.add(source_path, arcname=source_path.name)
            
            return {
                "success": True,
                "original": str(source_path),
                "compressed": str(compressed_path),
                "original_size": source_path.stat().st_size,
                "compressed_size": compressed_path.stat().st_size,
                "compression_ratio": compressed_path.stat().st_size / source_path.stat().st_size
            }
        
        else:
            return {"error": f"Unsupported compression type: {compression_type}"}
    
    except Exception as e:
        return {"error": str(e)}

def _extract_archive(self, filepath: str, extract_to: str = ".") -> Dict[str, Any]:
    """Extract archive file."""
    try:
        import tarfile
        import zipfile
        
        archive_path = Path(filepath)
        if not archive_path.exists():
            return {"error": f"Archive {filepath} does not exist"}
        
        extract_path = Path(extract_to)
        extract_path.mkdir(exist_ok=True)
        
        if archive_path.suffix == ".gz":
            # Compressed tar file
            if archive_path.name.endswith(".tar.gz"):
                with tarfile.open(archive_path, "r:gz") as tar:
                    tar.extractall(path=extract_path)
                    members = tar.getmembers()
            else:
                # Single gzip file
                import gzip
                extracted_path = extract_path / archive_path.stem
                with gzip.open(archive_path, 'rb') as f_in:
                    with open(extracted_path, 'wb') as f_out:
                        f_out.writelines(f_in)
                members = [{"name": extracted_path.name, "size": extracted_path.stat().st_size}]
        
        elif archive_path.suffix == ".tar":
            with tarfile.open(archive_path, "r") as tar:
                tar.extractall(path=extract_path)
                members = tar.getmembers()
        
        elif archive_path.suffix == ".zip":
            with zipfile.ZipFile(archive_path, 'r') as zip_file:
                zip_file.extractall(path=extract_path)
                members = zip_file.infolist()
        
        else:
            return {"error": f"Unsupported archive format: {archive_path.suffix}"}
        
        return {
            "success": True,
            "archive": str(archive_path),
            "extract_to": str(extract_path),
            "files_extracted": len(members),
            "total_size": sum(getattr(member, 'size', 0) for member in members)
        }
    
    except Exception as e:
        return {"error": str(e)}

def _change_permissions(self, filepath: str, permissions: str) -> Dict[str, Any]:
    """Change file permissions."""
    try:
        import stat
        import octet
        
        path = Path(filepath)
        if not path.exists():
            return {"error": f"File {filepath} does not exist"}
        
        # Parse permissions (e.g., "755", "644", "rwxr-xr-x")
        if permissions.isdigit():
            mode = int(permissions, 8)
        else:
            # Parse symbolic permissions
            mode = 0
            perm_map = {
                'r': 4, 'w': 2, 'x': 1, '-': 0,
                'u': 'u', 'g': 'g', 'o': 'o', 'a': 'a'
            }
            # Simplified symbolic parsing
            if permissions.startswith('u='):
                u_perms = permissions[2:5]
                mode |= sum(perm_map[p] for p in u_perms if p != '-') << 6
            if len(permissions) > 5 and permissions[5] == 'g=':
                g_perms = permissions[6:9]
                mode |= sum(perm_map[p] for p in g_perms if p != '-') << 3
            if len(permissions) > 9 and permissions[10] == 'o=':
                o_perms = permissions[11:14]
                mode |= sum(perm_map[p] for p in o_perms if p != '-')
        
        # Apply permissions
        path.chmod(mode)
        
        # Get current permissions
        current_stat = path.stat()
        current_mode = octet(current_stat.st_mode)[-3:]
        
        return {
            "success": True,
            "file": str(path),
            "requested_permissions": permissions,
            "applied_permissions": current_mode,
            "octal_mode": oct(current_mode),
            "timestamp": datetime.now().isoformat()
        }
    
    except Exception as e:
        return {"error": str(e)}

# ============================================================================
# System Command Tools
# ============================================================================

def _execute_command(self, command: str, shell: bool = True) -> Dict[str, Any]:
    """Execute a system command and return results."""
    try:
        start_time = time.time()
        result = subprocess.run(
            command,
            shell=shell,
            capture_output=True,
            text=True,
            timeout=self.settings["timeout_seconds"]
        )
        execution_time = time.time() - start_time
        
        return {
            "success": result.returncode == 0,
            "return_code": result.returncode,
            "stdout": result.stdout,
            "stderr": result.stderr,
            "execution_time": execution_time,
            "command": command
        }
    
    except subprocess.TimeoutExpired:
        return {
            "success": False,
            "error": "Command timed out",
            "timeout": self.settings["timeout_seconds"],
            "command": command
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "command": command
        }

def _get_process_info(self, pid: Optional[int] = None) -> Dict[str, Any]:
    """Get process information."""
    try:
        if not PSUTIL_AVAILABLE:
            return {"error": "psutil not available. Install with: pip install psutil"}
        
        if pid is None:
            # Return info about current process
            pid = os.getpid()
        
        process = psutil.Process(pid)
        return {
            "pid": pid,
            "name": process.name(),
            "status": process.status(),
            "cpu_percent": process.cpu_percent(),
            "memory_info": {
                "rss": process.memory_info().rss,
                "vms": process.memory_info().vms,
                "percent": process.memory_percent()
            },
            "create_time": datetime.fromtimestamp(process.create_time()).isoformat(),
            "cmdline": process.cmdline(),
            "cwd": process.cwd(),
            "num_threads": process.num_threads(),
            "connections": len(process.connections())
        }
    
    except Exception as e:
        return {"error": str(e)}

def _get_disk_usage(self, path: str = ".") -> Dict[str, Any]:
    """Get disk usage information."""
    try:
        if not PSUTIL_AVAILABLE:
            # Fallback to subprocess
            result = subprocess.run(['du', '-sh', path], capture_output=True, text=True)
            if result.returncode == 0:
                return {
                    "path": path,
                    "size": result.stdout.strip().split()[0],
                    "method": "subprocess"
                }
            return {"error": "Could not get disk usage"}
        
        disk_usage = psutil.disk_usage(path)
        return {
            "path": str(Path(path).absolute()),
            "total": disk_usage.total,
            "used": disk_usage.used,
            "free": disk_usage.free,
            "percent": (disk_usage.used / disk_usage.total) * 100,
            "method": "psutil"
        }
    
    except Exception as e:
        return {"error": str(e)}

def _check_network(self) -> Dict[str, Any]:
    """Check network connectivity."""
    try:
        if not REQUESTS_AVAILABLE:
            # Basic connectivity check using subprocess
            result = subprocess.run(['ping', '-c', '1', 'google.com'], 
                                  capture_output=True, timeout=5)
            return {
                "connected": result.returncode == 0,
                "method": "ping",
                "target": "google.com"
            }
        
        # Test connectivity with requests
        response = requests.get('https://www.google.com', timeout=5)
        return {
            "connected": response.status_code == 200,
            "method": "http",
            "response_time": response.elapsed.total_seconds(),
            "status_code": response.status_code
        }
    
    except Exception as e:
        return {
            "connected": False,
            "error": str(e),
            "method": "error"
        }

def _get_system_info(self) -> Dict[str, Any]:
    """Get system information."""
    import platform
    import sys
    
    try:
        info = {
            "platform": platform.system(),
            "platform_version": platform.version(),
            "architecture": platform.machine(),
            "python_version": sys.version,
            "hostname": platform.node(),
            "processor": platform.processor(),
            "current_directory": str(Path.cwd()),
            "user": os.getenv('USER', os.getenv('USERNAME', 'unknown')),
            "timestamp": datetime.now().isoformat()
        }
        
        # Add psutil information if available
        if PSUTIL_AVAILABLE:
            info.update({
                "cpu_count": psutil.cpu_count(),
                "cpu_freq": psutil.cpu_freq()._asdict() if psutil.cpu_freq() else None,
                "memory_total": psutil.virtual_memory().total,
                "boot_time": datetime.fromtimestamp(psutil.boot_time()).isoformat()
            })
        
        return info
    
    except Exception as e:
        return {"error": str(e)}

def _install_package(self, package: str, package_manager: str = "pip") -> Dict[str, Any]:
    """Install a package using specified package manager."""
    try:
        if package_manager == "pip":
            command = f"pip install {package}"
        elif package_manager == "conda":
            command = f"conda install -y {package}"
        elif package_manager == "apt":
            command = f"sudo apt-get install {package}"
        else:
            return {"error": f"Unsupported package manager: {package_manager}"}
        
        result = subprocess.run(command, shell=True, capture_output=True, text=True)
        
        return {
            "success": result.returncode == 0,
            "package": package,
            "manager": package_manager,
            "command": command,
            "stdout": result.stdout,
            "stderr": result.stderr,
            "return_code": result.returncode
        }
    
    except Exception as e:
        return {"error": str(e)}

def _control_service(self, action: str, service_name: str) -> Dict[str, Any]:
    """Control system services (Linux/Unix)."""
    try:
        if platform.system() != "Linux":
            return {"error": "Service control only supported on Linux"}
        
        valid_actions = ["start", "stop", "restart", "status", "enable", "disable"]
        if action not in valid_actions:
            return {"error": f"Invalid action. Must be one of: {valid_actions}"}
        
        command = f"sudo systemctl {action} {service_name}"
        result = subprocess.run(command, shell=True, capture_output=True, text=True)
        
        return {
            "success": result.returncode == 0,
            "action": action,
            "service": service_name,
            "command": command,
            "stdout": result.stdout,
            "stderr": result.stderr,
            "return_code": result.returncode
        }
    
    except Exception as e:
        return {"error": str(e)}

def _analyze_logs(self, logfile: str, pattern: Optional[str] = None, lines: int = 100) -> Dict[str, Any]:
    """Analyze log file for patterns and statistics."""
    try:
        path = Path(logfile)
        if not path.exists():
            return {"error": f"Log file {logfile} does not exist"}
        
        with open(path, 'r', encoding='utf-8') as f:
            all_lines = f.readlines()
        
        # Get recent lines
        recent_lines = all_lines[-lines:] if lines > 0 else all_lines
        
        # Filter by pattern if provided
        if pattern:
            filtered_lines = [line for line in recent_lines if pattern.lower() in line.lower()]
        else:
            filtered_lines = recent_lines
        
        # Analyze patterns
        error_lines = [line for line in filtered_lines if 'error' in line.lower()]
        warning_lines = [line for line in filtered_lines if 'warning' in line.lower()]
        info_lines = [line for line in filtered_lines if 'info' in line.lower()]
        
        # Extract timestamps if present
        timestamps = []
        timestamp_pattern = r'\d{4}-\d{2}-\d{2}[T ]\d{2}:\d{2}:\d{2}'
        for line in filtered_lines[:20]:  # Check first 20 lines
            matches = re.findall(timestamp_pattern, line)
            timestamps.extend(matches)
        
        return {
            "logfile": str(path),
            "total_lines": len(all_lines),
            "analyzed_lines": len(filtered_lines),
            "pattern_filter": pattern,
            "statistics": {
                "errors": len(error_lines),
                "warnings": len(warning_lines),
                "info": len(info_lines),
                "other": len(filtered_lines) - len(error_lines) - len(warning_lines) - len(info_lines)
            },
            "sample_errors": error_lines[:5],
            "sample_warnings": warning_lines[:5],
            "timestamps_found": len(timestamps),
            "date_range": {
                "earliest": timestamps[0] if timestamps else None,
                "latest": timestamps[-1] if timestamps else None
            }
        }
    
    except Exception as e:
        return {"error": str(e)}

# ============================================================================
# Web Operation Tools
# ============================================================================

def _web_search(self, query: str, num_results: int = 10) -> Dict[str, Any]:
    """Perform web search using Google or fallback methods."""
    try:
        if GOOGLE_SEARCH_AVAILABLE:
            start_time = time.time()
            search_results = []
            
            for result in search(query, num_results=num_results):
                search_results.append({
                    "title": getattr(result, 'title', 'No title'),
                    "url": getattr(result, 'url', ''),
                    "description": getattr(result, 'description', 'No description')
                })
            
            execution_time = time.time() - start_time
            
            return {
                "success": True,
                "query": query,
                "results": search_results,
                "total_results": len(search_results),
                "execution_time": execution_time,
                "method": "google_search"
            }
        
        elif REQUESTS_AVAILABLE:
            # Fallback to DuckDuckGo or other search APIs
            start_time = time.time()
            
            # Use DuckDuckGo instant answer API
            response = requests.get(
                'https://api.duckduckgo.com/',
                params={
                    'q': query,
                    'format': 'json',
                    'no_html': '1',
                    'skip_disambig': '1'
                },
                timeout=10
            )
            
            data = response.json()
            results = []
            
            # Extract results from various fields
            for field in ['RelatedTopics', 'Results']:
                if field in data and isinstance(data[field], list):
                    for item in data[field][:num_results//2]:
                        if isinstance(item, dict):
                            results.append({
                                "title": item.get('Text', 'No title'),
                                "url": item.get('FirstURL', ''),
                                "description": item.get('Text', '')
                            })
            
            execution_time = time.time() - start_time
            
            return {
                "success": True,
                "query": query,
                "results": results,
                "total_results": len(results),
                "execution_time": execution_time,
                "method": "duckduckgo_api"
            }
        
        else:
            return {
                "success": False,
                "error": "No search capabilities available. Install google-search or requests.",
                "query": query
            }
    
    except Exception as e:
        return {"success": False, "error": str(e), "query": query}

def _download_file(self, url: str, filepath: str) -> Dict[str, Any]:
    """Download file from URL."""
    try:
        if not REQUESTS_AVAILABLE:
            return {"error": "requests not available. Install with: pip install requests"}
        
        start_time = time.time()
        response = requests.get(url, stream=True, timeout=30)
        response.raise_for_status()
        
        # Create directory if needed
        Path(filepath).parent.mkdir(parents=True, exist_ok=True)
        
        # Download with progress
        total_size = int(response.headers.get('content-length', 0))
        downloaded = 0
        
        with open(filepath, 'wb') as f:
            for chunk in response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded += len(chunk)
                    
                    # Show progress for large files
                    if self.settings["show_progress"] and total_size > 1024*1024:
                        progress = (downloaded / total_size) * 100
                        print(f"\rDownloading: {progress:.1f}%", end="", flush=True)
        
        execution_time = time.time() - start_time
        
        if self.settings["show_progress"] and total_size > 1024*1024:
            print()  # New line after progress
        
        return {
            "success": True,
            "url": url,
            "filepath": filepath,
            "size": downloaded,
            "execution_time": execution_time,
            "speed_mbps": (downloaded / (1024*1024)) / execution_time if execution_time > 0 else 0
        }
    
    except Exception as e:
        return {"success": False, "error": str(e), "url": url}

def _check_http_status(self, url: str, method: str = "GET") -> Dict[str, Any]:
    """Check HTTP status of URL."""
    try:
        if not REQUESTS_AVAILABLE:
            return {"error": "requests not available"}
        
        start_time = time.time()
        
        # Use curl subprocess if available for more detailed info
        try:
            result = subprocess.run(
                ['curl', '-I', '-s', '-w', '%{http_code}|%{time_total}', url],
                capture_output=True,
                text=True,
                timeout=10
            )
            
            if result.returncode == 0:
                lines = result.stdout.split('\n')
                status_line = [line for line in lines if line.startswith('HTTP/')]
                timing_line = result.stderr.strip()
                
                if status_line and timing_line:
                    status_info = status_line[0]
                    timing_info = timing_line.split('|')
                    
                    return {
                        "success": True,
                        "url": url,
                        "method": method,
                        "status_line": status_info,
                        "http_code": timing_info[0] if len(timing_info) > 0 else "unknown",
                        "response_time": float(timing_info[1]) if len(timing_info) > 1 else 0,
                        "method": "curl"
                    }
        except (subprocess.TimeoutExpired, FileNotFoundError):
            pass
        
        # Fallback to requests
        response = requests.request(method, url, timeout=10)
        execution_time = time.time() - start_time
        
        return {
            "success": True,
            "url": url,
            "method": method,
            "status_code": response.status_code,
            "status_text": response.reason,
            "response_time": execution_time,
            "headers": dict(response.headers),
            "method": "requests"
        }
    
    except Exception as e:
        return {"success": False, "error": str(e), "url": url}

def _curl_request(self, url: str, method: str = "GET", headers: Dict[str, str] = None, data: str = None) -> Dict[str, Any]:
    """Make HTTP request using curl."""
    try:
        cmd = ['curl', '-s', '-X', method]
        
        if headers:
            for key, value in headers.items():
                cmd.extend(['-H', f'{key}: {value}'])
        
        if data:
            cmd.extend(['--data', data])
        
        cmd.append(url)
        
        start_time = time.time()
        result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
        execution_time = time.time() - start_time
        
        return {
            "success": result.returncode == 0,
            "url": url,
            "method": method,
            "stdout": result.stdout,
            "stderr": result.stderr,
            "return_code": result.returncode,
            "execution_time": execution_time
        }
    
    except Exception as e:
        return {"success": False, "error": str(e), "url": url}

def _make_api_call(self, endpoint: str, method: str = "GET", headers: Dict[str, str] = None, data: Dict[str, Any] = None) -> Dict[str, Any]:
    """Make API call with JSON data."""
    try:
        if not REQUESTS_AVAILABLE:
            return {"error": "requests not available"}
        
        start_time = time.time()
        
        response = requests.request(
            method,
            endpoint,
            headers=headers or {},
            json=data if data else None,
            timeout=30
        )
        
        execution_time = time.time() - start_time
        
        try:
            response_json = response.json()
        except:
            response_json = {"raw_response": response.text}
        
        return {
            "success": response.status_code < 400,
            "url": endpoint,
            "method": method,
            "status_code": response.status_code,
            "status_text": response.reason,
            "headers": dict(response.headers),
            "response": response_json,
            "execution_time": execution_time
        }
    
    except Exception as e:
        return {"success": False, "error": str(e), "endpoint": endpoint}

# ============================================================================
# Code Operation Tools
# ============================================================================

def _run_code(self, code: str, language: str = "python") -> Dict[str, Any]:
    """Run code in specified language."""
    try:
        start_time = time.time()
        
        if language.lower() == "python":
            # Execute Python code
            result = subprocess.run(
                ['python3', '-c', code],
                capture_output=True,
                text=True,
                timeout=30
            )
            
            return {
                "success": result.returncode == 0,
                "language": "python",
                "stdout": result.stdout,
                "stderr": result.stderr,
                "return_code": result.returncode,
                "execution_time": time.time() - start_time
            }
        
        elif language.lower() == "javascript":
            # Execute JavaScript code
            result = subprocess.run(
                ['node', '-e', code],
                capture_output=True,
                text=True,
                timeout=30
            )
            
            return {
                "success": result.returncode == 0,
                "language": "javascript",
                "stdout": result.stdout,
                "stderr": result.stderr,
                "return_code": result.returncode,
                "execution_time": time.time() - start_time
            }
        
        else:
            return {"error": f"Unsupported language: {language}"}
    
    except Exception as e:
        return {"error": str(e), "language": language}

def _lint_code(self, filepath: str) -> Dict[str, Any]:
    """Lint code file."""
    try:
        path = Path(filepath)
        if not path.exists():
            return {"error": f"File {filepath} does not exist"}
        
        if path.suffix == '.py':
            # Use pylint or flake8 for Python
            linters = ['pylint', 'flake8']
            results = {}
            
            for linter in linters:
                try:
                    result = subprocess.run(
                        [linter, str(path)],
                        capture_output=True,
                        text=True,
                        timeout=30
                    )
                    
                    results[linter] = {
                        "available": True,
                        "return_code": result.returncode,
                        "output": result.stdout,
                        "errors": result.stderr
                    }
                except (subprocess.TimeoutExpired, FileNotFoundError):
                    results[linter] = {"available": False}
            
            return {
                "file": str(path),
                "language": "python",
                "linters": results
            }
        
        elif path.suffix in ['.js', '.ts']:
            # Use eslint for JavaScript/TypeScript
            try:
                result = subprocess.run(
                    ['eslint', str(path)],
                    capture_output=True,
                    text=True,
                    timeout=30
                )
                
                return {
                    "file": str(path),
                    "language": "javascript",
                    "return_code": result.returncode,
                    "output": result.stdout,
                    "errors": result.stderr
                }
            except (subprocess.TimeoutExpired, FileNotFoundError):
                return {
                    "file": str(path),
                    "language": "javascript",
                    "error": "ESLint not found. Install with: npm install -g eslint"
                }
        
        else:
            return {"error": f"Linting not supported for {path.suffix} files"}
    
    except Exception as e:
        return {"error": str(e)}

def _format_code(self, filepath: str) -> Dict[str, Any]:
    """Format code file."""
    try:
        path = Path(filepath)
        if not path.exists():
            return {"error": f"File {filepath} does not exist"}
        
        if path.suffix == '.py':
            # Use black or autopep8 for Python
            formatters = ['black', 'autopep8']
            results = {}
            
            for formatter in formatters:
                try:
                    # Test if formatter is available
                    subprocess.run(
                        [formatter, '--version'],
                        capture_output=True,
                        timeout=5
                    )
                    
                    # Format the file
                    if formatter == 'black':
                        result = subprocess.run(
                            [formatter, str(path)],
                            capture_output=True,
                            text=True,
                            timeout=30
                        )
                    else:  # autopep8
                        # Read file, format, write back
                        with open(path, 'r') as f:
                            content = f.read()
                        
                        result = subprocess.run(
                            [formatter, '-i', str(path)],
                            capture_output=True,
                            text=True,
                            timeout=30
                        )
                    
                    results[formatter] = {
                        "available": True,
                        "success": result.returncode == 0,
                        "output": result.stdout,
                        "errors": result.stderr
                    }
                except (subprocess.TimeoutExpired, FileNotFoundError):
                    results[formatter] = {"available": False}
            
            return {
                "file": str(path),
                "language": "python",
                "formatters": results
            }
        
        else:
            return {"error": f"Code formatting not supported for {path.suffix} files"}
    
    except Exception as e:
        return {"error": str(e)}

def _run_tests(self, filepath: str = ".", test_pattern: str = "*test*.py") -> Dict[str, Any]:
    """Run tests in directory or file."""
    try:
        path = Path(filepath)
        
        if path.is_file():
            # Run tests for specific file
            if path.suffix == '.py':
                result = subprocess.run(
                    ['python3', '-m', 'pytest', str(path), '-v'],
                    capture_output=True,
                    text=True,
                    timeout=60
                )
                
                return {
                    "success": result.returncode == 0,
                    "target": str(path),
                    "stdout": result.stdout,
                    "stderr": result.stderr,
                    "return_code": result.returncode
                }
            else:
                return {"error": "Testing only supported for Python files"}
        
        elif path.is_dir():
            # Run tests in directory
            test_files = list(path.rglob(test_pattern))
            
            if not test_files:
                return {"error": f"No test files found matching pattern: {test_pattern}"}
            
            result = subprocess.run(
                ['python3', '-m', 'pytest', str(path), '-v', '--tb=short'],
                capture_output=True,
                text=True,
                timeout=120
            )
            
            return {
                "success": result.returncode == 0,
                "target": str(path),
                "test_files_found": len(test_files),
                "stdout": result.stdout,
                "stderr": result.stderr,
                "return_code": result.returncode
            }
        
        else:
            return {"error": f"Path {filepath} does not exist"}
    
    except Exception as e:
        return {"error": str(e)}

def _debug_code(self, filepath: str) -> Dict[str, Any]:
    """Debug code using pdb or debugger."""
    try:
        path = Path(filepath)
        if not path.exists():
            return {"error": f"File {filepath} does not exist"}
        
        if path.suffix == '.py':
            # Use pdb to debug Python code
            debug_script = f"""
import pdb pdb.set_trace() exec(open('{path}').read()) """

            result = subprocess.run(
                ['python3', '-c', debug_script],
                capture_output=True,
                text=True,
                timeout=60
            )
            
            return {
                "success": result.returncode == 0,
                "file": str(path),
                "debugger": "pdb",
                "stdout": result.stdout,
                "stderr": result.stderr,
                "return_code": result.returncode
            }
        
        else:
            return {"error": f"Debugging not supported for {path.suffix} files"}
    
    except Exception as e:
        return {"error": str(e)}

def _profile_code(self, filepath: str) -> Dict[str, Any]:
    """Profile code performance."""
    try:
        path = Path(filepath)
        if not path.exists():
            return {"error": f"File {filepath} does not exist"}
        
        if path.suffix == '.py':
            # Use cProfile to profile Python code
            profile_script = f"""
import cProfile import pstats cProfile.run('exec(open(\'{path}\').read())', 'profile_output') stats = pstats.Stats('profile_output') stats.sort_stats('cumulative') stats.print_stats(10) """

            result = subprocess.run(
                ['python3', '-c', profile_script],
                capture_output=True,
                text=True,
                timeout=60
            )
            
            return {
                "success": result.returncode == 0,
                "file": str(path),
                "profiler": "cProfile",
                "output": result.stdout,
                "stderr": result.stderr,
                "return_code": result.returncode
            }
        
        else:
            return {"error": f"Profiling not supported for {path.suffix} files"}
    
    except Exception as e:
        return {"error": str(e)}

def _generate_docstring(self, filepath: str, style: str = "google") -> Dict[str, Any]:
    """Generate docstrings for code."""
    try:
        path = Path(filepath)
        if not path.exists():
            return {"error": f"File {filepath} does not exist"}
        
        if path.suffix == '.py':
            # Use pydocstyle or similar to generate docstrings
            try:
                result = subprocess.run(
                    ['pydocstyle', '--docstring-style', style, str(path)],
                    capture_output=True,
                    text=True,
                    timeout=30
                )
                
                return {
                    "success": result.returncode == 0,
                    "file": str(path),
                    "style": style,
                    "output": result.stdout,
                    "errors": result.stderr,
                    "return_code": result.returncode
                }
            except (subprocess.TimeoutExpired, FileNotFoundError):
                return {
                    "error": "pydocstyle not found. Install with: pip install pydocstyle",
                    "file": str(path),
                    "suggested_style": style
                }
        
        else:
            return {"error": f"Docstring generation not supported for {path.suffix} files"}
    
    except Exception as e:
        return {"error": str(e)}

# ============================================================================
# Data Operation Tools
# ============================================================================

def _parse_csv(self, filepath: str, delimiter: str = ",", encoding: str = "utf-8") -> Dict[str, Any]:
    """Parse CSV file and return structured data."""
    try:
        import csv
        
        path = Path(filepath)
        if not path.exists():
            return {"error": f"File {filepath} does not exist"}
        
        data = []
        headers = []
        
        with open(path, 'r', encoding=encoding) as f:
            csv_reader = csv.reader(f, delimiter=delimiter)
            
            # Read headers
            try:
                headers = next(csv_reader)
            except StopIteration:
                return {"error": "Empty CSV file"}
            
            # Read data rows
            for row_num, row in enumerate(csv_reader, start=2):
                if len(row) != len(headers):
                    # Handle rows with different number of columns
                    padded_row = row + [''] * (len(headers) - len(row))
                    truncated_row = padded_row[:len(headers)]
                else:
                    truncated_row = row
                
                row_dict = dict(zip(headers, truncated_row))
                row_dict['_row_number'] = row_num
                data.append(row_dict)
        
        # Analyze data
        analysis = {
            "total_rows": len(data),
            "total_columns": len(headers),
            "headers": headers,
            "sample_data": data[:3] if data else [],
            "data_types": {}
        }
        
        # Analyze column types
        for header in headers:
            values = [row.get(header, '') for row in data[:10]]  # Sample first 10 rows
            numeric_count = sum(1 for v in values if v.replace('.', '').replace('-', '').isdigit())
            date_count = sum(1 for v in values if self._is_date(v))
            
            if numeric_count > len(values) * 0.8:
                analysis["data_types"][header] = "numeric"
            elif date_count > len(values) * 0.8:
                analysis["data_types"][header] = "date"
            else:
                analysis["data_types"][header] = "text"
        
        return {
            "success": True,
            "file": str(path),
            "delimiter": delimiter,
            "encoding": encoding,
            **analysis
        }
    
    except Exception as e:
        return {"error": str(e)}

def _is_date(self, value: str) -> bool:
    """Check if string looks like a date."""
    date_patterns = [
        r'\d{4}-\d{2}-\d{2}',  # YYYY-MM-DD
        r'\d{2}/\d{2}/\d{4}',  # MM/DD/YYYY
        r'\d{2}-\d{2}-\d{4}',  # MM-DD-YYYY
        r'\d{1,2}/\d{1,2}/\d{2,4}'  # M/D/YYYY
    ]
    
    return any(re.match(pattern, value.strip()) for pattern in date_patterns)

def _parse_json(self, filepath: str) -> Dict[str, Any]:
    """Parse JSON file and return structured data."""
    try:
        path = Path(filepath)
        if not path.exists():
            return {"error": f"File {filepath} does not exist"}
        
        with open(path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        def analyze_structure(obj, depth=0, max_depth=3):
            if depth > max_depth:
                return "max_depth_reached"
            
            if isinstance(obj, dict):
                return {
                    "type": "object",
                    "keys": list(obj.keys())[:10],  # Limit to first 10 keys
                    "total_keys": len(obj),
                    "key_types": {k: type(v).__name__ for k, v in list(obj.items())[:5]}
                }
            elif isinstance(obj, list):
                return {
                    "type": "array",
                    "length": len(obj),
                    "element_types": [type(item).__name__ for item in obj[:5]],
                    "sample_elements": obj[:3] if len(obj) <= 10 else obj[:3]
                }
            else:
                return {"type": type(obj).__name__, "value": str(obj)[:50]}
        
        return {
            "success": True,
            "file": str(path),
            "structure": analyze_structure(data),
            "size_bytes": path.stat().st_size,
            "data": data if len(str(data)) < 1000 else f"Data too large to display ({len(str(data))} chars)"
        }
    
    except Exception as e:
        return {"error": str(e)}

def _parse_xml(self, filepath: str) -> Dict[str, Any]:
    """Parse XML file and return structured data."""
    try:
        import xml.etree.ElementTree as ET
        
        path = Path(filepath)
        if not path.exists():
            return {"error": f"File {filepath} does not exist"}
        
        tree = ET.parse(path)
        root = tree.getroot()
        
        def analyze_element(element, depth=0, max_depth=3):
            if depth > max_depth:
                return "max_depth_reached"
            
            children = list(element)
            attributes = element.attrib
            text = element.text.strip() if element.text else None
            
            analysis = {
                "tag": element.tag,
                "attributes": attributes,
                "text": text[:100] if text and len(text) > 100 else text,
                "children": len(children),
                "child_tags": list(set(child.tag for child in children[:10]))[:5]
            }
            
            if children and depth < max_depth:
                analysis["child_analysis"] = [analyze_element(child, depth + 1, max_depth) for child in children[:3]]
            
            return analysis
        
        return {
            "success": True,
            "file": str(path),
            "root_tag": root.tag,
            "root_attributes": root.attrib,
            "structure": analyze_element(root),
            "total_elements": len(list(root.iter())),
            "size_bytes": path.stat().st_size
        }
    
    except Exception as e:
        return {"error": str(e)}

def _query_database(self, connection_string: str, query: str) -> Dict[str, Any]:
    """Query database using SQL."""
    try:
        import sqlite3
        
        # Simple SQLite support
        if connection_string.startswith("sqlite://"):
            db_path = connection_string.replace("sqlite://", "")
            conn = sqlite3.connect(db_path)
            cursor = conn.cursor()
            
            start_time = time.time()
            cursor.execute(query)
            
            if query.strip().upper().startswith('SELECT'):
                results = cursor.fetchall()
                columns = [description[0] for description in cursor.description] if cursor.description else []
                execution_time = time.time() - start_time
                
                return {
                    "success": True,
                    "database": "sqlite",
                    "query": query,
                    "columns": columns,
                    "row_count": len(results),
                    "results": results[:100],  # Limit to first 100 rows
                    "execution_time": execution_time
                }
            else:
                conn.commit()
                execution_time = time.time() - start_time
                
                return {
                    "success": True,
                    "database": "sqlite",
                    "query": query,
                    "affected_rows": cursor.rowcount,
                    "execution_time": execution_time
                }
        
        else:
            return {"error": "Only SQLite databases supported in this version"}
    
    except Exception as e:
        return {"error": str(e)}

def _analyze_data(self, data: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Perform statistical analysis on data."""
    try:
        if not data:
            return {"error": "No data provided"}
        
        # Extract numeric columns
        numeric_data = {}
        text_columns = []
        
        for key in data[0].keys():
            values = [row.get(key, 0) for row in data if key in row]
            
            # Try to convert to numbers
            numeric_values = []
            for v in values:
                try:
                    if isinstance(v, str):
                        v = v.replace(',', '')  # Remove commas
                    numeric_values.append(float(v))
                except (ValueError, TypeError):
                    break
            else:
                if numeric_values:
                    numeric_data[key] = numeric_values
                else:
                    text_columns.append(key)
        
        # Statistical analysis
        analysis = {
            "total_records": len(data),
            "columns": list(data[0].keys()),
            "numeric_columns": list(numeric_data.keys()),
            "text_columns": text_columns,
            "statistics": {}
        }
        
        # Calculate statistics for numeric columns
        for column, values in numeric_data.items():
            if values:
                analysis["statistics"][column] = {
                    "count": len(values),
                    "mean": sum(values) / len(values),
                    "min": min(values),
                    "max": max(values),
                    "median": sorted(values)[len(values) // 2],
                    "sum": sum(values)
                }
        
        # Text analysis
        if text_columns:
            analysis["text_analysis"] = {}
            for column in text_columns[:3]:  # Analyze first 3 text columns
                values = [str(row.get(column, '')) for row in data if column in row]
                word_counts = [len(v.split()) for v in values]
                char_counts = [len(v) for v in values]
                
                analysis["text_analysis"][column] = {
                    "unique_values": len(set(values)),
                    "most_common": Counter(values).most_common(5),
                    "avg_word_count": sum(word_counts) / len(word_counts) if word_counts else 0,
                    "avg_char_count": sum(char_counts) / len(char_counts) if char_counts else 0
                }
        
        return analysis
    
    except Exception as e:
        return {"error": str(e)}

def _create_visualization(self, data: List[Dict[str, Any]], chart_type: str = "bar", output_file: str = "chart.png") -> Dict[str, Any]:
    """Create data visualization."""
    try:
        import matplotlib.pyplot as plt
        
        if not data:
            return {"error": "No data provided"}
        
        # Extract numeric data for plotting
        numeric_data = {}
        for key in data[0].keys():
            values = []
            for row in data:
                try:
                    v = row.get(key, 0)
                    if isinstance(v, str):
                        v = v.replace(',', '')
                    values.append(float(v))
                except (ValueError, TypeError):
                    break
            else:
                if values:
                    numeric_data[key] = values
        
        if not numeric_data:
            return {"error": "No numeric data found for visualization"}
        
        plt.figure(figsize=(10, 6))
        
        if chart_type == "bar":
            for i, (column, values) in enumerate(numeric_data.items()):
                plt.bar([j + i*0.2 for j in range(len(values))], values, 
                       width=0.2, label=column, alpha=0.7)
            plt.xlabel('Index')
            plt.ylabel('Value')
            plt.title('Bar Chart')
            plt.legend()
        
        elif chart_type == "line":
            for i, (column, values) in enumerate(numeric_data.items()):
                plt.plot(values, label=column, marker='o')
            plt.xlabel('Index')
            plt.ylabel('Value')
            plt.title('Line Chart')
            plt.legend()
        
        elif chart_type == "scatter":
            keys = list(numeric_data.keys())
            if len(keys) >= 2:
                plt.scatter(numeric_data[keys[0]], numeric_data[keys[1]], alpha=0.7)
                plt.xlabel(keys[0])
                plt.ylabel(keys[1])
                plt.title('Scatter Plot')
            else:
                return {"error": "Scatter plot requires at least 2 numeric columns"}
        
        plt.tight_layout()
        plt.savefig(output_file, dpi=300, bbox_inches='tight')
        plt.close()
        
        return {
            "success": True,
            "chart_type": chart_type,
            "output_file": output_file,
            "columns_visualized": list(numeric_data.keys()),
            "data_points": sum(len(v) for v in numeric_data.values())
        }
    
    except Exception as e:
        return {"error": str(e)}

def _clean_data(self, data: List[Dict[str, Any]], operations: List[str]) -> Dict[str, Any]:
    """Clean and preprocess data."""
    try:
        if not data:
            return {"error": "No data provided"}
        
        cleaned_data = data.copy()
        operations_applied = []
        
        for operation in operations:
            if operation == "remove_duplicates":
                seen = set()
                unique_data = []
                for row in cleaned_data:
                    row_str = json.dumps(row, sort_keys=True)
                    if row_str not in seen:
                        seen.add(row_str)
                        unique_data.append(row)
                cleaned_data = unique_data
                operations_applied.append("remove_duplicates")
            
            elif operation == "remove_nulls":
                cleaned_data = [row for row in cleaned_data if any(v for v in row.values() if v is not None and v != "")]
                operations_applied.append("remove_nulls")
            
            elif operation == "trim_whitespace":
                for row in cleaned_data:
                    for key, value in row.items():
                        if isinstance(value, str):
                            row[key] = value.strip()
                operations_applied.append("trim_whitespace")
            
            elif operation == "normalize_case":
                for row in cleaned_data:
                    for key, value in row.items():
                        if isinstance(value, str):
                            row[key] = value.lower()
                operations_applied.append("normalize_case")
            
            elif operation == "convert_numbers":
                for row in cleaned_data:
                    for key, value in row.items():
                        if isinstance(value, str):
                            try:
                                # Remove common formatting
                                clean_value = value.replace(',', '').replace('$', '').replace('%', '')
                                row[key] = float(clean_value)
                            except ValueError:
                                pass  # Keep as string if conversion fails
                operations_applied.append("convert_numbers")
        
        return {
            "success": True,
            "original_count": len(data),
            "cleaned_count": len(cleaned_data),
            "removed_count": len(data) - len(cleaned_data),
            "operations_applied": operations_applied,
            "sample_cleaned_data": cleaned_data[:3]
        }
    
    except Exception as e:
        return {"error": str(e)}

# ============================================================================
# Analysis Operation Tools
# ============================================================================

def _review_code(self, filepath: str, review_type: str = "general") -> Dict[str, Any]:
    """Perform code review."""
    try:
        path = Path(filepath)
        if not path.exists():
            return {"error": f"File {filepath} does not exist"}
        
        with open(path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        lines = content.split('\n')
        
        review = {
            "file": str(path),
            "review_type": review_type,
            "lines_of_code": len([line for line in lines if line.strip() and not line.strip().startswith('#')]),
            "total_lines": len(lines),
            "issues": [],
            "recommendations": [],
            "score": 0
        }
        
        # Check for common issues
        issues = []
        
        # Check for TODO/FIXME comments
        todo_count = sum(1 for line in lines if 'TODO' in line.upper() or 'FIXME' in line.upper())
        if todo_count > 0:
            issues.append(f"Found {todo_count} TODO/FIXME comments")
        
        # Check for long lines
        long_lines = [i+1 for i, line in enumerate(lines) if len(line) > 120]
        if long_lines:
            issues.append(f"Found {len(long_lines)} lines longer than 120 characters")
        
        # Check for magic numbers
        magic_numbers = re.findall(r'\b\d{3,}\b', content)
        if magic_numbers:
            issues.append(f"Found {len(set(magic_numbers))} potential magic numbers")
        
        # Check for missing docstrings
        if path.suffix == '.py':
            functions = re.findall(r'def\s+(\w+)', content)
            classes = re.findall(r'class\s+(\w+)', content)
            docstrings = content.count('"""')
            
            if len(functions) + len(classes) > docstrings // 2:
                issues.append("Missing docstrings for functions/classes")
        
        # Check for security issues
        security_issues = []
        if 'eval(' in content:
            security_issues.append("Use of eval() function")
        if 'exec(' in content:
            security_issues.append("Use of exec() function")
        if 'subprocess.call' in content and 'shell=True' in content:
            security_issues.append("Potential shell injection vulnerability")
        
        if security_issues:
            issues.extend(security_issues)
        
        review["issues"] = issues
        
        # Calculate score
        score = 100
        score -= len(issues) * 5
        if todo_count > 0:
            score -= todo_count * 2
        if len(long_lines) > 0:
            score -= min(len(long_lines) * 1, 20)
        
        review["score"] = max(0, score)
        
        # Generate recommendations
        recommendations = []
        if score < 80:
            recommendations.append("Consider refactoring to improve code quality")
        if todo_count > 0:
            recommendations.append("Address TODO/FIXME comments")
        if len(long_lines) > 0:
            recommendations.append("Break long lines for better readability")
        if security_issues:
            recommendations.append("Review and fix security issues")
        
        review["recommendations"] = recommendations
        
        return review
    
    except Exception as e:
        return {"error": str(e)}

def _security_scan(self, filepath: str) -> Dict[str, Any]:
    """Perform security scan on code."""
    try:
        path = Path(filepath)
        if not path.exists():
            return {"error": f"File {filepath} does not exist"}
        
        with open(path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        security_issues = []
        risk_level = "low"
        
        # Check for common security vulnerabilities
        vulnerabilities = {
            "SQL Injection": [
                r'[\'"]\s*\+.*\+.*[\'"]',  # String concatenation in SQL
                r'f?["\'].*%s.*["\']',     # Format strings with %s
            ],
            "Command Injection": [
                r'subprocess\..*shell\s*=\s*True',
                r'os\.system\(',
                r'commands\.\(',
            ],
            "Code Injection": [
                r'eval\(',
                r'exec\(',
                r'compile\(',
            ],
            "Path Traversal": [
                r'\.\.\/',
                r'open\s*\([^)]*\.\./',
            ],
            "Hardcoded Credentials": [
                r'(?i)(password\s*[=:]\s*["\'][^"\']+["\'])',
                r'(?i)(secret\s*[=:]\s*["\'][^"\']+["\'])',
                r'(?i)(key\s*[=:]\s*["\'][^"\']+["\'])',
            ],
            "Weak Cryptography": [
                r'md5\s*\(',
                r'sha1\s*\(',
                r'DES\s*\(',
            ],
            "Debug Code": [
                r'print\s*\(',
                r'console\.log\s*\(',
                r'pprint\s*\(',
            ]
        }
        
        for category, patterns in vulnerabilities.items():
            found_issues = []
            for pattern in patterns:
                matches = re.finditer(pattern, content, re.IGNORECASE | re.MULTILINE)
                for match in matches:
                    line_num = content[:match.start()].count('\n') + 1
                    line_content = lines[line_num - 1] if line_num <= len(lines) else ""
                    found_issues.append({
                        "line": line_num,
                        "code": line_content.strip()[:100],
                        "match": match.group()[:50]
                    })
            
            if found_issues:
                security_issues.extend([{
                    "category": category,
                    "issues": found_issues
                }])
        
        # Calculate risk level
        critical_count = sum(1 for cat in security_issues 
                           if cat["category"] in ["SQL Injection", "Command Injection", "Code Injection"])
        high_count = sum(1 for cat in security_issues 
                       if cat["category"] in ["Path Traversal", "Hardcoded Credentials"])
        
        if critical_count > 0:
            risk_level = "critical"
        elif high_count > 0:
            risk_level = "high"
        elif len(security_issues) > 0:
            risk_level = "medium"
        
        return {
            "success": True,
            "file": str(path),
            "risk_level": risk_level,
            "total_issues": sum(len(cat["issues"]) for cat in security_issues),
            "categories_found": [cat["category"] for cat in security_issues],
            "security_issues": security_issues,
            "recommendations": [
                "Review all identified security issues",
                "Implement proper input validation",
                "Use parameterized queries",
                "Avoid shell=True in subprocess calls",
                "Store credentials in environment variables or secure vaults"
            ]
        }
    
    except Exception as e:
        return {"error": str(e)}

def _analyze_performance(self, filepath: str) -> Dict[str, Any]:
    """Analyze code performance."""
    try:
        path = Path(filepath)
        if not path.exists():
            return {"error": f"File {filepath} does not exist"}
        
        with open(path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        lines = content.split('\n')
        
        performance_issues = []
        suggestions = []
        
        # Check for common performance anti-patterns
        anti_patterns = {
            "Nested Loops": [
                r'for\s+\w+\s+in.*:\s*\n\s*for\s+\w+\s+in.*:',  # Python nested loops
                r'for\s*\(\s*\w+\s*;\s*\w+\s*;\s*\w+\s*\)\s*\{\s*for\s*\(\s*\w+',  # JS nested loops
            ],
            "Inefficient String Operations": [
                r'\+\s*=',  # String concatenation in loops
                r'string\s*\+\s*=',
            ],
            "Recursion": [
                r'def\s+\w+\s*\([^)]*\):\s*\n\s*return\s+\w+\s*\([^)]*\)',
            ],
            "Database Queries in Loops": [
                r'for\s+\w+\s+in.*:\s*\n.*cursor\.execute',
                r'for\s*\(\s*\w+\s*;\s*\w+\s*;\s*\w+\s*\)\s*\{[^}]*execute',
            ]
        }
        
        for pattern_name, patterns in anti_patterns.items():
            for pattern in patterns:
                matches = re.finditer(pattern, content, re.MULTILINE | re.IGNORECASE)
                for match in matches:
                    line_num = content[:match.start()].count('\n') + 1
                    performance_issues.append({
                        "type": pattern_name,
                        "line": line_num,
                        "code": lines[line_num - 1].strip()[:100] if line_num <= len(lines) else ""
                    })
        
        # Calculate complexity metrics
        complexity_metrics = {
            "cyclomatic_complexity": self._calculate_cyclomatic_complexity(content),
            "lines_of_code": len([line for line in lines if line.strip() and not line.strip().startswith('#')]),
            "comment_ratio": len([line for line in lines if line.strip().startswith('#')]) / len(lines) if lines else 0,
            "function_count": len(re.findall(r'def\s+\w+|function\s+\w+', content)),
            "class_count": len(re.findall(r'class\s+\w+', content))
        }
        
        # Generate performance suggestions
        if complexity_metrics["cyclomatic_complexity"] > 10:
            suggestions.append("Consider breaking down complex functions")
        if complexity_metrics["comment_ratio"] < 0.1:
            suggestions.append("Add more comments for better code documentation")
        if len(performance_issues) > 0:
            suggestions.append("Address performance anti-patterns identified")
        
        return {
            "success": True,
            "file": str(path),
            "complexity_metrics": complexity_metrics,
            "performance_issues": performance_issues,
            "total_issues": len(performance_issues),
            "suggestions": suggestions,
            "performance_score": max(0, 100 - len(performance_issues) * 10 - max(0, complexity_metrics["cyclomatic_complexity"] - 10) * 2)
        }
    
    except Exception as e:
        return {"error": str(e)}

def _calculate_cyclomatic_complexity(self, code: str) -> int:
    """Calculate cyclomatic complexity of code."""
    complexity = 1  # Base complexity
    
    # Count decision points
    decision_keywords = ['if', 'elif', 'else', 'for', 'while', 'except', 'finally', 
                       'and', 'or', 'case', '?', '&&', '||']
    
    for keyword in decision_keywords:
        complexity += len(re.findall(r'\b' + keyword + r'\b', code, re.IGNORECASE))
    
    return complexity

def _check_dependencies(self, filepath: str = ".", dependency_file: str = None) -> Dict[str, Any]:
    """Check project dependencies."""
    try:
        path = Path(filepath)
        
        # Find dependency files
        dependency_files = []
        if dependency_file:
            dep_path = Path(dependency_file)
            if dep_path.exists():
                dependency_files.append(dep_path)
        else:
            # Auto-detect dependency files
            patterns = ['requirements.txt', 'package.json', 'Pipfile', 'poetry.lock', 
                       'Cargo.toml', 'Gemfile', 'go.mod', 'pom.xml']
            for pattern in patterns:
                if path.is_dir():
                    dep_files = list(path.glob(pattern))
                    dependency_files.extend(dep_files)
                elif path.name == pattern:
                    dependency_files.append(path)
        
        if not dependency_files:
            return {"error": "No dependency files found"}
        
        results = {}
        
        for dep_file in dependency_files:
            file_name = dep_file.name
            try:
                if file_name == 'requirements.txt':
                    with open(dep_file, 'r') as f:
                        lines = f.readlines()
                    
                    dependencies = []
                    for line in lines:
                        line = line.strip()
                        if line and not line.startswith('#'):
                            dependencies.append(line)
                    
                    results[file_name] = {
                        "type": "pip",
                        "dependencies": dependencies,
                        "count": len(dependencies)
                    }
                
                elif file_name == 'package.json':
                    with open(dep_file, 'r') as f:
                        data = json.load(f)
                    
                    results[file_name] = {
                        "type": "npm",
                        "dependencies": data.get('dependencies', {}),
                        "dev_dependencies": data.get('devDependencies', {}),
                        "total_count": len(data.get('dependencies', {})) + len(data.get('devDependencies', {}))
                    }
                
                elif file_name.endswith('.lock'):
                    with open(dep_file, 'r') as f:
                        content = f.read()
                    
                    # Count lines that look like dependencies
                    dep_lines = [line for line in content.split('\n') 
                               if line.strip() and not line.strip().startswith('#') 
                               and not line.strip().startswith('[') and '=' in line]
                    
                    results[file_name] = {
                        "type": "locked",
                        "dependencies": dep_lines[:50],  # Sample
                        "count": len(dep_lines)
                    }
            
            except Exception as e:
                results[file_name] = {"error": str(e)}
        
        return {
            "success": True,
            "dependency_files": list(results.keys()),
            "analysis": results,
            "total_files_analyzed": len(results)
        }
    
    except Exception as e:
        return {"error": str(e)}

def _review_architecture(self, directory: str) -> Dict[str, Any]:
    """Review project architecture."""
    try:
        path = Path(directory)
        if not path.exists() or not path.is_dir():
            return {"error": f"Directory {directory} does not exist"}
        
        # Analyze directory structure
        structure = self._analyze_directory_structure(path)
        
        # Check for common architectural patterns
        patterns = {
            "MVC Pattern": ["models/", "views/", "controllers/"],
            "Layered Architecture": ["data/", "business/", "presentation/", "service/"],
            "Microservices": ["api/", "service/", "gateway/", "registry/"],
            "Clean Architecture": ["domain/", "application/", "infrastructure/", "interface/"],
            "Modular Structure": ["core/", "plugins/", "extensions/", "modules/"]
        }
        
        found_patterns = []
        for pattern_name, indicators in patterns.items():
            matches = sum(1 for indicator in indicators 
                        if any(path.glob(f"**/{indicator}*")) or any(path.glob(f"*{indicator}*")))
            if matches > 0:
                found_patterns.append({
                    "pattern": pattern_name,
                    "indicators_found": matches,
                    "confidence": matches / len(indicators)
                })
        
        # Analyze file organization
        file_analysis = self._analyze_file_organization(path)
        
        # Check for common anti-patterns
        anti_patterns = []
        if structure["max_depth"] > 5:
            anti_patterns.append("Deep nesting (depth > 5)")
        if structure["files_in_root"] > 20:
            anti_patterns.append("Too many files in root directory")
        if any("utils" in str(p).lower() for p in path.rglob("*")):
            utils_count = len(list(path.rglob("*/utils/*"))) + len(list(path.rglob("utils*")))
            if utils_count > 3:
                anti_patterns.append("Too many utility modules")
        
        # Generate architecture score
        score = 100
        score -= len(anti_patterns) * 15
        if found_patterns:
            score += len(found_patterns) * 10
        score = max(0, min(100, score))
        
        return {
            "success": True,
            "directory": str(path),
            "architecture_patterns": found_patterns,
            "anti_patterns": anti_patterns,
            "file_organization": file_analysis,
            "structure_analysis": structure,
            "architecture_score": score,
            "recommendations": [
                "Consider implementing clear separation of concerns",
                "Organize code into logical modules or layers",
                "Reduce deep nesting and improve file organization",
                "Document architectural decisions and patterns used"
            ]
        }
    
    except Exception as e:
        return {"error": str(e)}

def _analyze_directory_structure(self, path: Path) -> Dict[str, Any]:
    """Analyze directory structure."""
    dirs = []
    files = []
    max_depth = 0
    
    for item in path.rglob("*"):
        if item.is_dir():
            dirs.append(str(item.relative_to(path)))
            depth = len(item.relative_to(path).parts)
            max_depth = max(max_depth, depth)
        else:
            files.append(str(item.relative_to(path)))
    
    return {
        "total_directories": len(dirs),
        "total_files": len(files),
        "max_depth": max_depth,
        "files_in_root": len(list(path.glob("*"))),
        "large_directories": [d for d in dirs if Path(path / d).is_dir() and 
                             len(list((path / d).rglob("*"))) > 50]
    }

def _analyze_file_organization(self, path: Path) -> Dict[str, Any]:
    """Analyze file organization patterns."""
    file_types = defaultdict(int)
    test_files = []
    config_files = []
    
    for file_path in path.rglob("*"):
        if file_path.is_file():
            ext = file_path.suffix.lower()
            file_types[ext] += 1
            
            name_lower = file_path.name.lower()
            if any(test_word in name_lower for test_word in ['test', 'spec', 'tests']):
                test_files.append(str(file_path.relative_to(path)))
            
            if any(config_word in name_lower for config_word in ['config', 'conf', 'ini', 'cfg', 'env']):
                config_files.append(str(file_path.relative_to(path)))
    
    return {
        "file_types": dict(file_types),
        "test_files": test_files,
        "config_files": config_files,
        "has_tests": len(test_files) > 0,
        "has_config": len(config_files) > 0,
        "organization_score": self._calculate_organization_score(file_types, test_files, config_files)
    }

def _calculate_organization_score(self, file_types: Dict[str, int], test_files: List[str], config_files: List[str]) -> int:
    """Calculate file organization score."""
    score = 50  # Base score
    
    # Bonus for having tests
    if test_files:
        score += 20
    
    # Bonus for having config files
    if config_files:
        score += 15
    
    # Bonus for diverse file types (indicates different components)
    if len(file_types) > 5:
        score += 15
    
    return min(100, score)

def _check_compliance(self, directory: str, framework: str = "general") -> Dict[str, Any]:
    """Check compliance with coding standards and frameworks."""
    try:
        path = Path(directory)
        if not path.exists() or not path.is_dir():
            return {"error": f"Directory {directory} does not exist"}
        
        compliance_issues = []
        recommendations = []
        
        if framework.lower() == "python":
            compliance_issues.extend(self._check_python_compliance(path))
        elif framework.lower() == "javascript":
            compliance_issues.extend(self._check_javascript_compliance(path))
        elif framework.lower() == "security":
            compliance_issues.extend(self._check_security_compliance(path))
        else:
            compliance_issues.extend(self._check_general_compliance(path))
        
        # Calculate compliance score
        total_checks = 10  # Assume 10 checks
        compliance_score = max(0, 100 - len(compliance_issues) * 10)
        
        # Generate recommendations
        recommendations = self._generate_compliance_recommendations(compliance_issues)
        
        return {
            "success": True,
            "directory": str(path),
            "framework": framework,
            "compliance_score": compliance_score,
            "total_issues": len(compliance_issues),
            "compliance_issues": compliance_issues,
            "recommendations": recommendations,
            "compliance_level": "high" if compliance_score >= 80 else "medium" if compliance_score >= 60 else "low"
        }
    
    except Exception as e:
        return {"error": str(e)}

def _check_python_compliance(self, path: Path) -> List[Dict[str, Any]]:
    """Check Python code compliance."""
    issues = []
    
    for py_file in path.rglob("*.py"):
        try:
            with open(py_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            lines = content.split('\n')
            
            # Check for import ordering (PEP 8)
            import_lines = [line for line in lines if line.strip().startswith(('import ', 'from '))]
            if import_lines:
                stdlib_imports = [line for line in import_lines if 'import ' in line and not any(pkg in line for pkg in ['.', '/'])]
                third_party = [line for line in import_lines if 'import ' in line and any(pkg in line for pkg in ['.', '/'])]
                local = [line for line in import_lines if line.strip().startswith('from .')]
                
                if not (stdlib_imports and third_party and local):
                    issues.append({
                        "file": str(py_file),
                        "rule": "Import ordering",
                        "severity": "minor",
                        "description": "Imports should be grouped: standard library, third-party, local"
                    })
            
            # Check for docstrings
            functions = re.findall(r'def\s+(\w+)', content)
            docstrings = content.count('"""')
            classes = re.findall(r'class\s+(\w+)', content)
            
            if functions and docstrings < len(functions) // 2:
                issues.append({
                    "file": str(py_file),
                    "rule": "Docstring coverage",
                    "severity": "moderate",
                    "description": f"Only {docstrings} docstrings for {len(functions)} functions"
                })
            
            # Check for proper exception handling
            try_blocks = content.count('try:')
            except_blocks = content.count('except')
            
            if try_blocks > except_blocks:
                issues.append({
                    "file": str(py_file),
                    "rule": "Exception handling",
                    "severity": "moderate",
                    "description": f"{try_blocks} try blocks but only {except_blocks} except blocks"
                })
        
        except Exception:
            continue
    
    return issues

def _check_javascript_compliance(self, path: Path) -> List[Dict[str, Any]]:
    """Check JavaScript code compliance."""
    issues = []
    
    for js_file in path.rglob("*.js"):
        try:
            with open(js_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Check for semicolons
            lines_without_semicolons = 0
            for line in content.split('\n'):
                stripped = line.strip()
                if (stripped and 
                    not stripped.startswith('//') and 
                    not stripped.startswith('/*') and 
                    not stripped.endswith('*/') and
                    not stripped.endswith('{') and
                    not stripped.endswith('}') and
                    not stripped.endswith(';') and
                    '=' in stripped):
                    lines_without_semicolons += 1
            
            if lines_without_semicolons > 5:
                issues.append({
                    "file": str(js_file),
                    "rule": "Semicolon usage",
                    "severity": "minor",
                    "description": f"{lines_without_semicolons} lines missing semicolons"
                })
            
            # Check for const/let usage
            var_usage = content.count('var ')
            const_let_usage = content.count('const ') + content.count('let ')
            
            if var_usage > 0 and const_let_usage == 0:
                issues.append({
                    "file": str(js_file),
                    "rule": "Variable declarations",
                    "severity": "moderate",
                    "description": "Use const/let instead of var"
                })
        
        except Exception:
            continue
    
    return issues

def _check_security_compliance(self, path: Path) -> List[Dict[str, Any]]:
    """Check security compliance."""
    issues = []
    
    # Security-related checks
    security_checks = [
        (r'password\s*=\s*["\'][^"\']+["\']', "Hardcoded passwords"),
        (r'api_key\s*=\s*["\'][^"\']+["\']', "Hardcoded API keys"),
        (r'subprocess.*shell\s*=\s*True', "Shell injection risk"),
        (r'eval\s*\(', "Code injection risk"),
        (r'exec\s*\(', "Code injection risk"),
    ]
    
    for file_path in path.rglob("*.py"):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            for pattern, description in security_checks:
                matches = re.findall(pattern, content, re.IGNORECASE)
                if matches:
                    issues.append({
                        "file": str(file_path),
                        "rule": "Security",
                        "severity": "high",
                        "description": f"{description}: {len(matches)} occurrences"
                    })
        
        except Exception:
            continue
    
    return issues

def _check_general_compliance(self, path: Path) -> List[Dict[str, Any]]:
    """Check general compliance."""
    issues = []
    
    # General compliance checks
    for file_path in path.rglob("*"):
        if file_path.is_file():
            try:
                # Check file naming conventions
                if file_path.suffix in ['.py', '.js', '.ts']:
                    if re.search(r'[A-Z]', file_path.stem.split('.')[0]):
                        issues.append({
                            "file": str(file_path),
                            "rule": "File naming",
                            "severity": "minor",
                            "description": "Use snake_case for source files"
                        })
                
                # Check for TODO/FIXME comments
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                todos = re.findall(r'#.*(TODO|FIXME|HACK)', content, re.IGNORECASE)
                if todos:
                    issues.append({
                        "file": str(file_path),
                        "rule": "Code maintenance",
                        "severity": "minor",
                        "description": f"{len(todos)} TODO/FIXME comments found"
                    })
            
            except Exception:
                continue
    
    return issues

def _generate_compliance_recommendations(self, issues: List[Dict[str, Any]]) -> List[str]:
    """Generate compliance recommendations based on issues."""
    recommendations = []
    
    issue_types = defaultdict(int)
    for issue in issues:
        issue_types[issue["rule"]] += 1
    
    # Generate specific recommendations
    if issue_types["Import ordering"] > 0:
        recommendations.append("Organize imports according to PEP 8 (stdlib, third-party, local)")
    
    if issue_types["Docstring coverage"] > 0:
        recommendations.append("Add docstrings to functions and classes")
    
    if issue_types["Security"] > 0:
        recommendations.append("Remove hardcoded credentials and fix security vulnerabilities")
    
    if issue_types["Semicolon usage"] > 0:
        recommendations.append("Use semicolons consistently in JavaScript")
    
    if issue_types["Variable declarations"] > 0:
        recommendations.append("Replace var with const/let in JavaScript")
    
    # General recommendations
    if len(issues) > 5:
        recommendations.append("Consider automated code analysis tools (linters, formatters)")
    
    if not recommendations:
        recommendations.append("Code appears to be well-structured and compliant")
    
    return recommendations

# ============================================================================
# Automation Operation Tools
# ============================================================================

def _schedule_task(self, task: str, schedule: str, command: str) -> Dict[str, Any]:
    """Schedule a task to run at specified intervals."""
    try:
        import schedule
        import time
        
        # Parse schedule format
        if schedule.lower() == "daily":
            schedule.every().day.at("09:00").do(lambda: subprocess.run(command, shell=True))
        elif schedule.lower() == "weekly":
            schedule.every().week.do(lambda: subprocess.run(command, shell=True))
        elif schedule.lower() == "hourly":
            schedule.every().hour.do(lambda: subprocess.run(command, shell=True))
        elif schedule.startswith("every "):
            # Parse "every X minutes/hours/days"
            parts = schedule.split()
            if len(parts) >= 3:
                interval = int(parts[1])
                unit = parts[2].lower()
                
                if unit == "minutes":
                    schedule.every(interval).minutes.do(lambda: subprocess.run(command, shell=True))
                elif unit == "hours":
                    schedule.every(interval).hours.do(lambda: subprocess.run(command, shell=True))
                elif unit == "days":
                    schedule.every(interval).days.do(lambda: subprocess.run(command, shell=True))
                else:
                    return {"error": f"Invalid time unit: {unit}"}
            else:
                return {"error": "Invalid schedule format"}
        else:
            return {"error": f"Invalid schedule: {schedule}"}
        
        return {
            "success": True,
            "task": task,
            "schedule": schedule,
            "command": command,
            "message": f"Task '{task}' scheduled to run {schedule}"
        }
    
    except Exception as e:
        return {"error": str(e)}

def _monitor_process(self, process_name: str, check_interval: int = 60) -> Dict[str, Any]:
    """Monitor a process and restart if it crashes."""
    try:
        if not PSUTIL_AVAILABLE:
            return {"error": "psutil not available. Install with: pip install psutil"}
        
        def monitor_worker():
            while True:
                try:
                    # Find process by name
                    processes = [p for p in psutil.process_iter(['name']) 
                               if process_name in p.info['name']]
                    
                    if not processes:
                        logger.warning(f"Process {process_name} not found. Attempting to restart...")
                        # Could add logic to restart the process here
                        break
                    
                    time.sleep(check_interval)
                except Exception as e:
                    logger.error(f"Error monitoring process {process_name}: {e}")
                    break
        
        # Start monitoring in background
        monitor_thread = threading.Thread(target=monitor_worker)
        monitor_thread.daemon = True
        monitor_thread.start()
        
        return {
            "success": True,
            "process": process_name,
            "check_interval": check_interval,
            "status": "Monitoring started"
        }
    
    except Exception as e:
        return {"error": str(e)}

def _create_script(self, script_name: str, script_type: str = "bash", content: str = "") -> Dict[str, Any]:
    """Create automation script."""
    try:
        path = Path(script_name)
        
        # Default content based on type
        if not content:
            if script_type.lower() == "bash":
                content = f"""#!/bin/bash
{script_name} - Automation Script
Created: {datetime.now().isoformat()}
Your automation logic here
echo "Running {script_name}"

Add your commands below
""" elif script_type.lower() == "python": content = f"""#!/usr/bin/env python3

{script_name} - Automation Script
Created: {datetime.now().isoformat()}
import os import subprocess from datetime import datetime

def main(): print("Running {script_name}")

# Your automation logic here
pass
if name == "main": main() """ else: content = f"""# {script_name} - Automation Script

Created: {datetime.now().isoformat()}
Your automation logic here
echo "Running {script_name}" """

        # Write script file
        path.write_text(content, encoding='utf-8')
        
        # Make executable for shell scripts
        if script_type.lower() in ["bash", "sh"]:
            os.chmod(path, 0o755)
        
        return {
            "success": True,
            "script_path": str(path),
            "script_type": script_type,
            "executable": script_type.lower() in ["bash", "sh"],
            "size": len(content),
            "timestamp": datetime.now().isoformat()
        }
    
    except Exception as e:
        return {"error": str(e)}

def _automate_workflow(self, workflow_name: str, steps: List[Dict[str, str]]) -> Dict[str, Any]:
    """Automate a workflow with multiple steps."""
    try:
        workflow_results = []
        
        for i, step in enumerate(steps):
            step_name = step.get("name", f"Step {i+1}")
            step_command = step.get("command", "")
            step_type = step.get("type", "command")
            
            logger.info(f"Executing workflow step: {step_name}")
            
            if step_type == "command":
                result = self._execute_command(step_command)
            elif step_type == "file_operation":
                # Simple file operation automation
                operation = step.get("operation", "read")
                filepath = step.get("filepath", "")
                
                if operation == "read":
                    result = {"success": True, "output": self._read_file(filepath)}
                elif operation == "write":
                    content = step.get("content", "")
                    result = self._write_file(filepath, content)
                else:
                    result = {"error": f"Unknown file operation: {operation}"}
            else:
                result = {"error": f"Unknown step type: {step_type}"}
            
            workflow_results.append({
                "step": step_name,
                "type": step_type,
                "command": step_command,
                "result": result,
                "success": result.get("success", False),
                "timestamp": datetime.now().isoformat()
            })
            
            # Stop workflow if step failed and not configured to continue
            if not result.get("success", False) and not step.get("continue_on_error", False):
                logger.error(f"Workflow {workflow_name} failed at step: {step_name}")
                break
        
        # Calculate workflow success
        total_steps = len(steps)
        successful_steps = sum(1 for r in workflow_results if r["success"])
        
        return {
            "success": successful_steps == total_steps,
            "workflow_name": workflow_name,
            "total_steps": total_steps,
            "successful_steps": successful_steps,
            "failed_steps": total_steps - successful_steps,
            "results": workflow_results,
            "completion_rate": successful_steps / total_steps if total_steps > 0 else 0
        }
    
    except Exception as e:
        return {"error": str(e)}

def _setup_alerts(self, alert_name: str, condition: str, action: str, threshold: str = None) -> Dict[str, Any]:
    """Setup monitoring alerts."""
    try:
        alert_script = f"""#!/usr/bin/env python3
{alert_name} - Alert System
Created: {datetime.now().isoformat()}
import subprocess import time import logging

def check_condition(): # Implement your condition checking logic here # This is a template - customize based on your needs try: {condition} return True except Exception as e: print(f"Condition check failed: {{e}}") return False

def trigger_alert(): # Implement your alert action here try: {action} print("Alert triggered successfully") except Exception as e: print(f"Alert action failed: {{e}}")

def main(): logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s') logger = logging.getLogger(name)

logger.info(f"Starting alert system: {alert_name}")

while True:
    try:
        if check_condition():
            trigger_alert()
        
        # Wait before next check (adjust interval as needed)
        time.sleep(60)  # Check every minute
        
    except KeyboardInterrupt:
        logger.info("Alert system stopped")
        break
    except Exception as e:
        logger.error(f"Alert system error: {{e}}")
        time.sleep(10)  # Wait before retry
if name == "main": main() """

        script_path = f"{alert_name}_alert.py"
        with open(script_path, 'w') as f:
            f.write(alert_script)
        
        os.chmod(script_path, 0o755)
        
        return {
            "success": True,
            "alert_name": alert_name,
            "script_path": script_path,
            "condition": condition,
            "action": action,
            "threshold": threshold,
            "message": f"Alert system '{alert_name}' created. Run 'python {script_path}' to start monitoring."
        }
    
    except Exception as e:
        return {"error": str(e)}

def _automate_backup(self, source: str, destination: str, backup_type: str = "incremental") -> Dict[str, Any]:
    """Automate backup process."""
    try:
        import shutil
        from datetime import datetime
        
        source_path = Path(source)
        dest_path = Path(destination)
        
        if not source_path.exists():
            return {"error": f"Source path {source} does not exist"}
        
        # Create backup directory with timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if backup_type == "full":
            backup_name = f"full_backup_{timestamp}"
        elif backup_type == "incremental":
            backup_name = f"incremental_backup_{timestamp}"
        else:
            return {"error": f"Unknown backup type: {backup_type}"}
        
        backup_path = dest_path / backup_name
        backup_path.mkdir(parents=True, exist_ok=True)
        
        # Perform backup
        start_time = time.time()
        
        if source_path.is_file():
            # Backup single file
            if backup_type == "incremental":
                # Check if file has been modified
                if not self._is_file_modified(source_path, dest_path):
                    return {
                        "success": True,
                        "message": "File not modified since last backup",
                        "backup_type": backup_type
                    }
            
            shutil.copy2(source_path, backup_path / source_path.name)
            backup_size = source_path.stat().st_size
            
        else:
            # Backup directory
            file_count = 0
            total_size = 0
            
            for item in source_path.rglob("*"):
                if item.is_file():
                    rel_path = item.relative_to(source_path)
                    dest_file = backup_path / rel_path
                    dest_file.parent.mkdir(parents=True, exist_ok=True)
                    
                    # For incremental backup, check if file exists and hasn't changed
                    if backup_type == "incremental" and dest_file.exists():
                        if item.stat().st_mtime <= dest_file.stat().st_mtime:
                            continue  # Skip unchanged file
                    
                    shutil.copy2(item, dest_file)
                    backup_size = item.stat().st_size
                    file_count += 1
                    total_size += backup_size
        
        execution_time = time.time() - start_time
        
        return {
            "success": True,
            "source": str(source_path),
            "destination": str(backup_path),
            "backup_type": backup_type,
            "files_backed_up": file_count if source_path.is_dir() else 1,
            "total_size": total_size if source_path.is_dir() else backup_size,
            "execution_time": execution_time,
            "timestamp": timestamp
        }
    
    except Exception as e:
        return {"error": str(e)}

def _is_file_modified(self, file_path: Path, backup_dir: Path) -> bool:
    """Check if file has been modified since last backup."""
    try:
        # Get the latest backup file
        backup_files = list(backup_dir.glob("*"))
        if not backup_files:
            return True  # No previous backup
        
        latest_backup = max(backup_files, key=lambda p: p.stat().st_mtime)
        
        # Compare modification times
        return file_path.stat().st_mtime > latest_backup.stat().st_mtime
    
    except Exception:
        return True  # Assume modified if can't determine

# ============================================================================
# Public Interface
# ============================================================================

def execute_command(self, command_line: str) -> CommandResult:
    """Execute a command line with full intelligence and experience."""
    start_time = time.time()
    
    try:
        # Parse command line
        parts = command_line.strip().split()
        if not parts:
            return CommandResult(
                success=False,
                error="Empty command",
                complexity=ComplexityLevel.SIMPLE,
                confidence=0.0
            )
        
        command = parts[0].lower()
        args = parts[1:]
        
        # Update context
        self.context["last_command"] = command
        self.context["last_command_time"] = datetime.now().isoformat()
        
        # Log command for learning
        self.command_history.append({
            "timestamp": datetime.now().isoformat(),
            "command": command_line,
            "command_name": command,
            "args": args,
            "session_id": self.session_id
        })
        
        # Keep only recent history
        if len(self.command_history) > 100:
            self.command_history = self.command_history[-50:]
        
        # Execute the command
        result = self._execute_command_internal(command, args)
        result.execution_time = time.time() - start_time
        
        # Generate suggestions
        result.suggestions = self._generate_suggestions(command, args, result)
        
        # Record experience
        self._record_experience(command, args, result)
        
        # Update metrics
        self._update_metrics(result.execution_time)
        
        # Store result in conversation history
        self.conversation_history.append({
            "timestamp": datetime.now().isoformat(),
            "command": command_line,
            "success": result.success,
            "execution_time": result.execution_time,
            "result": result
        })
        
        # Keep only recent conversation
        if len(self.conversation_history) > 50:
            self.conversation_history = self.conversation_history[-25:]
        
        logger.info(f"Command '{command}' executed in {result.execution_time:.2f}s with {'success' if result.success else 'failure'}")
        
        return result
    
    except Exception as e:
        logger.error(f"Error executing command '{command_line}': {e}")
        return CommandResult(
            success=False,
            error=str(e),
            execution_time=time.time() - start_time,
            complexity=ComplexityLevel.SIMPLE,
            confidence=0.0
        )

def get_status(self) -> Dict[str, Any]:
    """Get assistant status and performance metrics."""
    session_duration = time.time() - self.session_start_time
    
    return {
        "version": self.version,
        "session_id": self.session_id,
        "uptime_seconds": session_duration,
        "uptime_formatted": self._format_duration(session_duration),
        "values": self.values,
        "experience_domains": {
            domain: {
                "success_rate": exp.success_rate,
                "total_attempts": exp.total_attempts,
                "successful_attempts": exp.successful_attempts,
                "common_patterns": dict(exp.patterns),
                "best_practices": exp.best_practices[-3:] if exp.best_practices else []
            }
            for domain, exp in self.experience.items()
        },
        "metrics": {
            "total_commands": self.metrics["total_commands"],
            "successful_commands": self.metrics["successful_commands"],
            "failed_commands": self.metrics["failed_commands"],
            "success_rate": (self.metrics["successful_commands"] / self.metrics["total_commands"]) 
                          if self.metrics["total_commands"] > 0 else 0,
            "average_execution_time": self.metrics["average_execution_time"],
            "most_used_commands": dict(self.metrics["most_used_commands"].most_common(5)),
            "learning_progress": self.metrics["learning_progress"]
        },
        "context": {
            "current_directory": str(self.current_directory),
            "recent_commands": len(self.command_history),
            "conversation_length": len(self.conversation_history),
            "tools_available": sum(len(category) for category in self.tools.values()),
            "settings": self.settings
        },
        "performance": {
            "commands_per_minute": (self.metrics["total_commands"] / (session_duration / 60)) if session_duration > 0 else 0,
            "memory_usage": self._get_memory_usage(),
            "active_threads": threading.active_count()
        }
    }

def _format_duration(self, seconds: float) -> str:
    """Format duration in human-readable format."""
    if seconds < 60:
        return f"{seconds:.1f}s"
    elif seconds < 3600:
        minutes = seconds / 60
        return f"{minutes:.1f}m"
    else:
        hours = seconds / 3600
        return f"{hours:.1f}h"

def _get_memory_usage(self) -> Dict[str, Any]:
    """Get memory usage statistics."""
    try:
        if PSUTIL_AVAILABLE:
            process = psutil.Process()
            memory_info = process.memory_info()
            return {
                "rss_mb": memory_info.rss / (1024 * 1024),
                "vms_mb": memory_info.vms / (1024 * 1024),
                "percent": process.memory_percent()
            }
        else:
            return {"error": "psutil not available"}
    except Exception:
        return {"error": "Unable to get memory usage"}

def get_available_commands(self) -> Dict[str, List[str]]:
    """Get list of available commands organized by category."""
    return {
        category: list(tools.keys()) for category, tools in self.tools.items()
    }

def get_command_help(self, command: str) -> str:
    """Get help information for a specific command."""
    # Command descriptions
    help_info = {
        # File operations
        "read": "Read file contents. Usage: read <filepath>",
        "write": "Write content to file. Usage: write <filepath> <content>",
        "list": "List files in directory. Usage: list [directory] [pattern]",
        "search": "Search files for text. Usage: search <query> [directory]",
        "analyze": "Analyze file characteristics. Usage: analyze <filepath>",
        "diff": "Compare two files. Usage: diff <file1> <file2>",
        "backup": "Create backup of file. Usage: backup <filepath> [backup_dir]",
        "compress": "Compress file. Usage: compress <filepath> [gzip|tar]",
        "extract": "Extract archive. Usage: extract <archive_path> [destination]",
        "permissions": "Change file permissions. Usage: permissions <filepath> <permissions>",
        
        # System commands
        "execute": "Execute system command. Usage: execute <command>",
        "process_info": "Get process information. Usage: process_info [pid]",
        "disk_usage": "Get disk usage. Usage: disk_usage [path]",
        "network_status": "Check network connectivity. Usage: network_status",
        "system_info": "Get system information. Usage: system_info",
        "install": "Install package. Usage: install <package> [package_manager]",
        "service_control": "Control system service. Usage: service_control <action> <service_name>",
        "log_analysis": "Analyze log file. Usage: log_analysis <logfile> [pattern] [lines]",
        
        # Web operations
        "web_search": "Search the web. Usage: web_search <query> [num_results]",
        "download": "Download file from URL. Usage: download <url> <filepath>",
        "http_status": "Check HTTP status. Usage: http_status <url> [method]",
        "curl": "Make HTTP request. Usage: curl <url> [method] [headers] [data]",
        "api_call": "Make API call. Usage: api_call <endpoint> [method] [headers] [data]",
        
        # Code operations
        "run": "Run code. Usage: run <code> [language]",
        "lint": "Lint code file. Usage: lint <filepath>",
        "format": "Format code file. Usage: format <filepath>",
        "test": "Run tests. Usage: test [filepath|directory] [pattern]",
        "debug": "Debug code. Usage: debug <filepath>",
        "profile": "Profile code performance. Usage: profile <filepath>",
        "docstring": "Generate docstrings. Usage: docstring <filepath> [style]",
        
        # Data operations
        "parse_csv": "Parse CSV file. Usage: parse_csv <filepath> [delimiter]",
        "parse_json": "Parse JSON file. Usage: parse_json <filepath>",
        "parse_xml": "Parse XML file. Usage: parse_xml <filepath>",
        "database_query": "Query database. Usage: database_query <connection_string> <query>",
        "analyze_data": "Analyze data. Usage: analyze_data <data>",
        "visualize": "Create visualization. Usage: visualize <data> <chart_type> [output_file]",
        "clean_data": "Clean data. Usage: clean_data <data> <operations>",
        
        # Analysis operations
        "review_code": "Review code quality. Usage: review_code <filepath> [review_type]",
        "security_scan": "Security scan. Usage: security_scan <filepath>",
        "analyze_performance": "Performance analysis. Usage: analyze_performance <filepath>",
        "check_dependencies": "Check dependencies. Usage: check_dependencies [directory] [dependency_file]",
        "review_architecture": "Architecture review. Usage: review_architecture <directory>",
        "check_compliance": "Compliance check. Usage: check_compliance <directory> [framework]",
        
        # Automation operations
        "schedule_task": "Schedule task. Usage: schedule_task <task> <schedule> <command>",
        "monitor_process": "Monitor process. Usage: monitor_process <process_name> [check_interval]",
        "create_script": "Create script. Usage: create_script <script_name> [script_type] [content]",
        "automate_workflow": "Automate workflow. Usage: automate_workflow <workflow_name> <steps>",
        "setup_alerts": "Setup alerts. Usage: setup_alerts <alert_name> <condition> <action> [threshold]",
        "automate_backup": "Automate backup. Usage: automate_backup <source> <destination> [backup_type]",
        
        # Utility commands
        "help": "Show help information. Usage: help [command]",
        "status": "Show assistant status. Usage: status",
        "history": "Show command history. Usage: history [count]",
        "clear": "Clear screen. Usage: clear",
        "quit": "Exit assistant. Usage: quit",
        "exit": "Exit assistant. Usage: exit"
    }
    
    return help_info.get(command, f"No help available for '{command}'. Available commands: {list(help_info.keys())}")

def save_session(self, filepath: str = None) -> Dict[str, Any]:
    """Save current session data."""
    try:
        if filepath is None:
            filepath = f"assistant_session_{self.session_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        session_data = {
            "version": self.version,
            "session_id": self.session_id,
            "timestamp": datetime.now().isoformat(),
            "conversation_history": self.conversation_history,
            "command_history": self.command_history,
            "experience": {
                domain: {
                    "success_rate": exp.success_rate,
                    "total_attempts": exp.total_attempts,
                    "successful_attempts": exp.successful_attempts,
                    "patterns": dict(exp.patterns),
                    "best_practices": exp.best_practices,
                    "failure_modes": exp.failure_modes
                }
                for domain, exp in self.experience.items()
            },
            "context": self.context,
            "metrics": self.metrics,
            "settings": self.settings
        }
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(session_data, f, indent=2, default=str)
        
        return {
            "success": True,
            "filepath": filepath,
            "session_id": self.session_id,
            "data_size": len(json.dumps(session_data, default=str))
        }
    
    except Exception as e:
        return {"error": str(e)}

def load_session(self, filepath: str) -> Dict[str, Any]:
    """Load session data from file."""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            session_data = json.load(f)
        
        # Restore data
        self.conversation_history = session_data.get("conversation_history", [])
        self.command_history = session_data.get("command_history", [])
        self.context = session_data.get("context", {})
        self.metrics = session_data.get("metrics", {})
        self.settings = session_data.get("settings", {})
        
        # Restore experience
        for domain, exp_data in session_data.get("experience", {}).items():
            if domain in self.experience:
                self.experience[domain].success_rate = exp_data.get("success_rate", 0.0)
                self.experience[domain].total_attempts = exp_data.get("total_attempts", 0)
                self.experience[domain].successful_attempts = exp_data.get("successful_attempts", 0)
                self.experience[domain].patterns = exp_data.get("patterns", {})
                self.experience[domain].best_practices = exp_data.get("best_practices", [])
                self.experience[domain].failure_modes = exp_data.get("failure_modes", [])
        
        logger.info(f"Session loaded from {filepath}")
        
        return {
            "success": True,
            "filepath": filepath,
            "session_id": session_data.get("session_id"),
            "loaded_commands": len(self.command_history),
            "loaded_conversations": len(self.conversation_history)
        }
    
    except Exception as e:
        return {"error": str(e)}
def main(): """Main entry point for the terminal assistant.""" parser = argparse.ArgumentParser( description="Battle-Tested Terminal Assistant", formatter_class=argparse.RawDescriptionHelpFormatter, epilog=""" Examples: %(prog)s "read file.txt" %(prog)s "search error logs/" %(prog)s "web_search python tutorial" %(prog)s "analyze performance script.py" %(prog)s "security_scan app.py" %(prog)s "automate_workflow deploy [{"name": "build", "command": "make build"}]" """ )

parser.add_argument(
    "command",
    nargs="?",
    help="Command to execute"
)

parser.add_argument(
    "--interactive", "-i",
    action="store_true",
    help="Start interactive mode"
)

parser.add_argument(
    "--verbose", "-v",
    action="store_true",
    help="Enable verbose output"
)

parser.add_argument(
    "--session", "-s",
    help="Load session from file"
)

parser.add_argument(
    "--save-session",
    help="Save session to file after execution"
)

args = parser.parse_args()

# Initialize assistant
assistant = TerminalAssistant()

# Load session if specified
if args.session:
    result = assistant.load_session(args.session)
    if not result.get("success"):
        print(f"âŒ Failed to load session: {result.get('error')}")
        return 1
    else:
        print(f"âœ… Session loaded: {result.get('session_id')}")

# Set verbosity
assistant.settings["verbose"] = args.verbose or args.interactive

# Interactive mode
if args.interactive:
    print("ðŸš€ Battle-Tested Terminal Assistant - Interactive Mode")
    print("=" * 60)
    print("Type 'help' for available commands, 'status' for performance metrics, or 'quit' to exit.")
    print("=" * 60)
    
    while True:
        try:
            command = input(f"\n[{assistant.session_id}] > ").strip()
            
            if not command:
                continue
            
            if command.lower() in ["quit", "exit", "q"]:
                print("ðŸ‘‹ Goodbye!")
                break
            
            if command.lower() == "help":
                print("\nðŸ“‹ Available Commands:")
                commands = assistant.get_available_commands()
                for category, cmd_list in commands.items():
                    print(f"\n  {category.replace('_', ' ').title()}:")
                    for cmd in cmd_list:
                        print(f"    - {cmd}")
                continue
            
            if command.lower() == "status":
                status = assistant.get_status()
                print(f"\nðŸ“Š Assistant Status:")
                print(f"  Version: {status['version']}")
                print(f"  Uptime: {status['uptime_formatted']}")
                print(f"  Commands: {status['metrics']['total_commands']} total")
                print(f"  Success Rate: {status['metrics']['success_rate']:.1%}")
                print(f"  Tools: {status['context']['tools_available']}")
                print(f"  Experience Domains: {len(status['experience_domains'])}")
                continue
            
            if command.lower() == "history":
                count = 10
                if len(assistant.command_history) >= count:
                    recent = assistant.command_history[-count:]
                else:
                    recent = assistant.command_history
                
                print(f"\nðŸ“ Command History (last {len(recent)}):")
                for i, cmd in enumerate(recent, 1):
                    status_icon = "âœ…" if cmd.get("success") else "âŒ"
                    print(f"  {i}. {status_icon} {cmd.get('command', '')}")
                continue
            
            if command.lower() == "clear":
                os.system('clear' if os.name != 'nt' else 'cls')
                continue
            
            # Execute command
            print(f"\nâš¡ Executing: {command}")
            result = assistant.execute_command(command)
            
            # Display result
            if result.success:
                print(f"âœ… Success ({result.execution_time:.2f}s)")
                if result.output:
                    print(f"ðŸ“„ Output:\n{result.output}")
            else:
                print(f"âŒ Failed ({result.execution_time:.2f}s)")
                if result.error:
                    print(f"ðŸš¨ Error: {result.error}")
            
            # Show suggestions
            if result.suggestions:
                print(f"\nðŸ’¡ Suggestions:")
                for suggestion in result.suggestions:
                    print(f"  - {suggestion}")
        
        except KeyboardInterrupt:
            print("\n\nðŸ‘‹ Interrupted. Use 'quit' to exit properly.")
        except Exception as e:
            print(f"\nâŒ Unexpected error: {e}")
            logger.error(f"Interactive mode error: {e}")
    
    # Save session if requested
    if args.save_session:
        result = assistant.save_session(args.save_session)
        if result.get("success"):
            print(f"ðŸ’¾ Session saved: {result.get('filepath')}")
        else:
            print(f"âŒ Failed to save session: {result.get('error')}")

else:
    # Single command mode
    if not args.command:
        print("âŒ No command provided. Use --help for usage information.")
        return 1
    
    result = assistant.execute_command(args.command)
    
    # Display result
    if args.verbose:
        print(f"\nðŸ“Š Execution Details:")
        print(f"  Command: {args.command}")
        print(f"  Success: {result.success}")
        print(f"  Execution Time: {result.execution_time:.2f}s")
        print(f"  Complexity: {result.complexity.value}")
        print(f"  Confidence: {result.confidence:.2f}")
    
    if result.success and result.output:
        print(result.output)
    elif not result.success and result.error:
        print(f"Error: {result.error}")
        return 1
    
    # Show suggestions if verbose
    if args.verbose and result.suggestions:
        print(f"\nðŸ’¡ Suggestions:")
        for suggestion in result.suggestions:
            print(f"  - {suggestion}")
    
    # Save session if requested
    if args.save_session:
        result = assistant.save_session(args.save_session)
        if result.get("success"):
            print(f"\nðŸ’¾ Session saved: {result.get('filepath')}")
        else:
            print(f"\nâŒ Failed to save session: {result.get('error')}")

return 0
if name == "main": exit(main()) </function_calls>


assistant.py
provide full repo scaffold
edit
concisely state what the code does step by step
edit
The assistant program has three main functions that interact with a Python user using ChatBot style commands:

Chat - this function prompts the user to enter chat commands, which are then sent to the assistant's chat interface for processing. The assistant responds with either text or voice messages based on the command.
Assistant - this function is called when a chat message is received by the assistant's chat interface. It allows the assistant to handle user requests and execute tasks.
Main Loop - this function starts the main loop, which continuously receives and processes chat messages from the assistant's chat interface. The program checks if there are any new chat messages and prompts the user if they want to interact with the assistant again. It also logs any errors that occur during the processing of chat messages. The assistant program uses ChatBot style commands to simulate a chat conversation between a human and an artificial intelligence system, simulating a virtual assistant. The program starts by importing necessary libraries for text messaging, voice recognition, and chatbot interaction. It also imports modules for handling user input and error logging.
To run the assistant program, first install the required dependencies using pip:

pip install -r requirements.txt
Then execute the main function:

main()
This will start the assistant's main loop and allow you to interact with it as a virtual human.